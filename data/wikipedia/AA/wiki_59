{"id": "7053", "revid": "1012842298", "url": "https://en.wikipedia.org/wiki?curid=7053", "title": "Cannon", "text": "A cannon is a large-caliber gun classified as a type of artillery, and usually launches a projectile using explosive chemical propellant. In the past, black gunpowder was the primary propellant before the invention of smokeless powder during the late 19th century. Cannons vary in gauge, effective range, mobility, rate of fire, angle of fire and firepower; different forms of cannon combine and balance these attributes in varying degrees, depending on their intended use on the battlefield. \nThe word \"cannon\" is derived from several languages, in which the original definition can usually be translated as \"tube\", \"cane\", or \"reed\". In the modern era, the term \"cannon\" has fallen into decline, replaced by \"guns\" or \"artillery\", if not a more specific term such as howitzer or mortar, except for high-caliber automatic weapons firing bigger rounds than machine guns, called autocannons. \nThe earliest known depiction of cannons appeared in Song dynasty China as early as the 12th century; however, solid archaeological and documentary evidence of cannons do not appear until the 13th century. In 1288 Yuan dynasty troops are recorded to have used hand cannon in combat, and the earliest extant cannon bearing a date of production comes from the same period. By the early 14th century, depictions of cannon had appeared in the Middle East and Europe, and recorded usage of cannon began appearing almost immediately after. By the end of the 14th century, cannons were widespread throughout Eurasia. Cannons were used primarily as anti-infantry weapons until around 1374, when large cannons were recorded to have breached walls for the first time in Europe. Cannons featured prominently as siege weapons, and ever larger pieces appeared. In 1464 a 16,000\u00a0kg (35,000\u00a0lbs) cannon known as the Great Turkish Bombard was created in the Ottoman Empire. Cannons as field artillery became more important after 1453, with the introduction of limber, which greatly improved cannon maneuverability and mobility. European cannons reached their longer, lighter, more accurate, and more efficient \"classic form\" around 1480. This classic European cannon design stayed relatively consistent in form with minor changes until the 1750s.\nEtymology and terminology.\n\"Cannon\" is derived from the Old Italian word \"cannone\", meaning \"large tube\", which came from Latin \"canna\", in turn originating from the Greek \u03ba\u03ac\u03bd\u03bd\u03b1 (\"kanna\"), \"reed\", and then generalised to mean any hollow tube-like object; cognate with Akkadian \"qanu(m)\" and Hebrew \"q\u0101neh\", \"tube, reed\". The word has been used to refer to a gun since 1326 in Italy, and 1418 in England. Both of the plural forms \"cannons\" and \"cannon\" are correct.\nHistory.\nEast Asia.\nThe cannon may have appeared as early as the 12th century in China, and was probably a parallel development or evolution of the fire-lance, a short ranged anti-personnel weapon combining a gunpowder-filled tube and a polearm of some sort. Co-viative projectiles such as iron scraps or porcelain shards were placed in fire lance barrels at some point, and eventually, the paper and bamboo materials of fire lance barrels were replaced by metal.\nThe earliest known depiction of a cannon is a sculpture from the Dazu Rock Carvings in Sichuan dated to 1128, however the earliest archaeological samples and textual accounts do not appear until the 13th century. The primary extant specimens of cannon from the 13th century are the Wuwei Bronze Cannon dated to 1227, the Heilongjiang hand cannon dated to 1288, and the Xanadu Gun dated to 1298. However, only the Xanadu gun contains an inscription bearing a date of production, so it is considered the earliest confirmed extant cannon. The Xanadu Gun is 34.7\u00a0cm in length and weighs 6.2\u00a0kg. The other cannons are dated using contextual evidence. The Heilongjiang hand cannon is also often considered by some to be the oldest firearm since it was unearthed near the area where the History of Yuan reports a battle took place involving hand cannons. According to the History of Yuan, in 1288, a Jurchen commander by the name of Li Ting led troops armed with hand cannons into battle against the rebel prince Nayan.\nChen Bingying argues there were no guns before 1259 while Dang Shoushan believes the Wuwei gun and other Western Xia era samples point to the appearance of guns by 1220, and Stephen Haw goes even further by stating that guns were developed as early as 1200. Sinologist Joseph Needham and renaissance siege expert Thomas Arnold provide a more conservative estimate of around 1280 for the appearance of the \"true\" cannon. Whether or not any of these are correct, it seems likely that the gun was born sometime during the 13th century.\nReferences to cannons proliferated throughout China in the following centuries. Cannon featured in literary pieces. In 1341 Xian Zhang wrote a poem called \"The Iron Cannon Affair\" describing a cannonball fired from an eruptor which could \"pierce the heart or belly when striking a man or horse, and even transfix several persons at once.\"\nThe Mongol invasion of Java in 1293 brought gunpowder technology to the Nusantara archipelago in the form of cannon (Chinese: \"Pao\"). By the 1350s the cannon was used extensively in Chinese warfare. In 1358 the Ming army failed to take a city due to its garrisons' usage of cannon, however they themselves would use cannon, in the thousands, later on during the siege of Suzhou in 1366. The Korean kingdom of Joseon started producing gunpowder in 1374 and cannons by 1377. Cannon appeared in \u0110\u1ea1i Vi\u1ec7t by 1390 at the latest.\nDuring the Ming dynasty cannons were used in riverine warfare at the Battle of Lake Poyang. One shipwreck in Shandong had a cannon dated to 1377 and an anchor dated to 1372. From the 13th to 15th centuries cannon-armed Chinese ships also travelled throughout Southeast Asia.\nThe first of the western cannon to be introduced were breech-loaders in the early 16th century which the Chinese began producing themselves by 1523 and improved on by including composite metal construction in their making.\nJapan did not acquire a cannon until 1510 when a monk brought one back from China, and did not produce any in appreciable numbers. During the 1593 Siege of Pyongyang, 40,000 Ming troops deployed a variety of cannons against Japanese troops. Despite their defensive advantage and the use of arquebus by Japanese soldiers, the Japanese were at a severe disadvantage due to their lack of cannon. Throughout the Japanese invasions of Korea (1592\u201398), the Ming-Joseon coalition used artillery widely in land and naval battles, including on the turtle ships of Yi Sun-sin.\nAccording to Ivan Petlin, the first Russian envoy to Beijing, in September 1619, the city was armed with large cannon with cannonballs weighing more than . His general observation was that the Chinese were militarily capable and had firearms:\nWestern Europe.\nOutside of China, the earliest texts to mention gunpowder are Roger Bacon's \"Opus Majus\" (1267) and \"Opus Tertium\" in what has been interpreted as references to firecrackers. In the early 20th century, a British artillery officer proposed that another work tentatively attributed to Bacon, \"Epistola de Secretis Operibus Artis et Naturae, et de Nullitate Magiae\", also known as \"Opus Minor\", dated to 1247, contained an encrypted formula for gunpowder hidden in the text. These claims have been disputed by science historians. In any case, the formula itself is not useful for firearms or even firecrackers, burning slowly and producing mostly smoke.\nThere is a record of a gun in Europe dating to 1322 being discovered in the nineteenth century but the artifact has since been lost. The earliest known European depiction of a gun appeared in 1326 in a manuscript by Walter de Milemete, although not necessarily drawn by him, known as \"De Nobilitatibus, sapientii et prudentiis regum\" (Concerning the Majesty, Wisdom, and Prudence of Kings), which displays a gun with a large arrow emerging from it and its user lowering a long stick to ignite the gun through the touchole In the same year, another similar illustration showed a darker gun being set off by a group of knights, which also featured in another work of de Milemete's, \"De secretis secretorum Aristotelis\". On 11 February of that same year, the Signoria of Florence appointed two officers to obtain \"canones de mettallo\" and ammunition for the town's defense. In the following year a document from the Turin area recorded a certain amount was paid \"for the making of a certain instrument or device made by Friar Marcello for the projection of pellets of lead.\" A reference from 1331 describes an attack mounted by two Germanic knights on Cividale del Friuli, using gunpowder weapons of some sort. The 1320s seem to have been the takeoff point for guns in Europe according to most modern military historians. Scholars suggest that the lack of gunpowder weapons in a well-traveled Venetian's catalogue for a new crusade in 1321 implies that guns were unknown in Europe up until this point, further solidifying the 1320 mark, however more evidence in this area may be forthcoming in the future.\nThe oldest extant cannon in Europe is a small bronze example unearthed in Loshult, Scania in southern Sweden. It dates from the early-mid 14th century, and is currently in the Swedish History Museum in Stockholm.\nEarly cannons in Europe often shot arrows and were known by an assortment of names such as \"pot-de-fer\", \"tonnoire\", \"ribaldis\", and \"b\u00fcszenpyle\". The \"ribaldi\"s, which shot large arrows and simplistic grapeshot, were first mentioned in the English Privy Wardrobe accounts during preparations for the Battle of Cr\u00e9cy, between 1345 and 1346. The Florentine Giovanni Villani recounts their destructiveness, indicating that by the end of the battle, \"the whole plain was covered by men struck down by arrows and cannon balls.\" Similar cannon were also used at the Siege of Calais (1346\u201347), although it was not until the 1380s that the \"ribaudekin\" clearly became mounted on wheels.\nEarly use.\nThe battle of Crecy which pitted the English against the French in 1346 featured the early use of cannon which helped the long-bowmen repulse a large force of Genoese crossbowmen deployed by the French. The English originally intended to use the cannon against cavalry sent to attack their archers, thinking that the loud noises produced by their cannon would panic the advancing horses along with killing the knights atop them.\nEarly cannons could also be used for more than simply killing men and scaring horses. English cannon were used defensively during the siege of the castle Breteuil to launch fire onto an advancing belfry. In this way cannons could be used to burn down siege equipment before it reached the fortifications. The use of cannons to shoot fire could also be used offensively as another battle involved the setting of a castle ablaze with similar methods. The particular incendiary used in these cannons was most likely a gunpowder mixture. This is one area where early Chinese and European cannons share a similarity as both were possibly used to shoot fire.\nAnother aspect of early European cannons is that they were rather small, dwarfed by the bombards which would come later. In fact, it is possible that the cannons used at Crecy were capable of being moved rather quickly as there is an anonymous chronicle that notes the guns being used to attack the French camp, indicating that they would have been mobile enough press the attack. These smaller cannons would eventually give way to larger, wall breaching guns by the end of the 1300s.\nEastern Europe.\nDocumentary evidence of cannons in Russia does not appear until 1382 and they were used only in sieges, often by the defenders. It was not until 1475 when Ivan III established the first Russian cannon foundry in Moscow that they began to produce cannons natively.\nLater on large cannons were known as bombards, ranging from three to five feet in length and were used by Dubrovnik and Kotor in defence during the later 14th century. The first bombards were made of iron, but bronze became more prevalent as it was recognized as more stable and capable of propelling stones weighing as much as . Around the same period, the Byzantine Empire began to accumulate its own cannon to face the Ottoman Empire, starting with medium-sized cannon long and of 10\u00a0in calibre. The earliest reliable recorded use of artillery in the region was against the Ottoman siege of Constantinople in 1396, forcing the Ottomans to withdraw. The Ottomans acquired their own cannon and laid siege to the Byzantine capital again in 1422. By 1453, the Ottomans used 68 Hungarian-made cannon for the 55-day bombardment of the walls of Constantinople, \"hurling the pieces everywhere and killing those who happened to be nearby.\" The largest of their cannons was the Great Turkish Bombard, which required an operating crew of 200 men and 70 oxen, and 10,000 men to transport it. Gunpowder made the formerly devastating Greek fire obsolete, and with the final fall of Constantinople\u2014which was protected by what were once the strongest walls in Europe\u2014on 29 May 1453, \"it was the end of an era in more ways than one.\"\nIslamic world.\nThere is no clear consensus of when the cannon first appeared in the Islamic world, with dates ranging from 1260 to the mid-14th century. The cannon may have appeared in the Islamic world in the late 13th century, with Ibn Khaldun in the 14th century stating that cannons were used in the Maghreb region of North Africa in 1274, and other Arabic military treatises in the 14th century referring to the use of cannon by Mamluk forces in 1260 and 1303, and by Muslim forces at the 1324 Siege of Huesca in Spain. However, some scholars do not accept these early dates. While the date of its first appearance is not entirely clear, the general consensus among most historians is that there is no doubt the Mamluk forces were using cannon by 1342.\nAccording to historian Ahmad Y. al-Hassan, during the Battle of Ain Jalut in 1260, the Mamluks used cannon against the Mongols. He claims that this was \"the first cannon in history\" and used a gunpowder formula almost identical to the ideal composition for explosive gunpowder. He also argues that this was not known in China or Europe until much later. Hassan further claims that the earliest textual evidence of cannon is from the Middle East, based on earlier originals which report hand-held cannons being used by the Mamluks at the Battle of Ain Jalut in 1260. Such an early date is not accepted by some historians, including David Ayalon, Iqtidar Alam Khan, Joseph Needham and Tonio Andrade. Khan argues that it was the Mongols who introduced gunpowder to the Islamic world, and believes cannon only reached Mamluk Egypt in the 1370s. Needham argued that the term \"midfa\", dated to textual sources from 1342 to 1352, did not refer to true hand-guns or bombards, and that contemporary accounts of a metal-barrel cannon in the Islamic world did not occur until 1365. Similarly, Andrade dates the textual appearance of cannons in middle eastern sources to the 1360s. Gabor \u00c1goston and David Ayalon note that the Mamluks had certainly used siege cannons by 1342 or the 1360s, respectively, but earlier uses of cannons in the Islamic World are vague with a possible appearance in the Emirate of Granada by the 1320s and 1330s, though evidence is inconclusive.\nIbn Khaldun reported the use of cannon as siege machines by the Marinid sultan Abu Yaqub Yusuf at the siege of Sijilmasa in 1274. The passage by Ibn Khaldun on the Marinid Siege of Sijilmassa in 1274 occurs as follows: \"[The Sultan] installed siege engines \u2026 and gunpowder engines \u2026, which project small balls of iron. These balls are ejected from a chamber \u2026 placed in front of a kindling fire of gunpowder; this happens by a strange property which attributes all actions to the power of the Creator.\" The source is not contemporary and was written a century later around 1382. Its interpretation has been rejected as anachronistic by some historians, who urge caution regarding claims of Islamic firearms use in the 1204\u20131324 period as late medieval Arabic texts used the same word for gunpowder, naft, as they did for an earlier incendiary, naphtha. \u00c1goston and Peter Purton note that in the 1204\u20131324 period, late medieval Arabic texts used the same word for gunpowder, \"naft\", that they used for an earlier incendiary naphtha. Needham believes Ibn Khaldun was speaking of fire lances rather than hand cannon.\nThe Ottoman Empire made good use of cannon as siege artillery. Sixty-eight super-sized bombards were used by Mehmed the Conqueror to capture Constantinople in 1453. Jim Bradbury argues that Urban, a Hungarian cannon engineer, introduced this cannon from Central Europe to the Ottoman realm; according to Paul Hammer, however, it could have been introduced from other Islamic countries which had earlier used cannons. These cannon could fire heavy stone balls a mile, and the sound of their blast could reportedly be heard from a distance of . Shkod\u00ebran historian Marin Barleti discusses Turkish bombards at length in his book \"De obsidione Scodrensi\" (1504), describing the 1478\u201379 siege of Shkodra in which eleven bombards and two mortars were employed. The Ottomans also used cannon to control passage of ships through the Bosphorus strait. Ottoman cannons also proved effective at stopping crusaders at Varna in 1444 and Kosovo in 1448 despite the presence of European cannon in the former case.\nThe similar Dardanelles Guns (for the location) were created by Munir Ali in 1464 and were still in use during the Anglo-Turkish War (1807\u201309). These were cast in bronze into two parts, the chase (the barrel) and the breech, which combined weighed 18.4\u00a0tonnes. The two parts were screwed together using levers to facilitate moving it.\nFathullah Shirazi, a Persian inhabitant of India who worked for Akbar in the Mughal Empire, developed a volley gun in the 16th century.\nIranian cannon.\nWhile there is evidence of cannons in Iran as early as 1405 they were not widespread. This changed following the increased use of firearms by Shah Isma il I, and the Iranian army used 500 cannons by the 1620s, probably captured from the Ottomans or acquired by allies in Europe. By 1443 Iranians were also making some of their own cannon, as Mir Khawand wrote of a 1200\u00a0kg metal piece being made by an Iranian \"rekhtagar\" which was most likely a cannon. Due to the difficulties of transporting cannon in mountainous terrain, their use was less common compared to their use in Europe.\nSoutheast Asia.\nThe Javanese Majapahit Empire was arguably able to encompass much of modern-day Indonesia due to its unique mastery of bronze-smithing and use of a central arsenal fed by a large number of cottage industries within the immediate region. Documentary and archeological evidence indicate that Arab traders introduced gunpowder, gonnes, muskets, blunderbusses, and cannon to the Javanese, Acehnese, and Batak via long established commercial trade routes around the early to mid 14th century. The resurgent Singhasari Empire overtook Sriwijaya and later emerged as the Majapahit whose warfare featured the use of fire-arms and cannonade. Cannons were introduced to Majapahit when Kublai Khan's Chinese army under the leadership of Ike Mese sought to invade Java in 1293. History of Yuan mentioned that the Mongol used cannons (Chinese: \"Pao\") against Daha forces. \nJavanese bronze breech-loaded swivel-guns, known as cetbang or lantaka, were used widely by the Majapahit navy as well as by pirates and rival lords. One of the earliest reference to cannons and artillerymen in Java is from the year 1346. The demise of the Majapahit empire and the dispersal of disaffected skilled bronze cannon-smiths to Brunei, modern Sumatra, Malaysia and the Philippines lead to widespread use, especially in the Makassar Strait. This event led to near universal use of the swivel-gun and cannon in the Nusantara archipelago. When the Portuguese first came to Malacca, they found a large colony of Javanese merchants under their own headmen; the Javanese were manufacturing their own cannon, which then, and for long after, were as necessary to merchant ships as sails.\nCannons derived from cetbang can be found in Nusantara, among others were lantaka and lela. Most lantakas were made of bronze and the earliest ones were breech-loaded. There is a trend toward muzzle-loading weapons during colonial times. Pole gun (bedil tombak) was recorded as being used by Java in 1413.\nPortuguese and Spanish invaders were unpleasantly surprised and even outgunned on occasion. Circa 1540, the Javanese, always alert for new weapons found the newly arrived Portuguese weaponry superior to that of the locally made variants. Majapahit-era cetbang cannon were further improved and used in the Demak Sultanate period during the Demak invasion of Portuguese Malacca. During this period, the iron, for manufacturing Javanese cannon was imported from Khorasan in northern Persia. The material was known by Javanese as \"wesi kurasani\" (Khorasan iron). When the Portuguese came to the archipelago, they referred to it as \"Ber\u00e7o\", which was also used to refer to any breech-loading swivel gun, while the Spaniards call it \"Verso\".\nDuarte Barbosa ca. 1514 said that the inhabitants of Java are great masters in casting artillery and very good artillerymen. They make many one-pounder cannon (cetbang or rentaka), long muskets, \"spingarde\" (arquebus), \"schioppi\" (hand cannon), Greek fire, guns (cannon), and other fire-works. Every place are considered excellent in casting artillery, and in the knowledge of using it. In 1513, the Javanese fleet led by Patih Yunus sailed to attack Portuguese Malacca \"with much artillery made in Java, for the Javanese are skilled in founding and casting, and in all works in iron, over and above what they have in India\". By early 16th century, the Javanese already locally-producing large guns, some of them still survived until the present day and dubbed as \"sacred cannon\" or \"holy cannon\". These cannons varied between 180- and 260-pounders, weighing anywhere between 3 and 8 tons, length of them between .\nCannons were used by the Ayutthaya Kingdom in 1352 during its invasion of the Khmer Empire. Within a decade large quantities of gunpowder could be found in the Khmer Empire. By the end of the century firearms were also used by the Tr\u1ea7n dynasty.\nSaltpeter harvesting was recorded by Dutch and German travelers as being common in even the smallest villages and was collected from the decomposition process of large dung hills specifically piled for the purpose. The Dutch punishment for possession of non-permitted gunpowder appears to have been amputation. Ownership and manufacture of gunpowder was later prohibited by the colonial Dutch occupiers. According to colonel McKenzie quoted in Sir Thomas Stamford Raffles', \"The History of Java\" (1817), the purest sulfur was supplied from a crater from a mountain near the straits of Bali.\nAfrica.\nIn Africa, the Adal Sultanate and the Abyssinian Empire both deployed cannons during the Adal-Abyssinian War. Imported from Arabia, and the wider Islamic world, the Adalites led by Ahmed ibn Ibrahim al-Ghazi were the first African power to introduce cannon warfare to the African continent. Later on as the Portuguese Empire entered the war it would supply and train the Abyssinians with cannons, while the Ottoman Empire sent soldiers and cannon to back Adal. The conflict proved, through their use on both sides, the value of firearms such as the matchlock musket, cannon, and the arquebus over traditional weapons.\nOffensive vs defensive use.\nWhile previous smaller guns could burn down structures with fire, larger cannons were so effective that engineers were forced to develop stronger castle walls to prevent their keeps from falling. This isn't to say that cannons were only used to batter down walls as fortifications began using cannons as defensive instruments such as an example in India where the fort of Raicher had gun ports built into its walls to accommodate the use of defensive cannons. In \"Art of War\" Niccol\u00f2 Machiavelli opined that field artillery forced an army to take up a defensive posture and this opposed a more ideal offensive stance. Machiavelli's concerns can be seen in the criticisms of Portuguese mortars being used in India during the sixteenth century as lack of mobility was one of the key problems with the design. In Russia the early cannons were again placed in forts as a defensive tool. Cannon were also difficult to move around in certain types of terrain with mountains providing a great obstacle for them, for these reasons offensives conducted with cannons would be difficult to pull off in places such as Iran.\nEarly modern period.\nBy the 16th century, cannons were made in a great variety of lengths and bore diameters, but the general rule was that the longer the barrel, the longer the range. Some cannons made during this time had barrels exceeding in length, and could weigh up to . Consequently, large amounts of gunpowder were needed to allow them to fire stone balls several hundred yards. By mid-century, European monarchs began to classify cannons to reduce the confusion. Henry II of France opted for six sizes of cannon, but others settled for more; the Spanish used twelve sizes, and the English sixteen. Better powder had been developed by this time as well. Instead of the finely ground powder used by the first bombards, powder was replaced by a \"corned\" variety of coarse grains. This coarse powder had pockets of air between grains, allowing fire to travel through and ignite the entire charge quickly and uniformly.\nThe end of the Middle Ages saw the construction of larger, more powerful cannon, as well as their spread throughout the world. As they were not effective at breaching the newer fortifications resulting from the development of cannon, siege engines\u2014such as siege towers and trebuchets\u2014became less widely used. However, wooden \"battery-towers\" took on a similar role as siege towers in the gunpowder age\u2014such as that used at Siege of Kazan in 1552, which could hold ten large-calibre cannon, in addition to 50 lighter pieces. Another notable effect of cannon on warfare during this period was the change in conventional fortifications. Niccol\u00f2 Machiavelli wrote, \"There is no wall, whatever its thickness that artillery will not destroy in only a few days.\" Although castles were not immediately made obsolete by cannon, their use and importance on the battlefield rapidly declined. Instead of majestic towers and merlons, the walls of new fortresses were thick, angled, and sloped, while towers became low and stout; increasing use was also made of earth and brick in breastworks and redoubts. These new defences became known as bastion forts, after their characteristic shape which attempted to force any advance towards it directly into the firing line of the guns. A few of these featured cannon batteries, such as the House of Tudor's Device Forts, in England. Bastion forts soon replaced castles in Europe, and, eventually, those in the Americas, as well.\nBy the end of the 15th century, several technological advancements made cannons more mobile. Wheeled gun carriages and trunnions became common, and the invention of the limber further facilitated transportation. As a result, field artillery became more viable, and began to see more widespread use, often alongside the larger cannons intended for sieges. Better gunpowder, cast-iron projectiles (replacing stone), and the standardisation of calibres meant that even relatively light cannons could be deadly. In \"The Art of War\", Niccol\u00f2 Machiavelli observed that \"It is true that the arquebuses and the small artillery do much more harm than the heavy artillery.\" This was the case at the Battle of Flodden, in 1513: the English field guns outfired the Scottish siege artillery, firing two or three times as many rounds. Despite the increased maneuverability, however, cannon were still the slowest component of the army: a heavy English cannon required 23 horses to transport, while a culverin needed nine. Even with this many animals pulling, they still moved at a walking pace. Due to their relatively slow speed, and lack of organisation, and undeveloped tactics, the combination of pike and shot still dominated the battlefields of Europe.\nInnovations continued, notably the German invention of the mortar, a thick-walled, short-barrelled gun that blasted shot upward at a steep angle. Mortars were useful for sieges, as they could hit targets behind walls or other defences. This cannon found more use with the Dutch, who learnt to shoot bombs filled with powder from them. Setting the bomb fuse was a problem. \"Single firing\" was first used to ignite the fuse, where the bomb was placed with the fuse down against the cannon's propellant. This often resulted in the fuse being blown into the bomb, causing it to blow up as it left the mortar. Because of this, \"double firing\" was tried where the gunner lit the fuse and then the touch hole. This, however, required considerable skill and timing, and was especially dangerous if the gun misfired, leaving a lighted bomb in the barrel. Not until 1650 was it accidentally discovered that double-lighting was superfluous as the heat of firing would light the fuse.\nGustavus Adolphus of Sweden emphasised the use of light cannon and mobility in his army, and created new formations and tactics that revolutionised artillery. He discontinued using all 12 pounder\u2014or heavier\u2014cannon as field artillery, preferring, instead, to use cannons that could be handled by only a few men. One obsolete type of gun, the \"leatheren\" was replaced by 4 pounder and 9 pounder demi-culverins. These could be operated by three men, and pulled by only two horses. Gustavus Adolphus's army was also the first to use a cartridge that contained both powder and shot which sped up reloading, increasing the rate of fire. Finally, against infantry he pioneered the use of canister shot\u2014essentially a tin can filled with musket balls. Until then there was no more than one cannon for every thousand infantrymen on the battlefield but Gustavus Adolphus increased the number of cannons sixfold. Each regiment was assigned two pieces, though he often arranged them into batteries instead of distributing them piecemeal. He used these batteries to break his opponent's infantry line, while his cavalry would outflank their heavy guns.\nAt the Battle of Breitenfeld, in 1631, Adolphus proved the effectiveness of the changes made to his army, by defeating Johann Tserclaes, Count of Tilly. Although severely outnumbered, the Swedes were able to fire between three and five times as many volleys of artillery, and their infantry's linear formations helped ensure they didn't lose any ground. Battered by cannon fire, and low on morale, Tilly's men broke ranks and fled.\nIn England cannons were being used to besiege various fortified buildings during the English Civil War. Nathaniel Nye is recorded as testing a Birmingham cannon in 1643 and experimenting with a saker in 1645. From 1645 he was the master gunner to the Parliamentarian garrison at Evesham and in 1646 he successfully directed the artillery at the Siege of Worcester, detailing his experiences and in his 1647 book \"The Art of Gunnery\". Believing that war was as much a science as an art, his explanations focused on triangulation, arithmetic, theoretical mathematics, and cartography as well as practical considerations such as the ideal specification for gunpowder or slow matches. His book acknowledged mathematicians such as Robert Recorde and Marcus Jordanus as well as earlier military writers on artillery such as Niccol\u00f2 Fontana Tartaglia and Thomas (or Francis) Malthus (author of \"A Treatise on Artificial Fire-Works\").\nAround this time also came the idea of aiming the cannon to hit a target. Gunners controlled the range of their cannons by measuring the angle of elevation, using a \"gunner's quadrant.\" Cannons did not have sights, therefore, even with measuring tools, aiming was still largely guesswork.\nIn the latter half of the 17th century, the French engineer S\u00e9bastien Le Prestre de Vauban introduced a more systematic and scientific approach to attacking gunpowder fortresses, in a time when many field commanders \"were notorious dunces in siegecraft.\" Careful sapping forward, supported by enfilading ricochets, was a key feature of this system, and it even allowed Vauban to calculate the length of time a siege would take. He was also a prolific builder of bastion forts, and did much to popularize the idea of \"depth in defence\" in the face of cannon. These principles were followed into the mid-19th century, when changes in armaments necessitated greater depth defence than Vauban had provided for. It was only in the years prior to World War I that new works began to break radically away from his designs.\n18th and 19th centuries.\nThe lower tier of 17th-century English ships of the line were usually equipped with demi-cannons, guns that fired a solid shot, and could weigh up to . Demi-cannons were capable of firing these heavy metal balls with such force that they could penetrate more than a metre of solid oak, from a distance of , and could dismast even the largest ships at close range. Full cannon fired a shot, but were discontinued by the 18th century, as they were too unwieldy. By the end of the 18th century, principles long adopted in Europe specified the characteristics of the Royal Navy's cannon, as well as the acceptable defects, and their severity. The United States Navy tested guns by measuring them, firing them two or three times\u2014termed \"proof by powder\"\u2014and using pressurized water to detect leaks.\nThe carronade was adopted by the Royal Navy in 1779; the lower muzzle velocity of the round shot when fired from this cannon was intended to create more wooden splinters when hitting the structure of an enemy vessel, as they were believed to be more deadly than the ball by itself. The carronade was much shorter, and weighed between a third to a quarter of the equivalent long gun; for example, a 32-pounder carronade weighed less than a ton, compared with a 32-pounder long gun, which weighed over 3 tons. The guns were, therefore, easier to handle, and also required less than half as much gunpowder, allowing fewer men to crew them. Carronades were manufactured in the usual naval gun calibres, but were not counted in a ship of the line's rated number of guns. As a result, the classification of Royal Navy vessels in this period can be misleading, as they often carried more cannons than were listed.\nCannons were crucial in Napoleon's rise to power, and continued to play an important role in his army in later years. During the French Revolution, the unpopularity of the Directory led to riots and rebellions. When over 25,000 royalists led by General Danican assaulted Paris, Paul Barras was appointed to defend the capital; outnumbered five to one and disorganised, the Republicans were desperate. When Napoleon arrived, he reorganised the defences but realised that without cannons the city could not be held. He ordered Joachim Murat to bring the guns from the Sablons artillery park; the Major and his cavalry fought their way to the recently captured cannons, and brought them back to Napoleon. When Danican's poorly trained men attacked, on 13 Vend\u00e9miaire, 1795 \u2013 5 October 1795, in the calendar used in France at the time\u2014Napoleon ordered his cannon to fire grapeshot into the mob, an act that became known as the \"whiff of grapeshot\". The slaughter effectively ended the threat to the new government, while, at the same time, made Bonaparte a famous\u2014and popular\u2014public figure. Among the first generals to recognise that artillery was not being used to its full potential, Napoleon often massed his cannon into batteries and introduced several changes into the French artillery, improving it significantly and making it among the finest in Europe. Such tactics were successfully used by the French, for example, at the Battle of Friedland, when sixty-six guns fired a total of 3,000 roundshot and 500 rounds of grapeshot, inflicting severe casualties to the Russian forces, whose losses numbered over 20,000 killed and wounded, in total. At the Battle of Waterloo\u2014Napoleon's final battle\u2014the French army had many more artillery pieces than either the British or Prussians. As the battlefield was muddy, recoil caused cannons to bury themselves into the ground after firing, resulting in slow rates of fire, as more effort was required to move them back into an adequate firing position; also, roundshot did not ricochet with as much force from the wet earth. Despite the drawbacks, sustained artillery fire proved deadly during the engagement, especially during the French cavalry attack. The British infantry, having formed infantry squares, took heavy losses from the French guns, while their own cannons fired at the cuirassiers and lancers, when they fell back to regroup. Eventually, the French ceased their assault, after taking heavy losses from the British cannon and musket fire.\nIn the 1810s and 1820s, greater emphasis was placed on the accuracy of long-range gunfire, and less on the weight of a broadside. The carronade, although initially very successful and widely adopted, disappeared from the Royal Navy in the 1850s after the development of wrought-iron-jacketed steel cannon by William Armstrong and Joseph Whitworth. Nevertheless, carronades were used in the American Civil War.\nWestern cannons during the 19th century became larger, more destructive, more accurate, and could fire at longer range. One example is the American wrought-iron, muzzle-loading rifle, or Griffen gun (usually called the 3-inch Ordnance Rifle), used during the American Civil War, which had an effective range of over . Another is the smoothbore 12-pounder Napoleon, which originated in France in 1853 and was widely used by both sides in the American Civil War. This cannon was renowned for its sturdiness, reliability, firepower, flexibility, relatively lightweight, and range of .\nThe practice of rifling\u2014casting spiralling lines inside the cannon's barrel\u2014was applied to artillery more frequently by 1855, as it gave cannon projectiles gyroscopic stability, which improved their accuracy. One of the earliest rifled cannons was the breech-loading Armstrong Gun\u2014also invented by William Armstrong\u2014which boasted significantly improved range, accuracy, and power than earlier weapons. The projectile fired from the Armstrong gun could reportedly pierce through a ship's side and explode inside the enemy vessel, causing increased damage and casualties. The British military adopted the Armstrong gun, and was impressed; the Duke of Cambridge even declared that it \"could do everything but speak.\" Despite being significantly more advanced than its predecessors, the Armstrong gun was rejected soon after its integration, in favour of the muzzle-loading pieces that had been in use before. While both types of gun were effective against wooden ships, neither had the capability to pierce the armour of ironclads; due to reports of slight problems with the breeches of the Armstrong gun, and their higher cost, the older muzzle-loaders were selected to remain in service instead. Realising that iron was more difficult to pierce with breech-loaded cannons, Armstrong designed rifled muzzle-loading guns, which proved successful; \"The Times\" reported: \"even the fondest believers in the invulnerability of our present ironclads were obliged to confess that against such artillery, at such ranges, their plates and sides were almost as penetrable as wooden ships.\"\nThe superior cannon of the Western world brought them tremendous advantages in warfare. For example, in the First Opium War in China, during the 19th century, British battleships bombarded the coastal areas and fortifications from afar, safe from the reach of the Chinese cannons. Similarly, the shortest war in recorded history, the Anglo-Zanzibar War of 1896, was brought to a swift conclusion by shelling from British cruisers. The cynical attitude towards recruited infantry in the face of ever more powerful field artillery is the source of the term \"cannon fodder\", first used by Fran\u00e7ois-Ren\u00e9 de Chateaubriand, in 1814; however, the concept of regarding soldiers as nothing more than \"food for powder\" was mentioned by William Shakespeare as early as 1598, in Henry IV, Part 1.\n20th and 21st centuries.\nCannons in the 20th and 21st centuries are usually divided into sub-categories and given separate names. Some of the most widely used types of modern cannon are howitzers, mortars, guns, and autocannon, although a few very large-calibre cannon, custom-designed, have also been constructed. Nuclear artillery was experimented with, but was abandoned as impractical. Modern artillery is used in a variety of roles, depending on its type. According to NATO, the general role of artillery is to provide fire support, which is defined as \"the application of fire, coordinated with the manoeuvre of forces to destroy, neutralize, or suppress the enemy.\"\nWhen referring to cannons, the term \"gun\" is often used incorrectly. In military usage, a gun is a cannon with a high muzzle velocity and a flat trajectory, useful for hitting the sides of targets such as walls, as opposed to howitzers or mortars, which have lower muzzle velocities, and fire indirectly, lobbing shells up and over obstacles to hit the target from above.\nBy the early 20th century, infantry weapons had become more powerful, forcing most artillery away from the front lines. Despite the change to indirect fire, cannons proved highly effective during World War I, directly or indirectly causing over 75% of casualties. The onset of trench warfare after the first few months of World War I greatly increased the demand for howitzers, as they were more suited at hitting targets in trenches. Furthermore, their shells carried more explosives than those of guns, and caused considerably less barrel wear. The German army had the advantage here as they began the war with many more howitzers than the French. World War I also saw the use of the Paris Gun, the longest-ranged gun ever fired. This calibre gun was used by the Germans against Paris and could hit targets more than away.\nThe Second World War sparked new developments in cannon technology. Among them were sabot rounds, hollow-charge projectiles, and proximity fuses, all of which increased the effectiveness of cannon against specific target. The proximity fuse emerged on the battlefields of Europe in late December 1944. Used to great effect in anti-aircraft projectiles, proximity fuses were fielded in both the European and Pacific Theatres of Operations; they were particularly useful against V-1 flying bombs and kamikaze planes. Although widely used in naval warfare, and in anti-air guns, both the British and Americans feared unexploded proximity fuses would be reverse engineered leading to them limiting its use in continental battles. During the Battle of the Bulge, however, the fuses became known as the American artillery's \"Christmas present\" for the German army because of their effectiveness against German personnel in the open, when they frequently dispersed attacks. Anti-tank guns were also tremendously improved during the war: in 1939, the British used primarily 2 pounder and 6 pounder guns. By the end of the war, 17 pounders had proven much more effective against German tanks, and 32 pounders had entered development. Meanwhile, German tanks were continuously upgraded with better main guns, in addition to other improvements. For example, the Panzer III was originally designed with a 37\u00a0mm gun, but was mass-produced with a 50\u00a0mm cannon. To counter the threat of the Russian T-34s, another, more powerful 50\u00a0mm gun was introduced, only to give way to a larger 75\u00a0mm cannon, which was in a fixed mount as the StuG III, the most-produced German World War II armoured fighting vehicle of any type. Despite the improved guns, production of the Panzer III was ended in 1943, as the tank still could not match the T-34, and was replaced by the Panzer IV and Panther tanks. In 1944, the 8.8\u00a0cm KwK 43 and many variations, entered service with the Wehrmacht, and was used as both a tank main gun, and as the PaK 43 anti-tank gun. One of the most powerful guns to see service in World War II, it was capable of destroying any Allied tank at very long ranges.\nDespite being designed to fire at trajectories with a steep angle of descent, howitzers can be fired directly, as was done by the 11th Marine Regiment at the Battle of Chosin Reservoir, during the Korean War. Two field batteries fired directly upon a battalion of Chinese infantry; the Marines were forced to brace themselves against their howitzers, as they had no time to dig them in. The Chinese infantry took heavy casualties, and were forced to retreat.\nThe tendency to create larger calibre cannons during the World Wars has reversed since. The United States Army, for example, sought a lighter, more versatile howitzer, to replace their ageing pieces. As it could be towed, the M198 was selected to be the successor to the World War II\u2013era cannons used at the time, and entered service in 1979. Still in use today, the M198 is, in turn, being slowly replaced by the M777 Ultralightweight howitzer, which weighs nearly half as much and can be more easily moved. Although land-based artillery such as the M198 are powerful, long-ranged, and accurate, naval guns have not been neglected, despite being much smaller than in the past, and, in some cases, having been replaced by cruise missiles. However, the 's planned armament includes the Advanced Gun System (AGS), a pair of 155\u00a0mm guns, which fire the Long Range Land-Attack Projectile. The warhead, which weighs , has a circular error of probability of , and will be mounted on a rocket, to increase the effective range to , further than that of the Paris Gun. The AGS's barrels will be water cooled, and will fire 10 rounds per minute, per gun. The combined firepower from both turrets will give a \"Zumwalt\"-class destroyer the firepower equivalent to 18 conventional M198 howitzers. The reason for the re-integration of cannons as a main armament in United States Navy ships is because satellite-guided munitions fired from a gun are less expensive than a cruise missile but have a similar guidance capability.\nAutocannon.\nAutocannons have an automatic firing mode, similar to that of a machine gun. They have mechanisms to automatically load their ammunition, and therefore have a higher rate of fire than artillery, often approaching, or, in the case of rotary autocannons, even surpassing the firing rate of a machine gun. While there is no minimum bore for autocannons, they are generally larger than machine guns, typically 20\u00a0mm or greater since World War II and are usually capable of using explosive ammunition even if it isn't always used. Machine guns in contrast are usually too small to use explosive ammunition.\nMost nations use rapid-fire cannon on light vehicles, replacing a more powerful, but heavier, tank gun. A typical autocannon is the 25\u00a0mm \"Bushmaster\" chain gun, mounted on the LAV-25 and M2 Bradley armoured vehicles. Autocannons may be capable of a very high rate of fire, but ammunition is heavy and bulky, limiting the amount carried. For this reason, both the 25\u00a0mm Bushmaster and the 30\u00a0mm RARDEN are deliberately designed with relatively low rates of fire. The typical rate of fire for a modern autocannon ranges from 90 to 1,800 rounds per minute. Systems with multiple barrels, such as a rotary autocannon, can have rates of fire of more than several thousand rounds per minute. The fastest of these is the GSh-6-23, which has a rate of fire of over 10,000\u00a0rounds per minute.\nAutocannons are often found in aircraft, where they replaced machine guns and as shipboard anti-aircraft weapons, as they provide greater destructive power than machine guns.\nAircraft use.\nThe first documented installation of a cannon on an aircraft was on the Voisin Canon in 1911, displayed at the Paris Exposition that year.\nBy World War I, all of the major powers were experimenting with aircraft mounted cannons; however their low rate of fire and great size and weight precluded any of them from being anything other than experimental. The most successful (or least unsuccessful) was the SPAD 12 Ca.1 with a single 37mm Puteaux mounted to fire between the cylinder banks and through the propeller boss of the aircraft's Hispano-Suiza 8C. The pilot (by necessity an ace) had to manually reload each round.\nThe first autocannon were developed during World War I as anti-aircraft guns, and one of these\u2014the Coventry Ordnance Works \"COW 37 mm gun\" was installed in an aircraft but the war ended before it could be given a field trial and never became standard equipment in a production aircraft. Later trials had it fixed at a steep angle upwards in both the Vickers Type 161 and the Westland C.O.W. Gun Fighter, an idea that would return later.\nDuring this period autocannons became available and several fighters of the German \"Luftwaffe\" and the Imperial Japanese Navy Air Service were fitted with 20mm cannons. They continued to be installed as an adjunct to machine guns rather than as a replacement, as the rate of fire was still too low and the complete installation too heavy. There was a some debate in the RAF as to whether the greater number of possible rounds being fired from a machine gun, or a smaller number of explosive rounds from a cannon was preferable. Improvements during the war in regards to rate of fire allowed the cannon to displace the machine gun almost entirely. The cannon was more effective against armour so they were increasingly used during the course of World War II, and newer fighters such as the Hawker Tempest usually carried two or four versus the six .50 Browning machine guns for US aircraft or eight to twelve M1919 Browning machine guns on earlier British aircraft. The Hispano-Suiza HS.404, Oerlikon 20\u00a0mm cannon, MG FF, and their numerous variants became among the most widely used autocannon in the war. Cannons, as with machine guns, were generally fixed to fire forwards (mounted in the wings, in the nose or fuselage, or in a pannier under either); or were mounted in gun turrets on heavier aircraft. Both the Germans and Japanese mounted cannons to fire upwards and forwards for use against heavy bombers, with the Germans calling guns so-installed \"Schr\u00e4ge Musik\". Schr\u00e4ge Musik derives from the German colloquialism for Jazz Music (the German word schr\u00e4g means slanted or oblique)\nPreceding the Vietnam War the high speeds aircraft were attaining led to a move to remove the cannon due to the mistaken belief that they would be useless in a dogfight, but combat experience during the Vietnam War showed conclusively that despite advances in missiles, there was still a need for them. Nearly all modern fighter aircraft are armed with an autocannon and they are also commonly found on ground-attack aircraft. One of the most powerful examples is the 30mm GAU-8/A Avenger Gatling-type rotary cannon, mounted exclusively on the Fairchild Republic A-10 Thunderbolt II. The Lockheed AC-130 gunship (a converted transport) can carry a 105mm howitzer as well as a variety of autocannons ranging up to 40mm. Both are used in the close air support role.\nMaterials, parts, and terms.\nCannons in general have the form of a truncated cone with an internal cylindrical bore for holding an explosive charge and a projectile. The thickest, strongest, and closed part of the cone is located near the explosive charge. As any explosive charge will dissipate in all directions equally, the thickest portion of the cannon is useful for containing and directing this force. The backward motion of the cannon as its projectile leaves the bore is termed its recoil, and the effectiveness of the cannon can be measured in terms of how much this response can be diminished, though obviously diminishing recoil through increasing the overall mass of the cannon means decreased mobility.\nField artillery cannon in Europe and the Americas were initially made most often of bronze, though later forms were constructed of cast iron and eventually steel. Bronze has several characteristics that made it preferable as a construction material: although it is relatively expensive, does not always alloy well, and can result in a final product that is \"spongy about the bore\", bronze is more flexible than iron and therefore less prone to bursting when exposed to high pressure; cast-iron cannon are less expensive and more durable generally than bronze and withstand being fired more times without deteriorating. However, cast-iron cannon have a tendency to burst without having shown any previous weakness or wear, and this makes them more dangerous to operate.\nThe older and more-stable forms of cannon were muzzle-loading as opposed to breech-loading\u2014in order to be used they had to have their ordnance packed down the bore through the muzzle rather than inserted through the breech.\nThe following terms refer to the components or aspects of a classical western cannon (c. 1850) as illustrated here. In what follows, the words \"near\", \"close\", and \"behind\" will refer to those parts towards the thick, closed end of the piece, and \"far\", \"front\", \"in front of\", and \"before\" to the thinner, open end.\nSolid spaces.\nThe main body of a cannon consists of three basic extensions: the foremost and the longest is called the \"chase\", the middle portion is the \"reinforce\", and the closest and briefest portion is the \"cascabel\" or \"cascable\".\nTo pack a muzzle-loading cannon, first gunpowder is poured down the bore. This is followed by a layer of wadding (often nothing more than paper), and then the cannonball itself. A certain amount of windage allows the ball to fit down the bore, though the greater the windage the less efficient the propulsion of the ball when the gunpowder is ignited. To fire the cannon, the fuse located in the vent is lit, quickly burning down to the gunpowder, which then explodes violently, propelling wadding and ball down the bore and out of the muzzle. A small portion of exploding gas also escapes through the vent, but this does not dramatically affect the total force exerted on the ball.\nAny large, smoothbore, muzzle-loading gun\u2014used before the advent of breech-loading, rifled guns\u2014may be referred to as a cannon, though once standardised names were assigned to different-sized cannon, the term specifically referred to a gun designed to fire a shot, as distinct from a demi-cannon \u2013 , culverin \u2013 , or demi-culverin \u2013 . \"Gun\" specifically refers to a type of cannon that fires projectiles at high speeds, and usually at relatively low angles; they have been used in warships, and as field artillery. The term \"cannon\" is also used for autocannon, a modern repeating weapon firing explosive projectiles. Cannon have been used extensively in fighter aircraft since World War II.\nOperation.\nIn the 1770s, cannon operation worked as follows: each cannon would be manned by two gunners, six soldiers, and four officers of artillery. The right gunner was to prime the piece and load it with powder, and the left gunner would fetch the powder from the magazine and be ready to fire the cannon at the officer's command. On each side of the cannon, three soldiers stood, to ram and sponge the cannon, and hold the ladle. The second soldier on the left was tasked with providing 50 bullets.\nBefore loading, the cannon would be cleaned with a wet sponge to extinguish any smouldering material from the last shot. Fresh powder could be set off prematurely by lingering ignition sources. The powder was added, followed by wadding of paper or hay, and the ball was placed in and rammed down. After ramming, the cannon would be aimed with the elevation set using a quadrant and a plummet. At 45 degrees, the ball had the utmost range: about ten times the gun's level range. Any angle above a horizontal line was called random-shot. Wet sponges were used to cool the pieces every ten or twelve rounds.\nDuring the Napoleonic Wars, a British gun team consisted of five gunners to aim it, clean the bore with a damp sponge to quench any remaining embers before a fresh charge was introduced, and another to load the gun with a bag of powder and then the projectile. The fourth gunner pressed his thumb on the vent hole, to prevent a draught that might fan a flame. The charge loaded, the fourth would prick the bagged charge through the vent hole, and fill the vent with powder. On command, the fifth gunner would fire the piece with a slow match. Friction primers replaced slow match ignition by the mid-19th century.\nWhen a cannon had to be abandoned such as in a retreat or surrender, the touch hole of the cannon would be plugged flush with an iron spike, disabling the cannon (at least until metal boring tools could be used to remove the plug). This was called \"spiking the cannon\".\nA gun was said to be \"honeycombed\" when the surface of the bore had cavities, or holes in it, caused either by corrosion or casting defects.\nDeceptive use.\nHistorically, logs or poles have been used as decoys to mislead the enemy as to the strength of an emplacement. The \"Quaker Gun trick\" was used by Colonel William Washington's Continental Army during the American Revolutionary War; in 1780, approximately 100 Loyalists surrendered to them, rather than face bombardment. During the American Civil War, Quaker guns were also used by the Confederates, to compensate for their shortage of artillery. The decoy cannon were painted black at the \"muzzle\", and positioned behind fortifications to delay Union attacks on those positions. On occasion, real gun carriages were used to complete the deception.\nIn popular culture.\nCannon sounds have sometimes been used in classical pieces with a military theme. One of the best known examples of such a piece is Pyotr Ilyich Tchaikovsky's \"1812 Overture\". The overture is to be performed using an artillery section together with the orchestra, resulting in noise levels high enough that musicians are required to wear ear protection. The cannon fire simulates Russian artillery bombardments of the Battle of Borodino, a critical battle in Napoleon's invasion of Russia, whose defeat the piece celebrates. When the overture was first performed, the cannon were fired by an electric current triggered by the conductor. However, the overture was not recorded with real cannon fire until Mercury Records and conductor Antal Dor\u00e1ti's 1958 recording of the Minnesota Orchestra. Cannon fire is also frequently used annually in presentations of the \"1812\" on the American Independence Day, a tradition started by Arthur Fiedler of the Boston Pops in 1974.\nThe hard rock band AC/DC also used cannon in their song \"For Those About to Rock (We Salute You)\", and in live shows replica Napoleonic cannon and pyrotechnics were used to perform the piece.\nRestoration.\nCannon recovered from the sea are often extensively damaged from exposure to salt water; because of this, electrolytic reduction treatment is required to forestall the process of corrosion. The cannon is then washed in deionized water to remove the electrolyte, and is treated in tannic acid, which prevents further rust and gives the metal a bluish-black colour. After this process, cannon on display may be protected from oxygen and moisture by a wax sealant. A coat of polyurethane may also be painted over the wax sealant, to prevent the wax-coated cannon from attracting dust in outdoor displays. In 2011, archaeologists say six cannon recovered from a river in Panama that could have belonged to legendary pirate Henry Morgan are being studied and could eventually be displayed after going through a restoration process."}
{"id": "7056", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=7056", "title": "Computer mouse", "text": "A computer mouse (plural mice, rarely mouses) is a hand-held pointing device that detects two-dimensional motion relative to a surface. This motion is typically translated into the motion of a pointer on a display, which allows a smooth control of the graphical user interface of a computer.\nThe first public demonstration of a mouse controlling a computer system was in 1968. Mice originally used a ball rolling on a surface to detect motion, but modern mice often have optical sensors that have no moving parts. Originally wired to a computer, many modern mice are cordless, relying on short-range radio communication with the connected system.\nIn addition to moving a cursor, computer mice have one or more buttons to allow operations such as selection of a menu item on a display. Mice often also feature other elements, such as touch surfaces and scroll wheels, which enable additional control and dimensional input.\nEtymology.\nThe earliest known written use of the term \"mouse\" in reference to a computer pointing device is in Bill English's July 1965 publication, \"Computer-Aided Display Control\" likely originating from its resemblance to the shape and size of a mouse, a rodent, with the cord resembling its tail. The popularity of wireless mice without cords makes the resemblance less obvious.\nThe plural for the small rodent is always \"mice\" in modern usage. The plural for a computer mouse is either \"mice\" or \"mouses\" according to most dictionaries, with \"mice\" being more common. The first recorded plural usage is \"mice\"; the online \"Oxford Dictionaries\" cites a 1984 use, and earlier uses include J. C. R. Licklider's \"The Computer as a Communication Device\" of 1968.\nHistory.\nThe trackball, a related pointing device, was invented in 1946 by Ralph Benjamin as part of a post-World War II-era fire-control radar plotting system called the Comprehensive Display System (CDS). Benjamin was then working for the British Royal Navy Scientific Service. Benjamin's project used analog computers to calculate the future position of target aircraft based on several initial input points provided by a user with a joystick. Benjamin felt that a more elegant input device was needed and invented what they called a \"roller ball\" for this purpose.\nThe device was patented in 1947, but only a prototype using a metal ball rolling on two rubber-coated wheels was ever built, and the device was kept as a military secret.\nAnother early trackball was built by Kenyon Taylor, a British electrical engineer working in collaboration with Tom Cranston and Fred Longstaff. Taylor was part of the original Ferranti Canada, working on the Royal Canadian Navy's DATAR (Digital Automated Tracking and Resolving) system in 1952.\nDATAR was similar in concept to Benjamin's display. The trackball used four disks to pick up motion, two each for the X and Y directions. Several rollers provided mechanical support. When the ball was rolled, the pickup discs spun and contacts on their outer rim made periodic contact with wires, producing pulses of output with each movement of the ball. By counting the pulses, the physical movement of the ball could be determined. A digital computer calculated the tracks and sent the resulting data to other ships in a task force using pulse-code modulation radio signals. This trackball used a standard Canadian five-pin bowling ball. It was not patented, since it was a secret military project.\nDouglas Engelbart of the Stanford Research Institute (now SRI International) has been credited in published books by Thierry Bardini, Paul Ceruzzi, Howard Rheingold, and several others as the inventor of the computer mouse. Engelbart was also recognized as such in various obituary titles after his death in July 2013.\nBy 1963, Engelbart had already established a research lab at SRI, the Augmentation Research Center (ARC), to pursue his objective of developing both hardware and software computer technology to \"augment\" human intelligence. That November, while attending a conference on computer graphics in Reno, Nevada, Engelbart began to ponder how to adapt the underlying principles of the planimeter to inputting X- and Y-coordinate data. On November 14, 1963, he first recorded his thoughts in his personal notebook about something he initially called a \"bug,\" which in a \"3-point\" form could have a \"drop point and 2 orthogonal wheels.\" He wrote that the \"bug\" would be \"easier\" and \"more natural\" to use, and unlike a stylus, it would stay still when let go, which meant it would be \"much better for coordination with the keyboard.\"\nIn 1964, Bill English joined ARC, where he helped Engelbart build the first mouse prototype. They christened the device the \"mouse\" as early models had a cord attached to the rear part of the device which looked like a tail, and in turn resembled the common mouse. As noted above, this \"mouse\" was first mentioned in print in a July 1965 report, on which English was the lead author. On 9 December 1968, Engelbart publicly demonstrated the mouse at what would come to be known as The Mother of All Demos. Engelbart never received any royalties for it, as his employer SRI held the patent, which expired before the mouse became widely used in personal computers. In any event, the invention of the mouse was just a small part of Engelbart's much larger project of augmenting human intellect.\nSeveral other experimental pointing-devices developed for Engelbart's oN-Line System (NLS) exploited different body movements \u2013\u00a0for example, head-mounted devices attached to the chin or nose\u00a0\u2013 but ultimately the mouse won out because of its speed and convenience. The first mouse, a bulky device (pictured) used two potentiometers perpendicular to each other and connected to wheels: the rotation of each wheel translated into motion along one axis. At the time of the \"Mother of All Demos\", Engelbart's group had been using their second generation, 3-button mouse for about a year.\nOn October 2, 1968, a mouse device named ' (German for \"rolling ball\") was described as an optional device for its SIG-100 terminal. It was developed by the German company Telefunken. As the name suggests and unlike Engelbart's mouse, the Telefunken model already had a ball. It was based on an earlier trackball-like device (also named ') that was embedded into radar flight control desks. This trackball had been developed by a team led by Rainer Mallebrein at Telefunken for the German \"Bundesanstalt f\u00fcr Flugsicherung (Federal Air Traffic Control)\" as part of their TR\u00a086 process computer system with its SIG\u00a0100-86 vector graphics terminal.\nWhen the development for the Telefunken main frame began in 1965, Mallebrein and his team came up with the idea of \"reversing\" the existing into a moveable mouse-like device, so that customers did not have to be bothered with mounting holes for the earlier trackball device. Together with light pens and trackballs, it was offered as an optional input device for their system since 1968. Footage exists from January 1969, filmed at Abbey Road studios, of Ringo Starr holding what appears to be a mouse, possibly using it as a remote, to start or stop a recording machine. Some Rollkugel mouses installed at the in Munich in 1972 are well preserved in a museum. Telefunken considered the invention too unimportant to apply for a patent on it.\nThe Xerox Alto was one of the first computers designed for individual use in 1973 and is regarded as the first modern computer to utilize a mouse. Inspired by PARC's Alto, the Lilith, a computer which had been developed by a team around at ETH Z\u00fcrich between 1978 and 1980, provided a mouse as well. The third marketed version of an integrated mouse shipped as a part of a computer and intended for personal computer navigation came with the Xerox 8010 Star in 1981.\nBy 1982, the Xerox 8010 was probably the best-known computer with a mouse. The Sun-1 also came with a mouse, and the forthcoming Apple Lisa was rumored to use one, but the peripheral remained obscure; Jack Hawley of The Mouse House reported that one buyer for a large organization believed at first that his company sold lab mice. Hawley, who manufactured mice for Xerox, stated that \"Practically, I have the market all to myself right now\"; a Hawley mouse cost $415. In 1982, Logitech introduced the P4 Mouse at the Comdex trade show in Las Vegas, its first hardware mouse. That same year Microsoft made the decision to make the MS-DOS program Microsoft Word mouse-compatible, and developed the first PC-compatible mouse. Microsoft's mouse shipped in 1983, thus beginning the Microsoft Hardware division of the company. However, the mouse remained relatively obscure until the appearance of the Macintosh 128K (which included an updated version of the single-button Lisa Mouse) in 1984, and of the Amiga 1000 and the Atari ST in 1985.\nOperation.\nA mouse typically controls the motion of a pointer in two dimensions in a graphical user interface (GUI). The mouse turns movements of the hand backward and forward, left and right into equivalent electronic signals that in turn are used to move the pointer.\nThe relative movements of the mouse on the surface are applied to the position of the pointer on the screen, which signals the point where actions of the user take place, so hand movements are replicated by the pointer. Clicking or hovering (stopping movement while the cursor is within the bounds of an area) can select files, programs or actions from a list of names, or (in graphical interfaces) through small images called \"icons\" and other elements. For example, a text file might be represented by a picture of a paper notebook and clicking while the cursor hovers this icon might cause a text editing program to open the file in a window.\nDifferent ways of operating the mouse cause specific things to happen in the GUI:\nGestures.\nUsers can also employ mice \"gesturally\"; meaning that a stylized motion of the mouse cursor itself, called a \"gesture\", can issue a command or map to a specific action. For example, in a drawing program, moving the mouse in a rapid \"x\" motion over a shape might delete the shape.\nGestural interfaces occur more rarely than plain pointing-and-clicking; and people often find them more difficult to use, because they require finer motor control from the user. However, a few gestural conventions have become widespread, including the drag and drop gesture, in which:\nFor example, a user might drag-and-drop a picture representing a file onto a picture of a trash can, thus instructing the system to delete the file.\nStandard semantic gestures include:\nSpecific uses.\nOther uses of the mouse's input occur commonly in special application-domains. In interactive three-dimensional graphics, the mouse's motion often translates directly into changes in the virtual objects' or camera's orientation. For example, in the first-person shooter genre of games (see below), players usually employ the mouse to control the direction in which the virtual player's \"head\" faces: moving the mouse up will cause the player to look up, revealing the view above the player's head. A related function makes an image of an object rotate, so that all sides can be examined. 3D design and animation software often modally chords many different combinations to allow objects and cameras to be rotated and moved through space with the few axes of movement mice can detect.\nWhen mice have more than one button, the software may assign different functions to each button. Often, the primary (leftmost in a right-handed configuration) button on the mouse will select items, and the secondary (rightmost in a right-handed) button will bring up a menu of alternative actions applicable to that item. For example, on platforms with more than one button, the Mozilla web browser will follow a link in response to a primary button click, will bring up a contextual menu of alternative actions for that link in response to a secondary-button click, and will often open the link in a new tab or window in response to a click with the tertiary (middle) mouse button.\nTypes.\nMechanical mice.\nThe German company Telefunken published on their early ball mouse on 2 October 1968. Telefunken's mouse was sold as optional equipment for their computer systems. Bill English, builder of Engelbart's original mouse, created a ball mouse in 1972 while working for Xerox PARC.\nThe ball mouse replaced the external wheels with a single ball that could rotate in any direction. It came as part of the hardware package of the Xerox Alto computer. Perpendicular chopper wheels housed inside the mouse's body chopped beams of light on the way to light sensors, thus detecting in their turn the motion of the ball. This variant of the mouse resembled an inverted trackball and became the predominant form used with personal computers throughout the 1980s and 1990s. The Xerox PARC group also settled on the modern technique of using both hands to type on a full-size keyboard and grabbing the mouse when required.\nThe ball mouse has two freely rotating rollers. These are located 90 degrees apart. One roller detects the forward\u2013backward motion of the mouse and other the left\u2013right motion. Opposite the two rollers is a third one (white, in the photo, at 45 degrees) that is spring-loaded to push the ball against the other two rollers. Each roller is on the same shaft as an encoder wheel that has slotted edges; the slots interrupt infrared light beams to generate electrical pulses that represent wheel movement. Each wheel's disc has a pair of light beams, located so that a given beam becomes interrupted or again starts to pass light freely when the other beam of the pair is about halfway between changes.\nSimple logic circuits interpret the relative timing to indicate which direction the wheel is rotating. This incremental rotary encoder scheme is sometimes called quadrature encoding of the wheel rotation, as the two optical sensors produce signals that are in approximately quadrature phase. The mouse sends these signals to the computer system via the mouse cable, directly as logic signals in very old mice such as the Xerox mice, and via a data-formatting IC in modern mice. The driver software in the system converts the signals into motion of the mouse cursor along X and Y axes on the computer screen.\nThe ball is mostly steel, with a precision spherical rubber surface. The weight of the ball, given an appropriate working surface under the mouse, provides a reliable grip so the mouse's movement is transmitted accurately. Ball mice and wheel mice were manufactured for Xerox by Jack Hawley, doing business as The Mouse House in Berkeley, California, starting in 1975. Based on another invention by Jack Hawley, proprietor of the Mouse House, Honeywell produced another type of mechanical mouse. Instead of a ball, it had two wheels rotating at off axes. Key Tronic later produced a similar product.\nModern computer mice took form at the \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL) under the inspiration of Professor Jean-Daniel Nicoud and at the hands of engineer and watchmaker Andr\u00e9 Guignard. This new design incorporated a single hard rubber mouseball and three buttons, and remained a common design until the mainstream adoption of the scroll-wheel mouse during the 1990s. In 1985, Ren\u00e9 Sommer added a microprocessor to Nicoud's and Guignard's design. Through this innovation, Sommer is credited with inventing a significant component of the mouse, which made it more \"intelligent\"; though optical mice from Mouse Systems had incorporated microprocessors by 1984.\nAnother type of mechanical mouse, the \"analog mouse\" (now generally regarded as obsolete), uses potentiometers rather than encoder wheels, and is typically designed to be plug compatible with an analog joystick. The \"Color Mouse\", originally marketed by RadioShack for their Color Computer (but also usable on MS-DOS machines equipped with analog joystick ports, provided the software accepted joystick input) was the best-known example.\nOptical and laser mice.\nEarly optical mice relied entirely on one or more light-emitting diodes (LEDs) and an imaging array of photodiodes to detect movement relative to the underlying surface, eschewing the internal moving parts a mechanical mouse uses in addition to its optics. A laser mouse is an optical mouse that uses coherent (laser) light.\nThe earliest optical mice detected movement on pre-printed mousepad surfaces, whereas the modern LED optical mouse works on most opaque diffuse surfaces; it is usually unable to detect movement on specular surfaces like polished stone. Laser diodes provide good resolution and precision, improving performance on opaque specular surfaces. Later, more surface-independent optical mice use an optoelectronic sensor (essentially, a tiny low-resolution video camera) to take successive images of the surface on which the mouse operates. Battery powered, wireless optical mice flash the LED intermittently to save power, and only glow steadily when movement is detected.\nInertial and gyroscopic mice.\nOften called \"air mice\" since they do not require a surface to operate, inertial mice use a tuning fork or other accelerometer (US Patent 4787051) to detect rotary movement for every axis supported. The most common models (manufactured by Logitech and Gyration) work using 2 degrees of rotational freedom and are insensitive to spatial translation. The user requires only small wrist rotations to move the cursor, reducing user fatigue or \"gorilla arm\".\nUsually cordless, they often have a switch to deactivate the movement circuitry between use, allowing the user freedom of movement without affecting the cursor position. A patent for an inertial mouse claims that such mice consume less power than optically based mice, and offer increased sensitivity, reduced weight and increased ease-of-use. In combination with a wireless keyboard an inertial mouse can offer alternative ergonomic arrangements which do not require a flat work surface, potentially alleviating some types of repetitive motion injuries related to workstation posture.\n3D mice.\nAlso known as bats, flying mice, or wands, these devices generally function through ultrasound and provide at least three degrees of freedom. Probably the best known example would be 3Dconnexion (\"Logitech's SpaceMouse\") from the early 1990s. In the late 1990s Kantek introduced the 3D RingMouse. This wireless mouse was worn on a ring around a finger, which enabled the thumb to access three buttons. The mouse was tracked in three dimensions by a base station. Despite a certain appeal, it was finally discontinued because it did not provide sufficient resolution.\nOne example of a 2000s consumer 3D pointing device is the Wii Remote. While primarily a motion-sensing device (that is, it can determine its orientation and direction of movement), Wii Remote can also detect its spatial position by comparing the distance and position of the lights from the IR emitter using its integrated IR camera (since the nunchuk accessory lacks a camera, it can only tell its current heading and orientation). The obvious drawback to this approach is that it can only produce spatial coordinates while its camera can see the sensor bar. More accurate consumer devices have since been released, including the PlayStation Move, the Razer Hydra and the controllers part of the HTC Vive virtual reality system. All of these devices can accurately detect position and orientation in 3D space regardless of angle relative to the sensor station.\nA mouse-related controller called the SpaceBall has a ball placed above the work surface that can easily be gripped. With spring-loaded centering, it sends both translational as well as angular displacements on all six axes, in both directions for each. In November 2010 a German Company called Axsotic introduced a new concept of 3D mouse called 3D Spheric Mouse. This new concept of a true six degree-of-freedom input device uses a ball to rotate in 3 axes without any limitations.\nTactile mice.\nIn 2000, Logitech introduced a \"tactile mouse\" known as the \"iFeel Mouse\" developed by Immersion Corporation that contained a small actuator to enable the mouse to generate simulated physical sensations. Such a mouse can augment user-interfaces with haptic feedback, such as giving feedback when crossing a window boundary. To surf the internet by touch-enabled mouse was first developed in1996 and first implemented commercially by the Wingman Force Feedback Mouse. It requires the user to be able to feel depth or hardness; this ability was realized with the first electrorheological tactile mice but never marketed.\nPucks.\nTablet digitizers are sometimes used with accessories called pucks, devices which rely on absolute positioning, but can be configured for sufficiently mouse-like relative tracking that they are sometimes marketed as mice.\nErgonomic mice.\nAs the name suggests, this type of mouse is intended to provide optimum comfort and avoid injuries such as carpal tunnel syndrome, arthritis and other repetitive strain injuries. It is designed to fit natural hand position and movements, to reduce discomfort.\nWhen holding a typical mouse, the ulna and radius bones on the arm are crossed. Some designs attempt to place the palm more vertically, so the bones take more natural parallel position. Some limit wrist movement, encouraging arm movement instead, that may be less precise but more optimal from the health point of view. A mouse may be angled from the thumb downward to the opposite side \u2013 this is known to reduce wrist pronation. However such optimizations make the mouse right or left hand specific, making more problematic to change the tired hand. \"Time\" has criticized manufacturers for offering few or no left-handed ergonomic mice: \"Oftentimes I felt like I was dealing with someone who\u2019d never actually met a left-handed person before.\"\nAnother solution is a pointing bar device. The so-called \"roller bar mouse\" is positioned snugly in front of the keyboard, thus allowing bi-manual accessibility.\nGaming mice.\nThese mice are specifically designed for use in computer games. They typically employ a wider array of controls and buttons and have designs that differ radically from traditional mice. They may also have decorative monochrome or programmable RGB LED lighting. The additional buttons can often be used for changing the sensitivity of the mouse or they can be assigned (programmed) to macros (i.e., for opening a program or for use instead of a key combination) It is also common for game mice, especially those designed for use in real-time strategy games such as \"StarCraft\", or in multiplayer online battle arena games such as \"Dota 2\" to have a relatively high sensitivity, measured in dots per inch (DPI), which can be as high as 25,600. Some advanced mice from gaming manufacturers also allow users to adjust the weight of the mouse by adding or subtracting weights to allow for easier control. Ergonomic quality is also an important factor in gaming mice, as extended gameplay times may render further use of the mouse to be uncomfortable. Some mice have been designed to have adjustable features such as removable and/or elongated palm rests, horizontally adjustable thumb rests and pinky rests. Some mice may include several different rests with their products to ensure comfort for a wider range of target consumers. Gaming mice are held by gamers in three styles of grip:\nConnectivity and communication protocols.\nTo transmit their input, typical cabled mice use a thin electrical cord terminating in a standard connector, such as RS-232C, PS/2, ADB or USB. Cordless mice instead transmit data via infrared radiation (see IrDA) or radio (including Bluetooth), although many such cordless interfaces are themselves connected through the aforementioned wired serial buses.\nWhile the electrical interface and the format of the data transmitted by commonly available mice is currently standardized on USB, in the past it varied between different manufacturers. A bus mouse used a dedicated interface card for connection to an IBM PC or compatible computer.\nMouse use in DOS applications became more common after the introduction of the Microsoft Mouse, largely because Microsoft provided an open standard for communication between applications and mouse driver software. Thus, any application written to use the Microsoft standard could use a mouse with a driver that implements the same API, even if the mouse hardware itself was incompatible with Microsoft's. This driver provides the state of the buttons and the distance the mouse has moved in units that its documentation calls \"mickeys\",\nEarly mice.\nIn the 1970s, the Xerox Alto mouse, and in the 1980s the Xerox optical mouse, used a quadrature-encoded X and Y interface. This two-bit encoding per dimension had the property that only one bit of the two would change at a time, like a Gray code or Johnson counter, so that the transitions would not be misinterpreted when asynchronously sampled.\nThe earliest mass-market mice, such as on the original Macintosh, Amiga, and Atari ST mice used a D-subminiature 9-pin connector to send the quadrature-encoded X and Y axis signals directly, plus one pin per mouse button. The mouse was a simple optomechanical device, and the decoding circuitry was all in the main computer.\nThe DE-9 connectors were designed to be electrically compatible with the joysticks popular on numerous 8-bit systems, such as the Commodore 64 and the Atari 2600. Although the ports could be used for both purposes, the signals must be interpreted differently. As a result, plugging a mouse into a joystick port causes the \"joystick\" to continuously move in some direction, even if the mouse stays still, whereas plugging a joystick into a mouse port causes the \"mouse\" to only be able to move a single pixel in each direction.\nSerial interface and protocol.\nBecause the IBM PC did not have a quadrature decoder built in, early PC mice used the RS-232C serial port to communicate encoded mouse movements, as well as provide power to the mouse's circuits. The Mouse Systems Corporation version used a five-byte protocol and supported three buttons. The Microsoft version used a three-byte protocol and supported two buttons. Due to the incompatibility between the two protocols, some manufacturers sold serial mice with a mode switch: \"PC\" for MSC mode, \"MS\" for Microsoft mode.\nApple Desktop Bus.\nIn 1986 Apple first implemented the Apple Desktop Bus allowing the daisy chaining of up to 16 devices, including mice and other devices on the same bus with no configuration whatsoever. Featuring only a single data pin, the bus used a purely polled approach to device communications and survived as the standard on mainstream models (including a number of non-Apple workstations) until 1998 when Apple's iMac line of computers joined the industry-wide switch to using USB. Beginning with the Bronze Keyboard PowerBook G3 in May 1999, Apple dropped the external ADB port in favor of USB, but retained an internal ADB connection in the PowerBook G4 for communication with its built-in keyboard and trackpad until early 2005.\nPS/2 interface and protocol.\nWith the arrival of the IBM PS/2 personal-computer series in 1987, IBM introduced the eponymous PS/2 port for mice and keyboards, which other manufacturers rapidly adopted. The most visible change was the use of a round 6-pin mini-DIN, in lieu of the former 5-pin MIDI style full sized DIN 41524 connector. In default mode (called \"stream mode\") a PS/2 mouse communicates motion, and the state of each button, by means of 3-byte packets. For any motion, button press or button release event, a PS/2 mouse sends, over a bi-directional serial port, a sequence of three bytes, with the following format:\nHere, XS and YS represent the sign bits of the movement vectors, XV and YV indicate an overflow in the respective vector component, and LB, MB and RB indicate the status of the left, middle and right mouse buttons (1 = pressed). PS/2 mice also understand several commands for reset and self-test, switching between different operating modes, and changing the resolution of the reported motion vectors.\nA Microsoft IntelliMouse relies on an extension of the PS/2 protocol: the ImPS/2 or IMPS/2 protocol (the abbreviation combines the concepts of \"IntelliMouse\" and \"PS/2\"). It initially operates in standard PS/2 format, for backwards compatibility. After the host sends a special command sequence, it switches to an extended format in which a fourth byte carries information about wheel movements. The IntelliMouse Explorer works analogously, with the difference that its 4-byte packets also allow for two additional buttons (for a total of five).\nMouse vendors also use other extended formats, often without providing public documentation. The Typhoon mouse uses 6-byte packets which can appear as a sequence of two standard 3-byte packets, such that an ordinary PS/2 driver can handle them. For 3-D (or 6-degree-of-freedom) input, vendors have made many extensions both to the hardware and to software. In the late 1990s, Logitech created ultrasound based tracking which gave 3D input to a few millimeters accuracy, which worked well as an input device but failed as a profitable product. In 2008, Motion4U introduced its \"OptiBurst\" system using IR tracking for use as a Maya (graphics software) plugin.\nUSB.\nThe industry-standard USB (Universal Serial Bus) protocol and its connector have become widely used for mice; it is among the most popular types.\nCordless or wireless.\nCordless or wireless mice transmit data via radio. Some mice connect to the computer through Bluetooth or Wi-Fi, while others use a receiver that plugs into the computer, for example through a USB port.\nMany mice that use a USB receiver have a storage compartment for it inside the mouse. Some \"nano receivers\" are designed to be small enough to remain plugged into a laptop during transport, while still being large enough to easily remove.\nOperating system support.\nMS-DOS and Windows 1.0 support connecting a mouse such as a Microsoft Mouse via multiple interfaces: BallPoint, Bus (InPort), Serial port or PS/2.\nWindows 98 added built-in support for USB Human Interface Device class (USB HID), with native vertical scrolling support. Windows 2000 and Windows Me expanded this built-in support to 5-button mice.\nWindows XP Service Pack 2 introduced a Bluetooth stack, allowing Bluetooth mice to be used without any USB receivers. Windows Vista added native support for horizontal scrolling and standardized wheel movement granularity for finer scrolling.\nWindows 8 introduced BLE (Bluetooth Low Energy) mouse/HID support.\nMultiple-mouse systems.\nSome systems allow two or more mice to be used at once as input devices. Late-1980s era home computers such as the Amiga used this to allow computer games with two players interacting on the same computer (Lemmings and The Settlers for example). The same idea is sometimes used in collaborative software, e.g. to simulate a whiteboard that multiple users can draw on without passing a single mouse around.\nMicrosoft Windows, since Windows 98, has supported multiple simultaneous pointing devices. Because Windows only provides a single screen cursor, using more than one device at the same time requires cooperation of users or applications designed for multiple input devices.\nMultiple mice are often used in multi-user gaming in addition to specially designed devices that provide several input interfaces.\nWindows also has full support for multiple input/mouse configurations for multi-user environments.\nStarting with Windows XP, Microsoft introduced an SDK for developing applications that allow multiple input devices to be used at the same time with independent cursors and independent input points. However, it no longer appears to be available.\nThe introduction of Windows Vista and Microsoft Surface (now known as Microsoft PixelSense) introduced a new set of input APIs that were adopted into Windows 7, allowing for 50 points/cursors, all controlled by independent users. The new input points provide traditional mouse input; however, they were designed with other input technologies like touch and image in mind. They inherently offer 3D coordinates along with pressure, size, tilt, angle, mask, and even an image bitmap to see and recognize the input point/object on the screen.\nAs of 2009, Linux distributions and other operating systems that use X.Org, such as OpenSolaris and FreeBSD, support 255 cursors/input points through Multi-Pointer X. However, currently no window managers support Multi-Pointer X leaving it relegated to custom software usage.\nThere have also been propositions of having a single operator use two mice simultaneously as a more sophisticated means of controlling various graphics and multimedia applications.\nButtons.\nMouse buttons are microswitches which can be pressed to select or interact with an element of a graphical user interface, producing a distinctive clicking sound.\nSince around the late 1990s, the three-button scrollmouse has become the de facto standard. Users most commonly employ the second button to invoke a contextual menu in the computer's software user interface, which contains options specifically tailored to the interface element over which the mouse cursor currently sits. By default, the primary mouse button sits located on the left-hand side of the mouse, for the benefit of right-handed users; left-handed users can usually reverse this configuration via software.\nScrolling.\nNearly all mice now have an integrated input primarily intended for scrolling on top, usually a single-axis digital wheel or rocker switch which can also be depressed to act as a third button. Though less common, many mice instead have two-axis inputs such as a tiltable wheel, trackball, or touchpad. Those with a trackball may be designed to stay stationary, using the trackball instead of moving the mouse.\nSpeed.\nMickeys per second is a unit of measurement for the speed and movement direction of a computer mouse, where direction is often expressed as \"horizontal\" versus \"vertical\" mickey count. However, speed can also refer to the ratio between how many pixels the cursor moves on the screen and how far the mouse moves on the mouse pad, which may be expressed as pixels per mickey, pixels per inch, or pixels per centimeter.\nThe computer industry often measures mouse sensitivity in terms of counts per inch (CPI), commonly expressed as dots per inch (DPI)the number of steps the mouse will report when it moves one inch. In early mice, this specification was called pulses per inch (ppi). The mickey originally referred to one of these counts, or one resolvable step of motion. If the default mouse-tracking condition involves moving the cursor by one screen-pixel or dot on-screen per reported step, then the CPI does equate to DPI: dots of cursor motion per inch of mouse motion. The CPI or DPI as reported by manufacturers depends on how they make the mouse; the higher the CPI, the faster the cursor moves with mouse movement. However, software can adjust the mouse sensitivity, making the cursor move faster or slower than its CPI. software can change the speed of the cursor dynamically, taking into account the mouse's absolute speed and the movement from the last stop-point. In most software, an example being the Windows platforms, this setting is named \"speed,\" referring to \"cursor precision\". However, some operating systems name this setting \"acceleration\", the typical Apple OS designation. This term is incorrect. Mouse acceleration in most mouse software refers to the change in speed of the cursor over time while the mouse movement is constant.\nFor simple software, when the mouse starts to move, the software will count the number of \"counts\" or \"mickeys\" received from the mouse and will move the cursor across the screen by that number of pixels (or multiplied by a rate factor, typically less than 1). The cursor will move slowly on the screen, with good precision. When the movement of the mouse passes the value set for some threshold, the software will start to move the cursor faster, with a greater rate factor. Usually, the user can set the value of the second rate factor by changing the \"acceleration\" setting.\nOperating systems sometimes apply acceleration, referred to as \"ballistics\", to the motion reported by the mouse. For example, versions of Windows prior to Windows XP doubled reported values above a configurable threshold, and then optionally doubled them again above a second configurable threshold. These doublings applied separately in the X and Y directions, resulting in very nonlinear response.\nMousepads.\nEngelbart's original mouse did not require a mousepad; the mouse had two large wheels which could roll on virtually any surface. However, most subsequent mechanical mice starting with the steel roller ball mouse have required a mousepad for optimal performance.\nThe mousepad, the most common mouse accessory, appears most commonly in conjunction with mechanical mice, because to roll smoothly the ball requires more friction than common desk surfaces usually provide. So-called \"hard mousepads\" for gamers or optical/laser mice also exist.\nMost optical and laser mice do not require a pad, the notable exception being early optical mice which relied on a grid on the pad to detect movement (e.g. Mouse Systems). Whether to use a hard or soft mousepad with an optical mouse is largely a matter of personal preference. One exception occurs when the desk surface creates problems for the optical or laser tracking, for example, a transparent or reflective surface, such as glass.\nSome mice also come with small \"pads\" attached to the bottom surface, also called mouse feet or mouse skates, that help the user slide the mouse smoothly across surfaces.\nIn the marketplace.\nAround 1981, Xerox included mice with its Xerox Star, based on the mouse used in the 1970s on the Alto computer at Xerox PARC. Sun Microsystems, Symbolics, Lisp Machines Inc., and Tektronix also shipped workstations with mice, starting in about 1981. Later, inspired by the Star, Apple Computer released the Apple Lisa, which also used a mouse. However, none of these products achieved large-scale success. Only with the release of the Apple Macintosh in 1984 did the mouse see widespread use.\nThe Macintosh design, commercially successful and technically influential, led many other vendors to begin producing mice or including them with their other computer products (by 1986, Atari ST, Amiga, Windows 1.0, GEOS for the Commodore 64, and the Apple IIGS).\nThe widespread adoption of graphical user interfaces in the software of the 1980s and 1990s made mice all but indispensable for controlling computers. In November 2008, Logitech built their billionth mouse.\nUse in games.\nThe Classic Mac OS Desk Accessory \"Puzzle\" in 1984 was the first game designed specifically for a mouse. The device often functions as an interface for PC-based computer games and sometimes for video game consoles.\nFirst-person shooters.\nFPSs naturally lend themselves to separate and simultaneous control of the player's movement and aim, and on computers this has traditionally been achieved with a combination of keyboard and mouse. Players use the X-axis of the mouse for looking (or turning) left and right, and the Y-axis for looking up and down; the keyboard is used for movement and supplemental inputs.\nMany shooting genre players prefer a mouse over a gamepad analog stick because the wide range of motion offered by a mouse allows for faster and more varied control. Although an analog stick allows the player more granular control, it is poor for certain movements, as the player's input is relayed based on a vector of both the stick\ns direction and magnitude. Thus, a small but fast movement (known as \"flick-shotting\") using a gamepad requires the player to quickly move the stick from its rest position to the edge and back again in quick succession, a difficult maneuver. In addition the stick also has a finite magnitude; if the player is currently using the stick to move at a non-zero velocity their ability to increase the rate of movement of the camera is further limited based on the position their displaced stick was already at before executing the maneuver. The effect of this is that a mouse is well suited not only to small, precise movements but also to large, quick movements and immediate, responsive movements; all of which are important in shooter gaming. This advantage also extends in varying degrees to similar game styles such as third-person shooters.\nSome incorrectly ported games or game engines have acceleration and interpolation curves which unintentionally produce excessive, irregular, or even negative acceleration when used with a mouse instead of their native platform's non-mouse default input device. Depending on how deeply hardcoded this misbehavior is, internal user patches or external 3rd-party software may be able to fix it. Individual game engines will also have their own sensitivities. This often restricts you from taking one games existing sensitivity, transferring it to another, and acquiring the same 360 rotational measurements. A sensitivity converter is required in order to translate rotational movements properly.\nDue to their similarity to the WIMP desktop metaphor interface for which mice were originally designed, and to their own tabletop game origins, computer strategy games are most commonly played with mice. In particular, real-time strategy and MOBA games usually require the use of a mouse.\nThe left button usually controls primary fire. If the game supports multiple fire modes, the right button often provides secondary fire from the selected weapon. Games with only a single fire mode will generally map secondary fire to \"ADS\". In some games, the right button may also invoke accessories for a particular weapon, such as allowing access to the scope of a sniper rifle or allowing the mounting of a bayonet or silencer.\nPlayers can use a scroll wheel for changing weapons (or for controlling scope-zoom magnification, in older games). On most first person shooter games, programming may also assign more functions to additional buttons on mice with more than three controls. A keyboard usually controls movement (for example, WASD for moving forward, left, backward and right, respectively) and other functions such as changing posture. Since the mouse serves for aiming, a mouse that tracks movement accurately and with less lag (latency) will give a player an advantage over players with less accurate or slower mice. In some cases the right mouse button may be used to move the player forward, either in lieu of, or in conjunction with the typical WASD configuration.\nMany games provide players with the option of mapping their own choice of a key or button to a certain control. An early technique of players, circle strafing, saw a player continuously strafing while aiming and shooting at an opponent by walking in circle around the opponent with the opponent at the center of the circle. Players could achieve this by holding down a key for strafing while continuously aiming the mouse towards the opponent.\nGames using mice for input are so popular that many manufacturers make mice specifically for gaming. Such mice may feature adjustable weights, high-resolution optical or laser components, additional buttons, ergonomic shape, and other features such as adjustable CPI. Mouse Bungees are typically used with gaming mice because it eliminates the annoyance of the cable.\nMany games, such as first- or third-person shooters, have a setting named \"invert mouse\" or similar (not to be confused with \"button inversion\", sometimes performed by left-handed users) which allows the user to look downward by moving the mouse forward and upward by moving the mouse backward (the opposite of non-inverted movement). This control system resembles that of aircraft control sticks, where pulling back causes pitch up and pushing forward causes pitch down; computer joysticks also typically emulate this control-configuration.\nAfter id Software's commercial hit of \"Doom\", which did not support vertical aiming, competitor Bungie's \"Marathon\" became the first first-person shooter to support using the mouse to aim up and down. Games using the Build engine had an option to invert the Y-axis. The \"invert\" feature actually made the mouse behave in a manner that users regard as non-inverted (by default, moving mouse forward resulted in looking down). Soon after, id Software released \"Quake\", which introduced the invert feature as users know it.\nHome consoles.\nIn 1988, the VTech Socrates educational video game console featured a wireless mouse with an attached mouse pad as an optional controller used for some games. In the early 1990s, the Super Nintendo Entertainment System video game system featured a mouse in addition to its controllers. The \"Mario Paint\" game in particular used the mouse's capabilities as did its successor on the N64. Sega released official mice for their Genesis/Mega Drive, Saturn and Dreamcast consoles. NEC sold official mice for its PC Engine and PC-FX consoles. Sony released an official mouse product for the PlayStation console, included one along with the Linux for PlayStation 2 kit, as well as allowing owners to use virtually any USB mouse with the PS2, PS3, and PS4. Nintendo's Wii also had this added on in a later software update, retained on the Wii U."}
{"id": "7059", "revid": "41194276", "url": "https://en.wikipedia.org/wiki?curid=7059", "title": "Civil defense", "text": "Civil defense (civil defence in UK English) or civil protection is an effort to protect the citizens of a state (generally non-combatants) from military attacks and natural disasters. It uses the principles of emergency operations: prevention, mitigation, preparation, response, or emergency evacuation and recovery. Programs of this sort were initially discussed at least as early as the 1920s and were implemented in some countries during the 1930s as the threat of war and aerial bombardment grew. It became widespread after the threat of nuclear weapons was realized.\nSince the end of the Cold War, the focus of civil defense has largely shifted from military attack to emergencies and disasters in general. The new concept is described by a number of terms, each of which has its own specific shade of meaning, such as \"crisis management\", \"emergency management\", \"emergency preparedness\", \"contingency planning\", \"civil contingency\", \"civil aid\" and \"civil protection\".\nIn some countries, civil defense is seen as a key part of defense in general. For example the Swedish language word \"totalf\u00f6rsvar\" (\"total defense\") refers to the commitment of a wide range of national resources to its defense, including the protection of all aspects of civilian life. Some countries have organized civil defense along paramilitary lines, or incorporated it within armed forces, such as the Soviet Civil Defense Forces (\u0412\u043e\u0439\u0441\u043a\u0430 \u0433\u0440\u0430\u0436\u0434\u0430\u043d\u0441\u043a\u043e\u0439 \u043e\u0431\u043e\u0440\u043e\u043d\u044b).\nHistory.\nOrigins.\nUnited Kingdom.\nThe advent of civil defense was stimulated by the experience of the bombing of civilian areas during the First World War. The bombing of the United Kingdom began on 19 January 1915 when German zeppelins dropped bombs on the Great Yarmouth area, killing six people. German bombing operations of the First World War were surprisingly effective, especially after the Gotha bombers surpassed the zeppelins. The most devastating raids inflicted 121 casualties for each ton of bombs dropped; this figure was then used as a basis for predictions.\nAfter the war, attention was turned toward civil defense in the event of war, and the Air Raid Precautions Committee (ARP) was established in 1924 to investigate ways for ensuring the protection of civilians from the danger of air-raids.\nThe Committee produced figures estimating that in London there would be 9,000 casualties in the first two days and then a continuing rate of 17,500 casualties a week. These rates were thought conservative. It was believed that there would be \"total chaos and panic\" and hysterical neurosis as the people of London would try to flee the city. To control the population harsh measures were proposed: bringing London under almost military control, and physically cordoning off the city with 120,000 troops to force people back to work. A different government department proposed setting up camps for refugees for a few days before sending them back to London.\nA special government department, the Civil Defence Service, was established by the Home Office in 1935. Its remit included the pre-existing ARP as well as wardens, firemen (initially the Auxiliary Fire Service (AFS) and latterly the National Fire Service (NFS)), fire watchers, rescue, first aid post, stretcher party and industry. Over 1.9 million people served within the CD; nearly 2,400 lost their lives to enemy action.\nThe organization of civil defense was the responsibility of the local authority. Volunteers were ascribed to different units depending on experience or training. Each local civil defense service was divided into several sections. Wardens were responsible for local reconnaissance and reporting, and leadership, organization, guidance and control of the general public. Wardens would also advise survivors of the locations of rest and food centers, and other welfare facilities.\nRescue Parties were required to assess and then access bombed-out buildings and retrieve injured or dead people. In addition they would turn off gas, electricity and water supplies, and repair or pull down unsteady buildings. Medical services, including First Aid Parties, provided on the spot medical assistance.\nThe expected stream of information that would be generated during an attack was handled by 'Report and Control' teams. A local headquarters would have an ARP controller who would direct rescue, first aid and decontamination teams to the scenes of reported bombing. If local services were deemed insufficient to deal with the incident then the controller could request assistance from surrounding boroughs.\nFire Guards were responsible for a designated area/building and required to monitor the fall of incendiary bombs and pass on news of any fires that had broken out to the NFS. They could deal with an individual magnesium electron incendiary bomb by dousing it with buckets of sand or water or by smothering. Additionally, 'Gas Decontamination Teams' kitted out with gas-tight and waterproof protective clothing were to deal with any gas attacks. They were trained to decontaminate buildings, roads, rail and other material that had been contaminated by liquid or jelly gases.\nLittle progress was made over the issue of air-raid shelters, because of the apparently irreconcilable conflict between the need to send the public underground for shelter and the need to keep them above ground for protection against gas attacks. In February 1936 the Home Secretary appointed a technical Committee on Structural Precautions against Air Attack. During the Munich crisis, local authorities dug trenches to provide shelter. After the crisis, the British Government decided to make these a permanent feature, with a standard design of precast concrete trench lining. They also decided to issue the Anderson shelter free to poorer households and to provide steel props to create shelters in suitable basements.\nDuring the Second World War, the ARP was responsible for the issuing of gas masks, pre-fabricated air-raid shelters (such as Anderson shelters, as well as Morrison shelters), the upkeep of local public shelters, and the maintenance of the blackout. The ARP also helped rescue people after air raids and other attacks, and some women became ARP Ambulance Attendants whose job was to help administer first aid to casualties, search for survivors, and in many grim instances, help recover bodies, sometimes those of their own colleagues.\nAs the war progressed, the military effectiveness of Germany's aerial bombardment was very limited. Thanks to the Luftwaffe's shifting aims, the strength of British air defenses, the use of early warning radar and the life-saving actions of local civil defense units, the aerial \"Blitz\" during the Battle of Britain failed to break the morale of the British people, destroy the Royal Air Force or significantly hinder British industrial production. Despite a significant investment in civil and military defense, British civilian losses during the Blitz were higher than in most strategic bombing campaigns throughout the war. For example, there were 14,000-20,000 UK civilian fatalities during the Battle of Britain, a relatively high number considering that the Luftwaffe dropped only an estimated 30,000 tons of ordinance during the battle. In comparison, Allied strategic bombing of Germany during the war was less lethal, with an estimated 400,000-600,000 German civilian fatalities for approximately 1.35 million tons of bombs dropped on Germany.\nUnited States.\nIn the United States, the Office of Civil Defense was established in May 1941 to coordinate civilian defense efforts. It coordinated with the Department of the Army and established similar groups to the British ARP. One of these groups that still exists today is the Civil Air Patrol, which was originally created as a civilian auxiliary to the Army. The CAP was created on December 1, 1941, with the main civil defense mission of search and rescue. The CAP also sank two Axis submarines and provided aerial reconnaissance for Allied and neutral merchant ships. In 1946, the Civil Air Patrol was barred from combat by Public Law 79-476. The CAP then received its current mission: search and rescue for downed aircraft. When the Air Force was created, in 1947, the Civil Air Patrol became the auxiliary of the Air Force.\nThe Coast Guard Auxiliary performs a similar role in support of the U.S. Coast Guard. Like the Civil Air Patrol, the Coast Guard Auxiliary was established in the run up to World War II. Auxiliarists were sometimes armed during the war, and extensively participated in port security operations. After the war, the Auxiliary shifted its focus to promoting boating safety and assisting the Coast Guard in performing search and rescue and marine safety and environmental protection.\nIn the United States a federal civil defense program existed under Public Law 920 of the 81st Congress, as amended, from 1951\u20131994. That statutory scheme was made so-called all-hazards by Public Law 103-160 in 1993 and largely repealed by Public Law 103-337 in 1994. Parts now appear in Title VI of the Robert T. Stafford Disaster Relief and Emergency Assistance Act, Public Law 100-107 [1988 as amended]. The term EMERGENCY PREPAREDNESS was largely codified by that repeal and amendment. See 42 USC Sections 5101 and following.\nIn most of the states of the North Atlantic Treaty Organization, such as the United States, the United Kingdom and West Germany, as well as the Soviet Bloc, and especially in the neutral countries, such as Switzerland and in Sweden during the 1950s and 1960s, many civil defense practices took place to prepare for the aftermath of a nuclear war, which seemed quite likely at that time.\nIn the United Kingdom, the Civil Defence Service was disbanded in 1945, followed by the ARP in 1946. With the onset of the growing tensions between East and West, the service was revived in 1949 as the Civil Defence Corps. As a civilian volunteer organization, it was tasked to take control in the aftermath of a major national emergency, principally envisaged as being a Cold War nuclear attack. Although under the authority of the Home Office, with a centralized administrative establishment, the corps was administered locally by Corps Authorities. In general every county was a Corps Authority, as were most county boroughs in England and Wales and large burghs in Scotland.\nEach division was divided into several sections, including the Headquarters, Intelligence and Operations, Scientific and Reconnaissance, Warden &amp; Rescue, Ambulance and First Aid and Welfare.\nIn 1954 Coventry City Council caused international controversy when it announced plans to disband its Civil Defence committee because the councillors had decided that hydrogen bombs meant that there could be no recovery from a nuclear attack. The British government opposed such a move and held a provocative Civil Defence exercise on the streets of Coventry which Labour council members protested against. The government also decided to implement its own committee at the city's cost until the council reinstituted its committee.\nIn the United States, the sheer power of nuclear weapons and the perceived likelihood of such an attack precipitated a greater response than had yet been required of civil defense. Civil defense, previously considered an important and commonsense step, became divisive and controversial in the charged atmosphere of the Cold War. In 1950, the National Security Resources Board created a 162-page document outlining a model civil defense structure for the U.S. Called the \"Blue Book\" by civil defense professionals in reference to its solid blue cover, it was the template for legislation and organization for the next 40 years.\nPerhaps the most memorable aspect of the Cold War civil defense effort was the educational effort made or promoted by the government. In \"Duck and Cover\", Bert the Turtle advocated that children \"duck and cover\" when they \"see the flash.\" Booklets such as \"Survival Under Atomic Attack\", \"Fallout Protection\" and \"Nuclear War Survival Skills\" were also commonplace. The transcribed radio program Stars for Defense combined hit music with civil defense advice. Government institutes created public service announcements including children's songs and distributed them to radio stations to educate the public in case of nuclear attack.\nThe US President Kennedy (1961\u201363) launched an ambitious effort to install fallout shelters throughout the United States. These shelters would not protect against the blast and heat effects of nuclear weapons, but would provide some protection against the radiation effects that would last for weeks and even affect areas distant from a nuclear explosion. In order for most of these preparations to be effective, there had to be some degree of warning. In 1951, CONELRAD (Control of Electromagnetic Radiation) was established. Under the system, a few primary stations would be alerted of an emergency and would broadcast an alert. All broadcast stations throughout the country would be constantly listening to an upstream station and repeat the message, thus passing it from station to station.\nIn a once classified US war game analysis, looking at varying levels of war escalation, warning and pre-emptive attacks in the late 1950s early 1960s, it was estimated that approximately 27 million US citizens would have been saved with civil defense education. At the time, however, the cost of a full-scale civil defense program was regarded as less effective in cost-benefit analysis than a ballistic missile defense (Nike Zeus) system, and as the Soviet adversary was increasing their nuclear stockpile, the efficacy of both would follow a diminishing returns trend.\nContrary to the largely noncommittal approach taken in NATO, with its stops and starts in civil defense depending on the whims of each newly elected government, the military strategy in the comparatively more ideologically consistent USSR held that, amongst other things, a winnable nuclear war was possible. To this effect the Soviets planned to minimize, as far as possible, the effects of nuclear weapon strikes on its territory, and therefore spent considerably more thought on civil defense preparations than in U.S., with defense plans that have been assessed to be far more effective than those in the U.S.\nSoviet Civil Defense Troops played the main role in the massive disaster relief operation following the 1986 Chernobyl nuclear accident. Defense Troop reservists were officially mobilized (as in a case of war) from throughout the USSR to join the Chernobyl task force and formed on the basis of the Kyiv Civil Defense Brigade. The task force performed some high-risk tasks including, with the failure of their robotic machinery, the manual removal of highly-radioactive debris. Many of their personnel were later decorated with medals for their work at containing the release of radiation into the environment, with a number of the 56 deaths from the accident being Civil defense troops.\nDecline.\nIn Western countries, strong civil defense policies were never properly implemented, because it was fundamentally at odds with the doctrine of \"mutual assured destruction\" (MAD) by making provisions for survivors. It was also considered that a full-fledged total defense would have not been worth the very large expense. For whatever reason, the public saw efforts at civil defense as fundamentally ineffective against the powerful destructive forces of nuclear weapons, and therefore a waste of time and money, although detailed scientific research programs did underlie the much-mocked government civil defense pamphlets of the 1950s and 1960s.\nGovernments in most Western countries, with the sole exception of Switzerland, generally sought to underfund Civil Defense due to its perceived pointlessness. Nevertheless, effective but commonly dismissed civil defense measures against nuclear attack were implemented, in the face of popular apathy and skepticism of authority. After the end of the Cold War, the focus moved from defense against nuclear war to defense against a terrorist attack possibly involving chemical or biological weapons.\nThe Civil Defence Corps was stood down in Great Britain in 1968 with the tacit realization that nothing practical could be done in the event of an unrestricted nuclear attack. Its neighbors, however, remained committed to Civil Defence, namely the Isle of Man Civil Defence Corps and Civil Defence Ireland (Republic of Ireland).\nIn the United States, the various civil defense agencies were replaced with the Federal Emergency Management Agency (FEMA) in 1979. In 2002 this became part of the Department of Homeland Security. The focus was shifted from nuclear war to an \"all-hazards\" approach of Comprehensive Emergency Management. Natural disasters and the emergence of new threats such as terrorism have caused attention to be focused away from traditional civil defense and into new forms of civil protection such as emergency management and homeland security.\nToday.\nMany countries still maintain a national Civil Defence Corps, usually having a wide brief for assisting in large scale civil emergencies such as flood, earthquake, invasion, or civil disorder.\nAfter the September 11 attacks in 2001, in the United States the concept of civil defense has been revisited under the umbrella term of homeland security and all-hazards emergency management.\nIn Europe, the triangle CD logo continues to be widely used. The old U.S. civil defense logo was used in the FEMA logo until 2006 and is hinted at in the United States Civil Air Patrol logo. Created in 1939 by Charles Coiner of the N. W. Ayer Advertising Agency, it was used throughout World War II and the Cold War era. In 2006, the National Emergency Management Association\u2014a U.S. organization made up of state emergency managers\u2014\"officially\" retired the Civil Defense triangle logo, replacing it with a stylised EM (standing for Emergency management). The name and logo, however, continue to be used by Hawaii State Civil Defense and Guam Homeland Security/Office of Civil Defense.\nThe term \"civil protection\" is currently widely used within the European Union to refer to government-approved systems and resources tasked with protecting the non-combat population, primarily in the event of natural and technological disasters. In recent years there has been emphasis on preparedness for technological disasters resulting from terrorist attack. Within EU countries the term \"crisis-management\" emphasizes the political and security dimension rather than measures to satisfy the immediate needs of the population.\nIn Australia, civil defense is the responsibility of the volunteer-based State Emergency Service.\nIn most former Soviet countries civil defense is the responsibility of governmental ministries, such as Russia's Ministry of Emergency Situations.\nImportance.\nRelatively small investments in preparation can speed up recovery by months or years and thereby prevent millions of deaths by hunger, cold and disease. According to human capital theory in economics, a country's population is more valuable than all of the land, factories and other assets that it possesses. People rebuild a country after its destruction, and it is therefore important for the economic security of a country that it protect its people. According to psychology, it is important for people to feel as though they are in control of their own destiny, and preparing for uncertainty via civil defense may help to achieve this.\nIn the United States, the federal civil defense program was authorized by statute and ran from 1951 to 1994. Originally authorized by Public Law 920 of the 81st Congress, it was repealed by Public Law 93-337 in 1994. Small portions of that statutory scheme were incorporated into the Robert T. Stafford Disaster Relief and Emergency Assistance Act (Public Law 100-707) which partly superseded in part, partly amended, and partly supplemented the Disaster Relief Act of 1974 (Public Law 93-288). In the portions of the civil defense statute incorporated into the Stafford Act, the primary modification was to use the term \"Emergency Preparedness\" wherever the term \"Civil Defence\" had previously appeared in the statutory language.\nAn important concept initiated by President Jimmy Carter was the so-called \"Crisis Relocation Program\" administered as part of the federal civil defense program. That effort largely lapsed under President Ronald Reagan, who discontinued the Carter initiative because of opposition from areas potentially hosting the relocated population.\nThreat assessment.\nThreats to civilians and civilian life include NBC (Nuclear, Biological, and Chemical warfare) and others, like the more modern term CBRN (Chemical Biological Radiological and Nuclear). Threat assessment involves studying each threat so that preventative measures can be built into civilian life.\nRefers to conventional explosives. A blast shelter designed to protect only from radiation and fallout would be much more vulnerable to conventional explosives. See also fallout shelter.\nShelter intended to protect against nuclear blast effects would include thick concrete and other sturdy elements which are resistant to conventional explosives. The biggest threats from a nuclear attack are effects from the blast, fires and radiation. One of the most prepared countries for a nuclear attack is Switzerland. Almost every building in Switzerland has an \"abri\" (shelter) against the initial nuclear bomb and explosion followed by the fall-out. Because of this, many people use it as a safe to protect valuables, photos, financial information and so on. Switzerland also has air-raid and nuclear-raid sirens in every village.\nA \"radiologically enhanced weapon\", or \"dirty bomb\", uses an explosive to spread radioactive material. This is a theoretical risk, and such weapons have not been used by terrorists. Depending on the quantity of the radioactive material, the dangers may be mainly psychological. Toxic effects can be managed by standard hazmat techniques.\nThe threat here is primarily from disease-causing microorganisms such as bacteria and viruses.\nVarious chemical agents are a threat, such as nerve gas (VX, Sarin, and so on.).\nStages.\nMitigation.\nMitigation is the process of actively prevents the war or the released of nuclear weapons. It includes policy analysis, diplomacy, political measures, nuclear disarmament and more military responses such as a National Missile Defense and air defense artillery. In the case of counter-terrorism, mitigation would include diplomacy, intelligence gathering and direct action against terrorist groups. Mitigation may also be reflected in long-term planning such as the design of the interstate highway system and the placement of military bases further away from populated areas.\nPreparation.\nPreparation consists of building blast shelters and pre-positioning information, supplies, and emergency infrastructure. For example, most larger cities in the U.S. now have underground emergency operations centers that can perform civil defense coordination. FEMA also has many underground facilities for the same purpose located near major railheads such as the ones in Denton, Texas and Mount Weather, Virginia.\nOther measures would include continual government inventories of grain silos, the Strategic National Stockpile, the uncapping of the Strategic Petroleum Reserve, the dispersal of lorry-transportable bridges, water purification, mobile refineries, mobile de-contamination facilities, mobile general and special purpose disaster mortuary facilities such as Disaster Mortuary Operational Response Team (DMORT) and DMORT-WMD, and other aids such as temporary housing to speed civil recovery.\nOn an individual scale, one means of preparation for exposure to nuclear fallout is to obtain potassium iodide (KI) tablets as a safety measure to protect the human thyroid gland from the uptake of dangerous radioactive iodine. Another measure is to cover the nose, mouth and eyes with a piece of cloth and sunglasses to protect against alpha particles, which are only an internal hazard.\nTo support and supplement efforts at national, regional and local level with regard to disaster prevention, the preparedness of those responsible for civil protection and the intervention in the event of disaster\nPreparing also includes sharing information:\nResponse.\nResponse consists first of warning civilians so they can enter fallout shelters and protect assets.\nStaffing a response is always full of problems in a civil defense emergency. After an attack, conventional full-time emergency services are dramatically overloaded, with conventional fire fighting response times often exceeding several days. Some capability is maintained by local and state agencies, and an emergency reserve is provided by specialized military units, especially civil affairs, Military Police, Judge Advocates and combat engineers.\nHowever, the traditional response to massed attack on civilian population centers is to maintain a mass-trained force of volunteer emergency workers. Studies in World War II showed that lightly trained (40 hours or less) civilians in organised teams can perform up to 95% of emergency activities when trained, liaised and supported by local government. In this plan, the populace rescues itself from most situations, and provides information to a central office to prioritize professional emergency services.\nIn the 1990s, this concept was revived by the Los Angeles Fire Department to cope with civil emergencies such as earthquakes. The program was widely adopted, providing standard terms for organization. In the U.S., this is now official federal policy, and it is implemented by community emergency response teams, under the Department of Homeland Security, which certifies training programs by local governments, and registers \"certified disaster service workers\" who complete such training.\nRecovery.\nRecovery consists of rebuilding damaged infrastructure, buildings and production. The recovery phase is the longest and ultimately most expensive phase. Once the immediate \"crisis\" has passed, cooperation fades away and recovery efforts are often politicized or seen as economic opportunities.\nPreparation for recovery can be very helpful. If mitigating resources are dispersed before the attack, cascades of social failures can be prevented. One hedge against bridge damage in riverine cities is to subsidize a \"tourist ferry\" that performs scenic cruises on the river. When a bridge is down, the ferry takes up the load.\nCivil defense organizations.\nCivil Defense is also the name of a number of organizations around the world dedicated to protecting civilians from military attacks, as well as to providing rescue services after natural and human-made disasters alike.\nWorldwide protection is managed by the United Nations Office for the Coordination of Humanitarian Affairs (OCHA).\nIn a few countries such as Jordan and Singapore (see Singapore Civil Defence Force), civil defense is essentially the same organization as the fire brigade. In most countries, however, civil defense is a government-managed, volunteer-staffed organization, separate from the fire brigade and the ambulance service.\nAs the threat of Cold War eased, a number of such civil defense organizations have been disbanded or mothballed (as in the case of the Royal Observer Corps in the United Kingdom and the United States civil defense), while others have changed their focuses into providing rescue services after natural disasters (as for the State Emergency Service in Australia). However, the ideals of Civil Defense have been brought back in the United States under FEMA's Citizen Corps and Community Emergency Response Team (CERT).\nIn the United Kingdom Civil Defence work is carried out by Emergency Responders under the Civil Contingencies Act 2004, with assistance from voluntary groups such as RAYNET, Search and Rescue Teams and 4x4 Response. In Ireland, the Civil Defence is still very much an active organization and is occasionally called upon for its Auxiliary Fire Service and ambulance/rescue services when emergencies such as flash flooding occur and require additional manpower. The organization has units of trained firemen and medical responders based in key areas around the country.\nBy country.\nUK:\nUS:\nGermany:\nSee also.\nGeneral:"}
{"id": "7060", "revid": "57939", "url": "https://en.wikipedia.org/wiki?curid=7060", "title": "Chymotrypsin", "text": " \nChymotrypsin (, chymotrypsins A and B, alpha-chymar ophth, avazyme, chymar, chymotest, enzeon, quimar, quimotrase, alpha-chymar, alpha-chymotrypsin A, alpha-chymotrypsin) is a digestive enzyme component of pancreatic juice acting in the duodenum, where it performs proteolysis, the breakdown of proteins and polypeptides. Chymotrypsin preferentially cleaves peptide amide bonds where the side chain of the amino acid N-terminal to the scissile amide bond (the P1 position) is a large hydrophobic amino acid (tyrosine, tryptophan, and phenylalanine). These amino acids contain an aromatic ring in their side chain that fits into a hydrophobic pocket (the S1 position) of the enzyme. It is activated in the presence of trypsin. The hydrophobic and shape complementarity between the peptide substrate P1 side chain and the enzyme S1 binding cavity accounts for the substrate specificity of this enzyme. Chymotrypsin also hydrolyzes other amide bonds in peptides at slower rates, particularly those containing leucine and methionine at the P1 position.\nStructurally, it is the archetypal structure for its superfamily, the PA clan of proteases.\nActivation.\nChymotrypsin is synthesized in the pancreas by protein biosynthesis as a precursor called chymotrypsinogen that is enzymatically inactive. Trypsin activates chymotrypsinogen by cleaving peptidic bonds in positions Arg15 - Ile16 and produces \u03c0-chymotrypsin. In turn, aminic group (-NH3+) of the Ile16 residue interacts with the side chain of Asp194, producing the \"oxyanion hole\" and the hydrophobic \"S1 pocket\". Moreover, chymotrypsin induces its own activation by cleaving in positions 14-15, 146-147, and 148-149, producing \u03b1-chymotrypsin (which is more active and stable than \u03c0-chymotrypsin). The resulting molecule is a three-polypeptide molecule interconnected via disulfide bonds.\nMechanism of action and kinetics.\n\"In vivo\", chymotrypsin is a proteolytic enzyme (serine protease) acting in the digestive systems of many organisms. It facilitates the cleavage of peptide bonds by a hydrolysis reaction, which despite being thermodynamically favorable, occurs extremely slowly in the absence of a catalyst. The main substrates of chymotrypsin are peptide bonds in which the amino acid N-terminal to the bond is a tryptophan, tyrosine, phenylalanine, or leucine. Like many proteases, chymotrypsin also hydrolyses amide bonds \"in vitro\", a virtue that enabled the use of substrate analogs such as N-acetyl-L-phenylalanine p-nitrophenyl amide for enzyme assays.\nChymotrypsin cleaves peptide bonds by attacking the unreactive carbonyl group with a powerful nucleophile, the serine 195 residue located in the active site of the enzyme, which briefly becomes covalently bonded to the substrate, forming an enzyme-substrate intermediate. Along with histidine 57 and aspartic acid 102, this serine residue constitutes the catalytic triad of the active site.\nThese findings rely on inhibition assays and the study of the kinetics of cleavage of the aforementioned substrate, exploiting the fact that the enzyme-substrate intermediate \"p\"-nitrophenolate has a yellow colour, enabling measurement of its concentration by measuring light absorbance at 410\u00a0nm.\nThe reaction of chymotrypsin with its substrate was found to take place in two stages, an initial \"burst\" phase at the beginning of the reaction and a steady-state phase following Michaelis-Menten kinetics. The mode of action of chymotrypsin explains this as hydrolysis takes place in two steps. First, acylation of the substrate to form an acyl-enzyme intermediate, and then deacylation to return the enzyme to its original state. This occurs via the concerted action of the three-amino-acid residues in the catalytic triad. Aspartate hydrogen bonds to the N-\u03b4 hydrogen of histidine, increasing the pKa of its \u03b5 nitrogen, thus making it able to deprotonate serine. This deprotonation allows the serine side chain to act as a nucleophile and bind to the electron-deficient carbonyl carbon of the protein main chain. Ionization of the carbonyl oxygen is stabilized by formation of two hydrogen bonds to adjacent main chain N-hydrogens. This occurs in the oxyanion hole. This forms a tetrahedral adduct and breakage of the peptide bond. An acyl-enzyme intermediate, bound to the serine, is formed, and the newly formed amino terminus of the cleaved protein can dissociate. In the second reaction step, a water molecule is activated by the basic histidine, and acts as a nucleophile. The oxygen of water attacks the carbonyl carbon of the serine-bound acyl group, resulting in formation of a second tetrahedral adduct, regeneration of the serine -OH group, and release of a proton, as well as the protein fragment with the newly formed carboxyl terminus "}
{"id": "7061", "revid": "1005330884", "url": "https://en.wikipedia.org/wiki?curid=7061", "title": "Community emergency response team", "text": "In the United States, community emergency response team (CERT) can refer to\nSometimes programs and organizations take different names, such as Neighborhood Emergency Response Team (NERT), or Neighborhood Emergency Team (NET).\nThe concept of civilian auxiliaries is similar to civil defense, which has a longer history. The CERT concept differs because it includes nonmilitary emergencies, and is coordinated with all levels of emergency authorities, local to national, via an overarching incident command system.\nOrganization.\nA local government agency, often a fire department, police department, or emergency management agency, agrees to sponsor CERT within its jurisdiction. The sponsoring agency liaises with, deploys and may train or supervise the training of CERT members. Many sponsoring agencies employ a full-time community-service person as liaison to the CERT members. In some communities, the liaison is a volunteer and CERT member.\nAs people are trained and agree to join the community emergency response effort, a CERT is formed. Initial efforts may result in a team with only a few members from across the community. As the number of members grow, a single community-wide team may subdivide. Multiple CERTs are organized into a hierarchy of teams consistent with ICS principles. This follows the Incident Command System (ICS) principle of Span of control until the ideal distribution is achieved: one or more teams are formed at each neighborhood within a community.\nA Teen Community Emergency Response Team (TEEN CERT), or Student Emergency Response Team (SERT), can be formed from any group of teens. A Teen CERT can be formed as a school club, service organization, Venturing Crew, Explorer Post, or the training can be added to a school's graduation curriculum. Some CERTs form a club or service corporation, and recruit volunteers to perform training on behalf of the sponsoring agency. This reduces the financial and human resource burden on the sponsoring agency.\nWhen not responding to disasters or large emergencies, CERTs may\nSome sponsoring agencies use state and federal grants to purchase response tools and equipment for their members and team(s) (subject to Stafford Act limitations). Most CERTs also acquire their own supplies, tools, and equipment. As community members, CERTs are aware of the specific needs of their community and equip the teams accordingly.\nResponse.\nThe basic idea is to use CERT to perform the large number of tasks needed in emergencies. This frees highly trained professional responders for more technical tasks. Much of CERT training concerns the Incident Command System and organization, so CERT members fit easily into larger command structures.\nA team may self-activate (self-deploy) when their own neighborhood is affected by disaster. An effort is made to report their response status to the sponsoring agency. A self-activated team will size-up the loss in their neighborhood and begin performing the skills they have learned to minimize further loss of life, property, and environment. They will continue to respond safely until redirected or relieved by the sponsoring agency or professional responders on-scene.\nTeams in neighborhoods not affected by disaster may be deployed or activated by the sponsoring agency. The sponsoring agency may communicate with neighborhood CERT leaders through an organic communication team. In some areas the communications may be by amateur radio, FRS, GMRS or MURS radio, dedicated telephone or fire-alarm networks. In other areas, relays of bicycle-equipped runners can effectively carry messages between the teams and the local emergency operations center.\nThe sponsoring agency may activate and dispatch teams in order to gather or respond to intelligence about an incident. Teams may be dispatched to affected neighborhoods, or organized to support operations. CERT members may augment support staff at an Incident Command Post or Emergency Operations Center. Additional teams may also be created to guard a morgue, locate supplies and food, convey messages to and from other CERTs and local authorities, and other duties on an as-needed basis as identified by the team leader.\nIn the short term, CERTs perform data gathering, especially to locate mass-casualties requiring professional response, or situations requiring professional rescues, simple fire-fighting tasks (for example, small fires, turning off gas), light search and rescue, damage evaluation of structures, triage and first aid. In the longer term, CERTs may assist in the evacuation of residents, or assist with setting up a neighborhood shelter.\nWhile responding, CERT members are temporary volunteer government workers. In some areas, (such as California, Hawaii and Kansas) registered, activated CERT members are eligible for worker's compensation for on-the-job injuries during declared disasters.\nMember roles.\nThe Federal Emergency Management Agency (FEMA) recommends that the standard, ten-person team be comprised as follows:\nBecause every CERT member in a community receives the same core instruction, any team member has the training necessary to assume any of these roles. This is important during a disaster response because not all members of a regular team may be available to respond. Hasty teams may be formed by whichever members are responding at the time. Additionally, members may need to adjust team roles due to stress, fatigue, injury, or other circumstances.\nTraining.\nWhile state and local jurisdictions will implement training in the manner that best suits the community, FEMA's National CERT Program has an established curriculum. Jurisdictions may augment the training, but are strongly encouraged to deliver the entire core content. The CERT core curriculum for the basic course is composed of the following nine units (time is instructional hours):\nCERT training emphasizes safely \"doing the most good for the most people as quickly as possible\" when responding to a disaster. For this reason, cardiopulmonary resuscitation (CPR) training is not included in the core curriculum, as it is time and responder intensive in a mass-casualty incident. However, many jurisdictions encourage or require CERT members to obtain CPR training. Many CERT programs provide or encourage members to take additional first aid training. Some CERT members may also take training to become a certified first responder or emergency medical technician.\nMany CERT programs also provide training in amateur radio operation, shelter operations, flood response, community relations, mass care, the incident command system (ICS), and the National Incident Management System (NIMS).\nEach unit of CERT training is ideally delivered by professional responders or other experts in the field addressed by the unit. This is done to help build unity between CERT members and responders, keep the attention of students, and help the professional response organizations be comfortable with the training which CERT members receive.\nEach course of instruction is ideally facilitated by one or more instructors certified in the CERT curriculum by the state or sponsoring agency. Facilitating instructors provide continuity between units, and help ensure that the CERT core curriculum is being delivered successfully. Facilitating instructors also perform set-up and tear-down of the classroom, provide instructional materials for the course, record student attendance and other tasks which assist the professional responder in delivering their unit as efficiently as possible.\nCERT training is provided free to interested members of the community, and is delivered in a group classroom setting. People may complete the training without obligation to join a CERT. Citizen Corps grant funds can be used to print and provide each student with a printed manual. Some sponsoring agencies use Citizen Corps grant funds to purchase disaster response tool kits. These kits are offered as an incentive to join a CERT, and must be returned to the sponsoring agency when members resign from CERT.\nSome sponsoring agencies require a criminal background-check of all trainees before allowing them to participate on a CERT. For example, the city of Albuquerque, New Mexico require all volunteers to pass a background check, while the city of Austin, Texas does not require a background check to take part in training classes but requires members to undergo a background check in order to receive a CERT badge and directly assist first responders during an activation of the Emergency Operations Center. However, most programs do not require a criminal background check in order to participate.\nThe CERT curriculum (including the Train-the-Trainer and Program Manager courses) was updated during the last half of 2017 to reflect feedback from instructors across the nation. The update is in final review, and is scheduled for release during 2018."}
{"id": "7063", "revid": "28903366", "url": "https://en.wikipedia.org/wiki?curid=7063", "title": "Catapult", "text": "A catapult is a ballistic device used to launch a projectile a great distance without the aid of gunpowder or other propellants \u2013 particularly various types of ancient and medieval siege engines. A catapult uses the sudden release of stored potential energy to propel its payload. Most convert tension or torsion energy that was more slowly and manually built up within the device before release, via springs, bows, twisted rope, elastic, or any of numerous other materials and mechanisms. The counterweight trebuchet is a type of catapult that uses gravity.\nIn use since ancient times, the catapult has proven to be one of the most persistently effective mechanisms in warfare. In modern times the term can apply to devices ranging from a simple hand-held implement (also called a \"slingshot\") to a mechanism for launching aircraft from a ship.\nThe earliest catapults date to at least the 4th century BC with the advent of the mangonel in ancient China, a type of traction trebuchet and catapult. Early uses were also attributed to Ajatashatru of Magadha in his war against the Licchavis. Greek catapults were invented in the early 4th century BC, being attested by Diodorus Siculus as part of the equipment of a Greek army in 399 BC, and subsequently used at the siege of Motya in 397 BC.\nEtymology.\nThe word 'catapult' comes from the Latin 'catapulta', which in turn comes from the Greek (\"katapelt\u0113s\"), itself from \u03ba\u03b1\u03c4\u03ac (\"kata\"), \"downwards\" and \u03c0\u03ac\u03bb\u03bb\u03c9 (\"pall\u014d\"), \"to toss, to hurl\". Catapults were invented by the ancient Greeks and in ancient India where they were used by the Magadhan Emperor Ajatshatru around the early to mid 5th century BC.\nGreek and Roman catapults.\nThe catapult and crossbow in Greece are closely intertwined. Primitive catapults were essentially \"the product of relatively straightforward attempts to increase the range and penetrating power of missiles by strengthening the bow which propelled them\". The historian Diodorus Siculus (fl. 1st century BC), described the invention of a mechanical arrow-firing catapult (\"katapeltikon\") by a Greek task force in 399\u00a0BC. The weapon was soon after employed against Motya (397\u00a0BC), a key Carthaginian stronghold in Sicily. Diodorus is assumed to have drawn his description from the highly rated history of Philistus, a contemporary of the events then. The introduction of crossbows however, can be dated further back: according to the inventor Hero of Alexandria (fl. 1st century AD), who referred to the now lost works of the 3rd-century BC engineer Ctesibius, this weapon was inspired by an earlier foot-held crossbow, called the \"gastraphetes\", which could store more energy than the Greek bows. A detailed description of the \"gastraphetes\", or the \"belly-bow\", along with a watercolor drawing, is found in Heron's technical treatise \"Belopoeica\".\nA third Greek author, Biton (fl. 2nd century BC), whose reliability has been positively reevaluated by recent scholarship, described two advanced forms of the \"gastraphetes\", which he credits to Zopyros, an engineer from southern Italy. Zopyrus has been plausibly equated with a Pythagorean of that name who seems to have flourished in the late 5th century BC. He probably designed his bow-machines on the occasion of the sieges of Cumae and Milet between 421\u00a0BC and 401\u00a0BC. The bows of these machines already featured a winched pull back system and could apparently throw two missiles at once.\nPhilo of Byzantium provides probably the most detailed account on the establishment of a theory of belopoietics (\"belos\" = \"projectile\"; \"poietike\" = \"(art) of making\") circa 200\u00a0BC. The central principle to this theory was that \"all parts of a catapult, including the weight or length of the projectile, were proportional to the size of the torsion springs\". This kind of innovation is indicative of the increasing rate at which geometry and physics were being assimilated into military enterprises.\nFrom the mid-4th century BC onwards, evidence of the Greek use of arrow-shooting machines becomes more dense and varied: arrow firing machines (\"katapaltai\") are briefly mentioned by Aeneas Tacticus in his treatise on siegecraft written around 350\u00a0BC. An extant inscription from the Athenian arsenal, dated between 338 and 326\u00a0BC, lists a number of stored catapults with shooting bolts of varying size and springs of sinews. The later entry is particularly noteworthy as it constitutes the first clear evidence for the switch to torsion catapults, which are more powerful than the more-flexible crossbows and which came to dominate Greek and Roman artillery design thereafter. This move to torsion springs was likely spurred by the engineers of Philip II of Macedonia. Another Athenian inventory from 330 to 329\u00a0BC includes catapult bolts with heads and flights. As the use of catapults became more commonplace, so did the training required to operate them. Many Greek children were instructed in catapult usage, as evidenced by \"a 3rd Century B.C. inscription from the island of Ceos in the Cyclades [regulating] catapult shooting competitions for the young\". Arrow firing machines in action are reported from Philip II's siege of Perinth (Thrace) in 340\u00a0BC. At the same time, Greek fortifications began to feature high towers with shuttered windows in the top, which could have been used to house anti-personnel arrow shooters, as in Aigosthena. Projectiles included both arrows and (later) stones that were sometimes lit on fire. Onomarchus of Phocis first used catapults on the battlefield against Philip II of Macedon. Philip's son, Alexander the Great, was the next commander in recorded history to make such use of catapults on the battlefield as well as to use them during sieges.\nThe Romans started to use catapults as arms for their wars against Syracuse, Macedon, Sparta and Aetolia (3rd and 2nd centuries BC). The Roman machine known as an arcuballista was similar to a large crossbow. Later the Romans used ballista catapults on their warships.\nOther ancient catapults.\nAjatshatru is recorded in Jaina texts as having used catapults in his campaign against the Licchavis.\nKing Uzziah, who reigned in Judah until 750 BC, is documented as having overseen the construction of machines to \"shoot great stones\" in .\nThe first recorded use of mangonels was in ancient China. They were probably used by the Mohists as early as 4th century BC, descriptions of which can be found in the \"Mojing\" (compiled in the 4th century BC). In Chapter 14 of the \"Mojing\", the mangonel is described hurling hollowed out logs filled with burning charcoal at enemy troops. The mangonel was carried westward by the Avars and appeared next in the eastern Mediterranean by the late 6th century AD, where it replaced torsion powered siege engines such as the ballista and onager due to its simpler design and faster rate of fire. The Byzantines adopted the mangonel possibly as early as 587, the Persians in the early 7th century, and the Arabs in the second half of the 7th century. The Franks and Saxons adopted the weapon in the 8th century.\nMedieval catapults.\nCastles and fortified walled cities were common during this period and catapults were used as siege weapons against them. As well as their use in attempts to breach walls, incendiary missiles, or diseased carcasses or garbage could be catapulted over the walls.\nDefensive techniques in the Middle Ages progressed to a point that rendered catapults largely ineffective. The Viking siege of Paris (885\u20136\u00a0A.D.) \"saw the employment by both sides of virtually every instrument of siege craft known to the classical world, including a variety of catapults\", to little effect, resulting in failure.\nThe most widely used catapults throughout the Middle Ages were as follows:\nModern use.\nMilitary.\nThe last large scale military use of catapults was during the trench warfare of World War I. During the early stages of the war, catapults were used to throw hand grenades across no man's land into enemy trenches. They were eventually replaced by small mortars.\nIn the 1840s the invention of vulcanized rubber allowed the making of small hand-held catapults, either improvised from Y-shaped sticks or manufactured for sale; both were popular with children and teenagers. These devices were also known as slingshots in the USA.\nSpecial variants called aircraft catapults are used to launch planes from land bases and sea carriers when the takeoff runway is too short for a powered takeoff or simply impractical to extend. Ships also use them to launch torpedoes and deploy bombs against submarines. Small catapults, referred to as \"traps\", are still widely used to launch clay targets into the air in the sport of clay pigeon shooting.\nEntertainment.\nIn the 1990s and into the early 2000s, a powerful catapult, a trebuchet, was used by thrill-seekers first on private property and in 2001-2002 at Middlemoor Water Park, Somerset, England, to experience being catapulted through the air for . The practice has been discontinued due to a fatality at the Water Park. There had been an injury when the trebuchet was in use on private property. Injury and death occurred when those two participants failed to land onto the safety net. The operators of the trebuchet were tried, but found not guilty of manslaughter, though the jury noted that the fatality might have been avoided had the operators \"imposed stricter safety measures.\" Human cannonball circus acts use a catapult launch mechanism, rather than gunpowder, and are risky ventures for the human cannonballs.\nEarly launched roller coasters used a catapult system powered by a diesel engine or a dropped weight to acquire their momentum, such as Shuttle Loop installations between 1977-1978. The catapult system for roller coasters has been replaced by flywheels and later linear motors.\n\"Pumpkin chunking\" is another widely popularized use, in which people compete to see who can launch a pumpkin the farthest by mechanical means (although the world record is held by a pneumatic air cannon).\nOther.\nIn January 2011, a homemade catapult was discovered that was used to smuggle cannabis into the United States from Mexico. The machine was found 20 feet from the border fence with bales of cannabis ready to launch."}
{"id": "7066", "revid": "965650547", "url": "https://en.wikipedia.org/wiki?curid=7066", "title": "Cinquain", "text": "Cinquain is a class of poetic forms that employ a 5-line pattern. Earlier used to describe any five-line form, it now refers to one of several forms that are defined by specific rules and guidelines.\nAmerican Cinquain.\nThe modern form, known as American Cinquain inspired by Japanese haiku and tanka, is akin in spirit to that of the Imagists.\nIn her 1915 collection titled \"Verse\", published one year after her death, Adelaide Crapsey included 28 cinquains.\nCrapsey's American Cinquain form developed in two stages. The first, fundamental form is a stanza of five lines of accentual verse, in which the lines comprise, in order, 1, 2, 3, 4, and 1 stresses. Then Crapsey decided to make the criterion a stanza of five lines of accentual-syllabic verse, in which the lines comprise, in order, 1, 2, 3, 4, and 1 stresses and 2, 4, 6, 8, and 2 syllables. Iambic feet were meant to be the standard for the cinquain, which made the dual criteria match perfectly. Some resource materials define classic cinquains as solely iambic, but that is not necessarily so. In contrast to the Eastern forms upon which she based them, Crapsey always titled her cinquains, effectively utilizing the title as a sixth line. Crapsey's cinquain depends on strict structure and intense physical imagery to communicate a mood or feeling.\nThe form is illustrated by Crapsey's \"November Night\":\nListen... &lt;br&gt;\nWith faint dry sound, &lt;br&gt;\nLike steps of passing ghosts, &lt;br&gt;\nThe leaves, frost-crisp'd, break from the trees &lt;br&gt;\nAnd fall.\nThe Scottish poet William Soutar also wrote over one hundred American Cinquains (he labelled them \"Epigrams\") between 1933 and 1940.\nCinquain variations.\nThe Crapsey cinquain has subsequently seen a number of variations by modern poets, including:\nDidactic cinquain.\nThe didactic cinquain is closely related to the Crapsey cinquain. It is an informal cinquain widely taught in elementary schools and has been featured in, and popularized by, children's media resources, including Junie B. Jones and PBS Kids. This form is also embraced by young adults and older poets for its expressive simplicity. The prescriptions of this type of cinquain refer to word count, not syllables and stresses. Ordinarily, the first line is a one-word title, the subject of the poem; the second line is a pair of adjectives describing that title; the third line is a three-word phrase that gives more information about the subject (often a list of three gerunds); the fourth line consists of four words describing feelings related to that subject; and the fifth line is a single word synonym or other reference for the subject from line one.\nFor example:\nSnow\nSilent, white\nDancing, falling, drifting\nCovering everything it touches\nBlanket"}
{"id": "7067", "revid": "1009157402", "url": "https://en.wikipedia.org/wiki?curid=7067", "title": "Cook Islands", "text": "The Cook Islands (Cook Islands M\u0101ori: \"K\u016bki '\u0100irani\") are a self-governing island country in the South Pacific Ocean in free association with New Zealand. It comprises 15 islands whose total land area is . The Cook Islands' Exclusive Economic Zone (EEZ) covers of ocean.\nNew Zealand is responsible for the Cook Islands' defence and foreign affairs, but these responsibilities are exercised in consultation with the Cook Islands. In recent times, the Cook Islands have adopted an increasingly independent foreign policy. Cook Islanders are citizens of New Zealand, but they also have the status of Cook Islands nationals, which is not given to other New Zealand citizens. The Cook Islands have been an active member of the Pacific Community since 1980.\nThe Cook Islands' main population centres are on the island of Rarotonga (13,007 in 2016), where there is an international airport. There is also a larger population of Cook Islanders in New Zealand itself: in the 2013 census, 61,839 people said they were Cook Islanders, or of Cook Islands descent.\nWith over 168,000 visitors travelling to the islands in 2018, tourism is the country's main industry, and the leading element of the economy, ahead of offshore banking, pearls, and marine and fruit exports.\nHistory.\nThe Cook Islands were first settled around AD 1000 by Polynesian people who are thought to have migrated from Tahiti, an island to the northeast of the main island of Rarotonga.\nThe first European contact with the islands took place in 1595 when the Spanish navigator \u00c1lvaro de Menda\u00f1a de Neira sighted the island of Pukapuka, which he named \"San Bernardo\" (Saint Bernard). Pedro Fernandes de Queir\u00f3s, a Portuguese captain at the service of the Spanish Crown, made the first European landing in the islands when he set foot on Rakahanga in 1606, calling the island \"Gente Hermosa\" (Beautiful People).\nThe British navigator Captain James Cook arrived in 1773 and again in 1777 giving the island of Manuae the name \"Hervey Island\". The \"Hervey Islands\" later came to be applied to the entire southern group. The name \"Cook Islands\", in honour of Cook, first appeared on a Russian naval chart published by Adam Johann von Krusenstern in the 1820s.\nIn 1813 John Williams, a missionary on the \"Endeavour\" (not the same ship as Cook's) made the first recorded European sighting of Rarotonga. The first recorded landing on Rarotonga by Europeans was in 1814 by the \"Cumberland\"; trouble broke out between the sailors and the Islanders and many were killed on both sides. The islands saw no more Europeans until English missionaries arrived in 1821. Christianity quickly took hold in the culture and many islanders are Christians today.\nThe islands were a popular stop in the 19th century for whaling ships from the United States, Britain and Australia. They visited, from at least 1826, to obtain water, food, and firewood. Their favourite islands were Rarotonga, Aitutaki, Mangaia and Penrhyn.\nThe Cook Islands became a British protectorate in 1888, largely because of community fears that France might occupy the islands as it already had Tahiti. On 6 September 1900, the islanders' leaders presented a petition asking that the islands (including Niue \"if possible\") should be annexed as British territory. On 8 and 9 October 1900, seven instruments of cession of Rarotonga and other islands were signed by their chiefs and people. A British Proclamation was issued, stating that the cessions were accepted and the islands declared parts of Her Britannic Majesty's dominions. However, it did not include Aitutaki. Even though the inhabitants regarded themselves as British subjects, the Crown's title was unclear until the island was formally annexed by that Proclamation. In 1901 the islands were included within the boundaries of the Colony of New Zealand by Order in Council under the Colonial Boundaries Act, 1895 of the United Kingdom. The boundary change became effective on 11 June 1901, and the Cook Islands have had a formal relationship with New Zealand since that time.\nWhen the British Nationality and New Zealand Citizenship Act 1948 came into effect on 1 January 1949, Cook Islanders who were British subjects automatically gained New Zealand citizenship. The islands remained a New Zealand dependent territory until the New Zealand Government decided to grant them self-governing status. On 4 August 1965, a constitution was promulgated. The first Monday in August is celebrated each year as Constitution Day. Albert Henry of the Cook Islands Party was elected as the first Premier and was knighted by Queen Elizabeth II. Henry led the nation until 1978, when he was accused of vote-rigging and resigned. He was stripped of his knighthood in 1979. He was succeeded by Tom Davis of the Democratic Party who held that position until March 1983.\nIn March 2019, it was reported that the Cook Islands had plans to change its name and remove the reference to Captain James Cook in favour of \"a title that reflects its 'Polynesian nature'\". It was later reported in May 2019 that the proposed name change had been poorly received by the Cook Islands diaspora. As a compromise, it was decided that the English name of the islands would not be altered, but that a new Cook Islands M\u0101ori name would be adopted to replace the current name, a transliteration from English. Discussions over the name continued in 2020.\nGeography.\nThe Cook Islands are in the South Pacific Ocean, north-east of New Zealand, between French Polynesia and American Samoa. There are 15\u00a0major islands spread over of ocean, divided into two distinct groups: the Southern Cook Islands and the Northern Cook Islands of coral atolls.\nThe islands were formed by volcanic activity; the northern group is older and consists of six atolls, which are sunken volcanoes topped by coral growth. The climate is moderate to tropical. The Cook Islands consist of 15 islands and two reefs. From March to December, the Cook Islands are in the path of tropical cyclones, the most notable of which were the cyclones Martin and Percy. Two terrestrial ecoregions lie within the islands' territory: Central Polynesian tropical moist forests and Cook Islands tropical moist forests.\nThe table is ordered from north to south. Population figures from the 2016 census.\nPolitics and foreign relations.\nThe Cook Islands are a representative democracy with a parliamentary system in an associated state relationship with New Zealand. Executive power is exercised by the government, with the Chief Minister as head of government. Legislative power is vested in both the government and the Parliament of the Cook Islands. There is a multi-party system. The Judiciary is independent of the executive and the legislature. The head of state is the Queen of New Zealand, who is represented in the Cook Islands by the Queen's Representative.\nThe islands are self-governing in \"free association\" with New Zealand. New Zealand retains primary responsibility for external affairs, acting in consultation with the Cook Islands government. Cook Islands nationals are citizens of New Zealand and can receive New Zealand government services, but the reverse is not true; New Zealand citizens are not Cook Islands nationals. Despite this, , the Cook Islands had diplomatic relations in its own name with 52 other countries. The Cook Islands is not a United Nations member state, but, along with Niue, has had their \"full treaty-making capacity\" recognised by the United Nations Secretariat, and is a full member of the WHO, UNESCO, the International Civil Aviation Organization and the UN Food and Agriculture Organization, all UN specialised agencies, and is an associate member of the Economic and Social Commission for Asia and the Pacific (UNESCAP) and a Member of the Assembly of States of the International Criminal Court.\nOn 11 June 1980, the United States signed a treaty with the Cook Islands specifying the maritime border between the Cook Islands and American Samoa and also relinquishing any American claims to Penrhyn Island, Pukapuka, Manihiki, and Rakahanga. In 1990 the Cook Islands and France signed a treaty that delimited the boundary between the Cook Islands and French Polynesia. In late August 2012, United States Secretary of State Hillary Clinton visited the islands. In 2017, the Cook Islands signed the UN treaty on the Prohibition of Nuclear Weapons.\nHuman rights.\nMale homosexuality is illegal in the Cook Islands and is punishable by a maximum term of seven years imprisonment.\nAdministrative subdivisions.\nThere are island councils on all of the inhabited outer islands (Outer Islands Local Government Act 1987 with amendments up to\u00a02004, and Palmerston Island Local Government Act 1993) except Nassau, which is governed by Pukapuka (Suwarrow, with only one caretaker living on the island, also governed by Pukapuka, is not counted with the inhabited islands in this context). Each council is headed by a mayor.\nThe three \"Vaka\" councils of Rarotonga established in 1997 (\"Rarotonga Local Government Act 1997\"), also headed by mayors, were abolished in February 2008, despite much controversy.\nOn the lowest level, there are village committees. Nassau, which is governed by Pukapuka, has an island committee (Nassau Island Committee), which advises the Pukapuka Island Council on matters concerning its own island.\nDemographics.\nBirths and deaths\nReligion.\nIn the Cook Islands the Church is separate from the state, and most of the population is Christian. The religious distribution is as follows:\nThe various Protestant groups account for 62.8% of the believers, the most followed denomination being the Cook Islands Christian Church with 49.1%. Other Protestant Christian groups include Seventh-day Adventist 7.9%, Assemblies of God 3.7% Apostolic Church 2.1%. The main non-Protestant group is Roman Catholics with 17% of the population. The Church of Jesus Christ of Latter-day Saints makes up 4.4%.\nEconomy.\nThe economy is strongly affected by geography. It is isolated from foreign markets, and has some inadequate infrastructure; it lacks major natural resources, has limited manufacturing and suffers moderately from natural disasters. Tourism provides the economic base that makes up approximately 67.5% of GDP. Additionally, the economy is supported by foreign aid, largely from New Zealand. China has also contributed foreign aid, which has resulted in, among other projects, the Police Headquarters building. The Cook Islands is expanding its agriculture, mining and fishing sectors, with varying success.\nSince approximately 1989, the Cook Islands have become a location specialising in so-called asset protection trusts, by which investors shelter assets from the reach of creditors and legal authorities. According to \"The New York Times\", the Cooks have \"laws devised to protect foreigners' assets from legal claims in their home countries\", which were apparently crafted specifically to thwart the long arm of American justice; creditors must travel to the Cook Islands and argue their cases under Cooks law, often at prohibitive expense. Unlike other foreign jurisdictions such as the British Virgin Islands, the Cayman Islands and Switzerland, the Cooks \"generally disregard foreign court orders\" and do not require that bank accounts, real estate, or other assets protected from scrutiny (it is illegal to disclose names or any information about Cooks trusts) be physically located within the archipelago. Taxes on trusts and trust employees account for some 8% of the Cook Islands economy, behind tourism but ahead of fishing.\nIn recent years, the Cook Islands has gained a reputation as a debtor paradise, through the enactment of legislation that permits debtors to shield their property from the claims of creditors.\nInfrastructure.\nThere are eleven airports in the Cook Islands, including one with a paved runway, Rarotonga International Airport, served by four passenger airlines.\nCulture.\nNewspapers.\nNewspapers in the Cook Islands are usually published in English with some articles in Cook Islands M\u0101ori. The Cook Islands News has been published since 1945, although it was owned by the government until 1989. Former newspapers include Te Akatauira, which was published from 1978 to 1980.\nLanguage.\nThe languages of the Cook Islands include English, Cook Islands M\u0101ori (or \"Rarotongan\"), and Pukapukan. Dialects of Cook Islands Maori include Penrhyn; Rakahanga-Manihiki; the Ngaputoru dialect of Atiu, Mitiaro, and Mauke; the Aitutaki dialect; and the Mangaian dialect. Cook Islands Maori and its dialectic variants are closely related to both Tahitian and to New Zealand M\u0101ori. Pukapukan is considered closely related to the Samoan language. English and Cook Islands M\u0101ori are official languages of the Cook Islands; per the Te Reo Maori Act. The legal definition of Cook Islands M\u0101ori includes Pukapukan.\nMusic.\nMusic in the Cook Islands is varied, with Christian songs being quite popular, but traditional dancing and songs in Polynesian languages remain popular.\nArt.\nCarving.\nWoodcarving is a common art form in the Cook Islands. The proximity of islands in the southern group helped produce a homogeneous style of carving but that had special developments in each island. Rarotonga is known for its fisherman's gods and staff-gods, Atiu for its wooden seats, Mitiaro, Mauke and Atiu for mace and slab gods and Mangaia for its ceremonial adzes. Most of the original wood carvings were either spirited away by early European collectors or were burned in large numbers by missionaries. Today, carving is no longer the major art form with the same spiritual and cultural emphasis given to it by the Maori in New Zealand. However, there are continual efforts to interest young people in their heritage and some good work is being turned out under the guidance of older carvers. Atiu, in particular, has a strong tradition of crafts both in carving and local fibre arts such as tapa. Mangaia is the source of many fine adzes carved in a distinctive, idiosyncratic style with the so-called double-k design. Mangaia also produces food pounders carved from the heavy calcite found in its extensive limestone caves.\nWeaving.\nThe outer islands produce traditional weaving of mats, basketware and hats. Particularly fine examples of rito hats are worn by women to church. They are made from the uncurled immature fibre of the coconut palm and are of very high quality. The Polynesian equivalent of Panama hats, they are highly valued and are keenly sought by Polynesian visitors from Tahiti. Often, they are decorated with hatbands made of minuscule pupu shells that are painted and stitched on by hand. Although pupu are found on other islands the collection and use of them in decorative work has become a speciality of Mangaia. The weaving of rito is a speciality of the northern islands, Manihiki, Rakahanga and Penrhyn.\nTivaevae.\nA major art form in the Cook Islands is tivaevae. This is, in essence, the art of handmade Island scenery patchwork quilts. Introduced by the wives of missionaries in the 19th century, the craft grew into a communal activity, which is probably one of the main reasons for its popularity.\nContemporary art.\nThe Cook Islands has produced internationally recognised contemporary artists, especially in the main island of Rarotonga. Artists include painter (and photographer) Mahiriki Tangaroa, sculptors Eruera (Ted) Nia (originally a film maker) and master carver Mike Tavioni, painter (and Polynesian tattoo enthusiast) Upoko'ina Ian George, Aitutakian-born painter Tim Manavaroa Buchanan, Loretta Reynolds, Judith Kunzl\u00e9, Joan Rolls Gragg, Kay George (who is also known for her fabric designs), Apii Rongo, Varu Samuel, and multi-media, installation and community-project artist Ani O'Neill, all of whom currently live on the main island of Rarotonga. Atiuan-based Andrea Eimke is an artist who works in the medium of tapa and other textiles, and also co-authored the book 'Tivaivai \u2013 The Social Fabric of the Cook Islands' with British academic Susanne Kuechler. Many of these artists have studied at university art schools in New Zealand and continue to enjoy close links with the New Zealand art scene.\nNew Zealand-based Cook Islander artists include Michel Tuffery, print-maker David Teata, Richard Shortland Cooper, Sylvia Marsters and Jim Vivieaere.\nOn Rarotonga, the main commercial galleries are Beachcomber Contemporary Art (Taputapuatea, Avarua) run by Ben &amp; Trevon Bergman, and The Art Studio Gallery (Arorangi) run by Ian and Kay George. The Cook Islands National Museum also exhibits art.\nSport.\nRugby league is the most popular sport in the Cook Islands."}
{"id": "7068", "revid": "1273126", "url": "https://en.wikipedia.org/wiki?curid=7068", "title": "History of the Cook Islands", "text": "The Cook Islands are named after Captain James Cook, who visited the islands in 1773 and 1777, although Spanish navigator Alvaro de Menda\u00f1a was the first European to reach the islands in 1595. The Cook Islands became a British protectorate in 1888.\nBy 1900, the islands were annexed as British territory. In 1901, the islands were included within the boundaries of the Colony of New Zealand.\nThe Cook Islands contain 15 islands in the group spread over a vast area in the South Pacific. The majority of islands are low coral atolls in the Northern Group, with Rarotonga, a volcanic island in the Southern Group, as the main administration and government centre. The main Cook Islands language is Rarotongan M\u0101ori. There are some variations in dialect in the 'outer' islands.\nEarly settlers of the Cooks.\nIt is thought that the Cook Islands may have been settled between the years 900-1200 AD. Early settlements suggest that the settlers migrated from Tahiti, to the northeast of the Cooks. The Cook Islands continue to hold important connections with Tahiti, and this is generally found in the two countries' culture, tradition and language. It is also thought that the early settlers were true Tahitians, who landed in Rarotonga (Takitumu district). There are notable historic epics of great warriors who travel between the two nations for a wide variety of reasons. The purpose of these missions is still unclear but recent research indicates that large to small groups often fled their island due to local wars being forced upon them. For each group to travel and to survive, they would normally rely on a warrior to lead them. Outstanding warriors are still mentioned in the countries' traditions and stories.\nThese arrivals are evidenced by an older road in Toi, the \"Ara Metua\", which runs around most of Rarotonga, and is believed to be at least 1200 years old. This 29\u00a0km long, paved road is a considerable achievement of ancient engineering, possibly unsurpassed elsewhere in Polynesia. The islands of Manihiki and Rakahanga trace their origins to the arrival of Toa, an outcast from Rarotonga, and Tupaeru, a high-ranking woman from the Puaikura tribe of Rarotonga. The remainder of the northern islands were probably settled by expeditions from Samoa.\nEarly European contact.\nSpanish ships visited the islands in the 16th century; the first written record of contact between Europeans and the native inhabitants of the Cook Islands came with the sighting of Pukapuka by Spanish sailor \u00c1lvaro de Menda\u00f1a in 1595, who called it \"San Bernardo\" (Saint Bernard). Portuguese-Spaniard Pedro Fern\u00e1ndez de Quir\u00f3s made the first recorded European landing in the islands when he set foot on Rakahanga in 1606, calling it \"Gente Hermosa\" (Beautiful People).\nBritish navigator Captain James Cook arrived in 1773 and 1777. Cook named the islands the 'Hervey Islands' to honour a British Lord of the Admiralty. Half a century later, the Russian Baltic German Admiral Adam Johann von Krusenstern published the \"Atlas de l'Ocean Pacifique\", in which he renamed the islands the Cook Islands to honour Cook. Captain Cook navigated and mapped much of the group. Surprisingly, Cook never sighted the largest island, Rarotonga, and the only island that he personally set foot on was the tiny, uninhabited Palmerston Atoll.\nThe first recorded landing by Europeans was in 1814 by the \"Cumberland\"; trouble broke out between the sailors and the Islanders and many were killed on both sides.\nThe islands saw no more Europeans until missionaries arrived from England in 1821. Christianity quickly took hold in the culture and remains the predominant religion today.\nIn 1823, Captain John Dibbs of the colonial barque \"Endeavour\" made the first official sighting of the island Rarotonga. The \"Endeavour\" was transporting Rev. John Williams on a missionary voyage to the islands.\nBrutal Peruvian slave traders, known as blackbirders, took a terrible toll on the islands of the Northern Group in 1862 and 1863. At first, the traders may have genuinely operated as labour recruiters, but they quickly turned to subterfuge and outright kidnapping to round up their human cargo. The Cook Islands was not the only island group visited by the traders, but Penrhyn Atoll was their first port of call and it has been estimated that three-quarters of the population was taken to Callao, Peru. Rakahanga and Pukapuka also suffered tremendous losses.\nBritish protectorate.\nThe Cook Islands became a British protectorate in 1888, due largely to community fears that France might occupy the territory as it had Tahiti. On 6 September 1900, the leading islanders presented a petition asking that the islands (including Niue \"if possible\") should be annexed as British territory. On 8\u20139 October 1900, seven instruments of cession of Rarotonga and other islands were signed by their chiefs and people, and a British proclamation issued at the same time accepted the cessions, the islands being declared parts of Her Britannic Majesty's dominions. These instruments did not include Aitutaki. It appears that, though the inhabitants regarded themselves as British subjects, the Crown's title was uncertain, and the island was formally annexed by Proclamation dated 9 October 1900. The islands were included within the boundaries of the Colony of New Zealand in 1901 by Order in Council under the Colonial Boundaries Act, 1895 of the United Kingdom. The boundary change became effective on 11 June 1901, and the Cook Islands have had a formal relationship with New Zealand since that time.\nRecent history.\nThe islands remained a New Zealand dependent territory until 1965, at which point they became a self-governing territory in free association with New Zealand. The first Prime Minister Albert Henry led the country until 1978 when he was accused of vote-rigging. Since 1965, the Cook Islands have been essentially independent (self-governing in free association with New Zealand), but remained officially placed under New Zealand sovereignty. New Zealand is tasked with overseeing the country's foreign relations and defense. The Cook Islands, Niue, and New Zealand (with its territories: Tokelau and the Ross Dependency) make up the Realm of New Zealand.\nAfter achieving autonomy in 1965, the Cook Islands elected Albert Henry of the Cook Islands Party as their first Prime Minister. He was succeeded in 1978 by Tom Davis of the Democratic Party.\nOn 11 June 1980, the United States signed a treaty with the Cook Islands specifying the maritime border between the Cook Islands and American Samoa and also relinquishing the US claim to the islands of Penrhyn, Pukapuka, Manihiki, and Rakahanga. In 1990, the Cook Islands signed a treaty with France which delimited the maritime boundary between the Cook Islands and French Polynesia.\nOn June 13, 2008, a small majority of members of the House of Ariki attempted a coup, claiming to dissolve the elected government and to take control of the country's leadership. \"Basically we are dissolving the leadership, the prime minister and the deputy prime minister and the ministers,\" chief Makea Vakatini Joseph Ariki explained. The \"Cook Islands Herald\" suggested that the \"ariki\" were attempting thereby to regain some of their traditional prestige or \"mana\". Prime Minister Jim Marurai described the take-over move as \"ill-founded and nonsensical\". By June 23, the situation appeared to have normalised, with members of the House of Ariki accepting to return to their regular duties.\nTimeline.\n900 - first People arrive to the islands \n1595 \u2014 Spaniard \u00c1lvaro de Menda\u00f1a de Neira is the first European to sight the islands.\n1606 \u2014 Portuguese-Spaniard Pedro Fern\u00e1ndez de Quir\u00f3s makes the first recorded European landing in the islands when he sets foot on Rakahanga.\n1773 \u2014 Captain James Cook explores the islands and names them the Hervey Islands. Fifty years later they are renamed in his honour by Russian Admiral Adam Johann von Krusenstern.\n1821 \u2014 English and Tahitian missionaries land in Aitutaki, become the first non-Polynesian settlers.\n1823 \u2014 English missionary John Williams lands in Rarotonga, converting Makea Pori Ariki to Christianity.\n1858 \u2014 The Cook Islands become united as a state, the Kingdom of Rarotonga.\n1862 \u2014 Peruvian slave traders take a terrible toll on the islands of Penrhyn, Rakahanga and Pukapuka in 1862 and 1863.\n1888 \u2014 Cook Islands are proclaimed a British protectorate and a single federal parliament is established.\n1900 \u2014 The Cook Islands are ceded to the United Kingdom as British territory, except for Aitutaki which was annexed by the United Kingdom at the same time.\n1901 \u2014 The boundaries of the Colony of New Zealand are extended by the United Kingdom to include the Cook Islands.\n1924 \u2014 The All Black \"Invincibles\" stop in Rarotonga on their way to the United Kingdom and play a friendly match against a scratch Rarotongan team.\n1946 \u2014 Legislative Council is established. For the first time since 1912, the territory has direct representation.\n1957 \u2014 Legislative Council is reorganized as the Legislative Assembly.\n1965 \u2014 The Cook Islands become a self-governing territory in free association with New Zealand. Albert Henry, leader of the Cook Islands Party, is elected as the territory's first prime minister.\n1974 \u2014 Albert Henry is knighted by Queen Elizabeth II\n1979 \u2014 Sir Albert Henry is found guilty of electoral fraud and stripped of his premiership and his knighthood. Tom Davis becomes Premier.\n1980 \u2014 Cook Islands \u2013 United States Maritime Boundary Treaty establishes the Cook Islands \u2013 American Samoa boundary\n1981 \u2014 Constitution is amended. Legislative Assembly is renamed Parliament, which grows from 22 to 24 seats, and the parliamentary term is extended from four to five years. Tom Davis is knighted.\n1984 \u2014 The country's first coalition government, between Sir Thomas and Geoffrey Henry, is signed in the lead up to hosting regional Mini Games in 1985. Shifting coalitions saw ten years of political instability. At one stage, all but two MPs were in government.\n1985 \u2014 Rarotonga Treaty is opened for signing in the Cook Islands, creating a nuclear-free zone in the South Pacific.\n1986 \u2014 In January 1986, following the rift between New Zealand and the US in respect of the ANZUS security arrangements Prime Minister Tom Davis declared the Cook Islands a neutral country, because he considered that New Zealand (which has control over the islands' defence and foreign policy) was no longer in a position to defend the islands. The proclamation of neutrality meant that the Cook Islands would not enter into a military relationship with any foreign power, and, in particular, would prohibit visits by US warships. Visits by US naval vessels were allowed to resume by Henry's Government.\n1990 \u2014 Cook Islands \u2013 France Maritime Delimitation Agreement establishes the Cook Islands\u2013French Polynesia boundary\n1991 \u2014 The Cook Islands signed a treaty of friendship and co-operation with France, covering economic development, trade and surveillance of the islands' EEZ. The establishment of closer relations with France was widely regarded as an expression of the Cook Islands' Government's dissatisfaction with existing arrangements with New Zealand which was no longer in a position to defend the Cook Islands.\n1995 \u2014 The French Government resumed its programme of nuclear-weapons testing at Mururoa Atoll in September 1995 upsetting the Cook Islands. New Prime Minister Geoffrey Henry was fiercely critical of the decision and dispatched a \"vaka\" (traditional voyaging canoe) with a crew of Cook Islands' traditional warriors to protest near the test site. The tests were concluded in January 1996 and a moratorium was placed on future testing by the French government.\n1997 \u2014 Full diplomatic relations established with the People's Republic of China.\n1997 \u2014 In November, Cyclone Martin in Manihiki kills at least six people; 80% of buildings are damaged and the black pearl industry suffered severe losses.\n1999 \u2014 A second era of political instability begins, starting with five different coalitions in less than nine months, and at least as many since then.\n2000 \u2014 Full diplomatic relations concluded with France.\n2002 \u2014 Prime Minister Terepai Maoate is ousted from government following second vote of no-confidence in his leadership.\n2004 \u2014 Prime Minister Robert Woonton visits China; Chinese Premier Wen Jiabao grants $16 million in development aid.\n2006 \u2014 Parliamentary elections held. The Democratic Party keeps majority of seats in parliament, but is unable to command a majority for confidence, forcing a coalition with breakaway MPs who left, then rejoined the \"Demos\".\n2008 \u2014 Pacific Island nations imposed a series of measures aimed at halting overfishing."}
{"id": "7069", "revid": "1027977", "url": "https://en.wikipedia.org/wiki?curid=7069", "title": "Geography of the Cook Islands", "text": "The Cook Islands can be divided into two groups: the Southern Cook Islands and the Northern Cook Islands. The country is located in Oceania, in the South Pacific Ocean, about one-half of the way from Hawaii to New Zealand."}
{"id": "7070", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=7070", "title": "Demographics of the Cook Islands", "text": "This article is about the demographic features of the population of the Cook Islands, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.\nA census is carried out every five years in the Cook Islands. The last census was carried out in 2016 and the next census will be carried out in December 2021.\nVital statistics.\nBirths and deaths\nCIA World Factbook demographic statistics.\nThe following demographic statistics are from the CIA World Factbook, unless otherwise indicated."}
{"id": "7071", "revid": "36465295", "url": "https://en.wikipedia.org/wiki?curid=7071", "title": "Politics of the Cook Islands", "text": "The politics of the Cook Islands, an associated state, takes place in a framework of a parliamentary representative democracy within a constitutional monarchy. The Queen of New Zealand, represented in the Cook Islands by the Queen's Representative, is the Head of State; the prime minister is the head of government and of a multi-party system. The Islands are self-governing in free association with New Zealand and are fully responsible for internal affairs. New Zealand retains some responsibility for external affairs, in consultation with the Cook Islands. In recent years, the Cook Islands have taken on more of its own external affairs; as of 2005, it has diplomatic relations in its own name with eighteen other countries. Executive power is exercised by the government, while legislative power is vested in both the government and the islands' parliament. The judiciary is independent of the executive and the legislatures.\nConstitution.\nThe Constitution of the Cook Islands took effect on August 4, 1965, when the Cook Islands became a self-governing territory in free association with New Zealand. The anniversary of these events in 1965 is commemorated annually on Constitution Day, with week long activities known as \"Te Maeva Nui Celebrations\" locally.\nExecutive.\nThe monarch is hereditary; her representative is appointed by the monarch on the recommendation of the Cook Islands Government. The cabinet is chosen by the prime minister and collectively responsible to Parliament.\nTen years of rule by the Cook Islands Party (CIP) came to an end 18 November 1999 with the resignation of Prime Minister Joe Williams. Williams had led a minority government since October 1999 when the New Alliance Party (NAP) left the government coalition and joined the main opposition Democratic Party (DAP). On 18 November 1999, DAP leader Dr. Terepai Maoate was sworn in as prime minister. He was succeeded by his co-partisan Robert Woonton. When Dr Woonton lost his seat in the 2004 elections, Jim Marurai took over. In the 2010 elections, the CIP regained power and Henry Puna was sworn in as prime minister on 30 November 2010.\nFollowing uncertainty about the ability of the government to maintain its majority, the Queen's representative dissolved parliament midway through its term and a 'snap' election was held on 26 September 2006. Jim Marurai's Democratic Party retained the Treasury benches with an increased majority.\nThe New Zealand High Commissioner is appointed by the New Zealand Government.\nLegislature.\nThe Parliament of the Cook Islands has 24 members, elected for a five-year term in single-seat constituencies. There is also a House of Ariki, composed of chiefs, which has a purely advisory role. The Koutu Nui is a similar organization consisting of sub-chiefs. It was established by an amendment in 1972 of the 1966 House of Ariki Act. The current president is Te Tika Mataiapo Dorice Reid.\nOn June 13, 2008, a small majority of members of the House of Ariki attempted a coup, claiming to dissolve the elected government and to take control of the country's leadership. \"Basically we are dissolving the leadership, the prime minister and the deputy prime minister and the ministers,\" chief Makea Vakatini Joseph Ariki explained. The \"Cook Islands Herald\" suggested that the \"ariki\" were attempting thereby to regain some of their traditional prestige or \"mana\". Prime Minister Jim Marurai described the take-over move as \"ill-founded and nonsensical\". By June 23, the situation appeared to have normalised, with members of the House of Ariki accepting to return to their regular duties.\nJudiciary.\nThe judiciary is established by part IV of the Constitution, and consists of the High Court of the Cook Islands and the Cook Islands Court of Appeal. The Judicial Committee of the Privy Council serves as a final court of appeal. Judges are appointed by the Queen's Representative on the advice of the Executive Council as given by the Chief Justice and the Minister of Justice. Non-resident Judges are appointed for a three-year term; other Judges are appointed for life. Judges may be removed from office by the Queen's Representative on the recommendation of an investigative tribunal and only for inability to perform their office, or for misbehaviour.\nWith regard to the legal profession, Iaveta Taunga o Te Tini Short was the first Cook Islander to establish a law practice in 1968. He would later become a Cabinet Minister (1978) and High Commissioner for the Cook Islands (1985).\nRecent political history.\nThe 1999 election produced a hung Parliament. Cook Islands Party leader Geoffrey Henry remained prime minister, but was replaced after a month by Joe Williams following a coalition realignment. A further realignment three months later saw Williams replaced by Democratic Party leader Terepai Maoate. A third realignment saw Maoate replaced mid-term by his deputy Robert Woonton in 2002, who ruled with the backing of the CIP.\nThe Democratic Party won a majority in the 2004 election, but Woonton lost his seat, and was replaced by Jim Marurai. In 2005 Marurai left the Democrats due to an internal disputes, founding his own Cook Islands First Party. He continued to govern with the support of the CIP, but in 2005 returned to the Democrats. The loss of several by-elections forced a snap-election in 2006, which produced a solid majority for the Democrats and saw Marurai continue as prime minister.\nIn December 2009, Marurai sacked his Deputy Prime Minister, Terepai Maoate, sparking a mass-resignation of Democratic Party cabinet members He and new Deputy Prime Minister Robert Wigmore were subsequently expelled from the Democratic Party. Marurai appointed three junior members of the Democratic party to Cabinet, but on 31 December 2009 the party withdrew its support.\nIn May 2014 a new party was formed by Teina Bishop of Aitutaki \"One Cook Islands\" Party."}
{"id": "7072", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=7072", "title": "Economy of the Cook Islands", "text": "The economy of the Cook Islands is based mainly on tourism, with minor exports made up of tropical and citrus fruit. Manufacturing activities are limited to fruit-processing, clothing and handicrafts.\nAs in many other South Pacific nations, the Cook Islands's economy is hindered by the country's isolation from foreign markets, lack of natural resources aside from fish, periodic devastation from natural disasters, and inadequate infrastructure.\nTrade deficits are made up for by remittances from emigrants and by foreign aid, overwhelmingly from New Zealand. Efforts to exploit tourism potential, encourage offshore banking, and expand the mining and fishing industries have been partially successful in stimulating investment and growth.\nBanking and finance.\nThe Cook Islands has \"Home Rule\" with respect to banking, similar to Guernsey, Jersey and the Isle of Man.\nThis \"Home Rule\" banking confuses New Zealanders on vacation in the Cooks. Cook automated teller machines often fail to fully disclose the fact that the Cooks are not part of the New Zealand banking system, thus legally requiring banks to charge the same fees for withdrawing or transferring money as if the person was in Australia or the EU. The New Zealand dollar is the official currency of the Cook Islands, adding to the confusion. Cook Islanders are NZ citizens.\nThe banking and incorporation laws of the Cook Islands make it an important centre for setting up companies that are involved in global trade.\nTelecommunications.\nTelecom Cook Islands Ltd (TCI) is the sole provider of telecommunications in the Cook Islands. TCI is a private company owned by Spark New Zealand Ltd (60%) and the Cook Islands Government (40%). In operation since July 1991, TCI provides local, national and international telecommunications as well as internet access on all islands except Suwarrow. Communications to Suwarrow is via HF radio."}
{"id": "7073", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=7073", "title": "Telecommunications in the Cook Islands", "text": "Like most countries and territories in Oceania, telecommunications in the Cook Islands is limited by its isolation and low population, with only one major television broadcasting station and six radio stations. However, most residents have a main line or mobile phone. Its telecommunications are mainly provided by Telecom Cook Islands, who is currently working with O3b Networks, Ltd. for faster Internet connection.\nTelephone.\nIn July 2012, there were about 7,500 main line telephones, which covers about 98% of the country's population. There were approximately 7,800 mobile phones in 2009. Telecom Cook Islands, owned by Spark New Zealand, is the islands' main telephone system and offers international direct dialling, Internet, email, fax, and Telex. The individual islands are connected by a combination of satellite earth stations, microwave systems, and very high frequency and high frequency radiotelephone; within the islands, service is provided by small exchanges connected to subscribers by open wire, cable, and fibre optic cable. For international communication, they rely on the satellite earth station Intelsat.\nIn 2003, the largest island of Rarotonga started using a GSM/GPRS mobile data service system with GSM 900 by 2013 3G UMTS 900 was introduce covering 98% of Rarotonga with HSPA+. In March 2017 4G+ launch in Rarotonga with LTE700 (B28A) and LTE1800 (B3) .\nMobile service covers Aitutaki GSM/GPRS mobile data service system in GSM 900 from 2006 to 2013 while in 2014 3G UMTS 900 was introduce with HSPA+ stand system. In March 2017 4G+ also launch in Aitutaki with LTE700 (B28A).\nThe rest of the Outer Islands (Pa Enua) mobile was well establish in 2007 with mobile coverage at GSM 900 from Mangaia 3 villages (Oneroa, Ivirua, Tamarua), Atiu, Mauke, Mitiaro, Palmerston in the Southern Group (Pa Enua Tonga) and the Northern Group (Pa Enua Tokerau) Nassau, Pukapuka, Rakahanga, Manihiki 2 Village (Tukao, Tauhunu) and Penrhyn 2 villages (Omoka Tetautua).\nThe Cook Islands uses the country calling code +682.\nBroadcasting.\nThere are six radio stations in the Cook Islands, with one reaching all islands. there were 14,000 radios.\nCook Islands Television broadcasts from Rarotonga, providing a mix of local news and overseas-sourced programs. there were 4,000 television sets.\nInternet.\nThere were 6,000 Internet users in 2009 and 3,562 Internet hosts as of 2012. The country code top-level domain for the Cook Islands is .ck.\nIn June 2010, Telecom Cook Islands partnered with O3b Networks, Ltd. to provide faster Internet connection to the Cook Islands. On 25 June 2013 the O3b satellite constellation was launched from an Arianespace Soyuz ST-B rocket in French Guiana. The medium Earth orbit satellite orbits at and uses the Ka band. It has a latency of about 100 milliseconds because it is much closer to Earth than standard geostationary satellites, whose latencies can be over 600 milliseconds. Although the initial launch consisted of 4 satellites, as many as 20 may be launched eventually to serve various areas with little or no optical fibre service, the first of which is the Cook Islands.\nIn December 2015, Alcatel-Lucent and Bluesky Pacific Group announced that they would build the Moana Cable system connecting New Zealand to Hawaii with a single fibre pair branching off to the Cook Islands. The Moana Cable is expected to be completed in 2018.\nIn July 2020 the Cook Islands were connected to the [[Manatua One Polynesia Fibre Cable]], which links the Cook Islands, [[Niue]], [[Samoa]] and [[Tahiti]]. The cable has landing points at [[Rarotonga]] and [[Aitutaki]].\nExternal links.\n[[Category:Communications in the Cook Islands| ]]\n[[Category:Telecommunications in Oceania|+Cook]]"}
{"id": "7074", "revid": "1027977", "url": "https://en.wikipedia.org/wiki?curid=7074", "title": "Transport in the Cook Islands", "text": "This article lists transport in the Cook Islands.\nRoad transport.\nThe Cook Islands uses left-handed traffic. The maximum speed limit is 50 km/h. On the main island of Rarotonga, there are no traffic lights and only two roundabouts. A bus operates clockwise and anti-clockwise services around the islands coastal ring-road.\nRoad safety is poor. In 2011, the Cook Islands had the second-highest per-capita road deaths in the world. In 2018, crashes neared a record high, with speeding, alcohol and careless behaviour being the main causes. Motor-scooters are a common form of transport, but there was no requirement for helmets, making them a common cause of death and injuries. Legislation requiring helmets was passed in 2007, but scrapped in early 2008 before it came into force. In 2016 a law was passed requiring visitors and riders aged 16 to 25 to wear helmets, but it was widely flouted. In March 2020 the Cook Islands parliament again legislated for compulsory helmets to be worn from June 26, but implementation was delayed until July 31, and then until September 30.\nRail transport.\nThe Cook Islands has no effective rail transport. Rarotonga had a 170m tourist railway, the Rarotonga Steam Railway, but it is no longer in working condition.\nWater transport.\nThe Cook Islands have a long history of sea transport. The islands were colonised from Tahiti, and in turn colonised New Zealand in ocean-going waka. In the late nineteenth century, following European contact, the islands had a significant fleet of schooners, which they used to travel between islands and to trade with Tahiti and New Zealand. In 1899, locally owned shipping carried 10% of all international trade to the islands, and 66% of all trade carried by sail. Indigenous-owned shipping was driven out of business following New Zealand's acquisition of the islands, replaced by government-owned vessels, New Zealand trading companies, and the steamships of the Union Steamship Company.\nInternational shipping is provided by Pacific Forum Line and Matson, Inc. (as EXCIL shipping). Only the port of Avatiu can handle containers, with ships unloading at Aitutaki using lighters.\nThere are two inter-island shipping companies: Taio Shipping, operating two vessels, and Cook Islands Towage, operating one. \nIn the past, shipping interruptions have led to shortages of imported goods and fuel, and electricity blackouts on the outer islands. Shipping has frequently been subsidised to ensure service. In 2019 the Cook Islands government announced that it would acquire a dedicated cargo ship for the outer islands after Cook Islands Towage's barge was sold. It subsequently delayed the purchase pending the development of a Cook Islands Shipping Roadmap, and issued a tender for a Pa Enua Shipping Charter. \nThe Cook Islands operates an open ship registry and has been placed on the Paris Memorandum of Understanding on Port State Control Black List as a flag of convenience. Ships registered in the Cook Islands have been used to smuggle oil from Iran in defiance of international sanctions. In February 2021 two ships were removed from the shipping register for concealing their movements by turning their Automatic identification system off.\nPorts and harbours.\nThe smaller islands have passages through their reefs, but these are unsuitable for large vessels.\nAir transport.\nThe Cook Islands is served by one domestic airline, Air Rarotonga. A further three foreign airlines provide international service.\nAirports.\nThere is one international airport, Rarotonga International Airport. Eight airports provide local or charter services. Only Rarotonga and Aitutaki Airport are paved."}
{"id": "7075", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=7075", "title": "Cook Islands/Military", "text": ""}
{"id": "7077", "revid": "6575218", "url": "https://en.wikipedia.org/wiki?curid=7077", "title": "Computer file", "text": "A computer file is a computer resource for recording data in a computer storage device. Just as words can be written to paper, so can data be written to a computer file. Files can be edited and transferred through the Internet on that particular computer system.\nDifferent types of computer files are designed for different purposes. A file may be designed to store a picture, a written message, a video, a computer program, or a wide variety of other kinds of data. Certain files can store multiple data types at once.\nBy using computer programs, a person can open, read, change, save, and close a computer file. Computer files may be reopened, modified, and copied an arbitrary number of times.\nFiles are typically organized in a file system, which tracks file locations on the disk and enables user access.\nEtymology.\nThe word \"file\" derives from the Latin \"filum\" (\"a thread\").\n\"File\" was used in the context of computer storage as early as January 1940. In \"Punched Card Methods in Scientific Computation\", W. J. Eckert stated, \"The first extensive use of the early Hollerith Tabulator in astronomy was made by Comrie. He used it for building a table from successive differences, and for adding large numbers of harmonic terms\". \"Tables of functions are constructed from their differences with great efficiency, either as printed tables or as a \"file of punched cards\".\"\nIn February 1950, in a Radio Corporation of America (RCA) advertisement in \"Popular Science\" magazine describing a new \"memory\" vacuum tube it had developed, RCA stated: \"the results of countless computations can be kept 'on file' and taken out again. Such a 'file' now exists in a 'memory' tube developed at RCA Laboratories. Electronically it retains figures fed into calculating machines, holds them in storage while it memorizes new ones \u2013 speeds intelligent solutions through mazes of mathematics.\"\nIn 1952, \"file\" denoted, among other things, information stored on punched cards.\nIn early use, the underlying hardware, rather than the contents stored on it, was denominated a \"file\". For example, the IBM 350 disk drives were denominated \"disk files\". The introduction, circa 1961, by the Burroughs MCP and the MIT Compatible Time-Sharing System of the concept of a \"file system\" that managed several virtual \"files\" on one storage device is the origin of the contemporary denotation of the word. Although the contemporary \"register file\" demonstrates the early concept of files, its use has greatly decreased.\nFile contents.\nOn most modern operating systems, files are organized into one-dimensional arrays of bytes. The format of a file is defined by its content since a file is solely a container for data, although on some platforms the format is usually indicated by its filename extension, specifying the rules for how the bytes must be organized and interpreted meaningfully. For example, the bytes of a plain text file ( in Windows) are associated with either ASCII or UTF-8 characters, while the bytes of image, video, and audio files are interpreted otherwise. Most file types also allocate a few bytes for metadata, which allows a file to carry some basic information about itself.\nSome file systems can store arbitrary (not interpreted by the file system) file-specific data outside of the file format, but linked to the file, for example extended attributes or forks. On other file systems this can be done via sidecar files or software-specific databases. All those methods, however, are more susceptible to loss of metadata than are container and archive file formats.\nFile size.\nAt any instant in time, a file might have a size, normally expressed as number of bytes, that indicates how much storage is associated with the file. In most modern operating systems the size can be any non-negative whole number of bytes up to a system limit. Many older operating systems kept track only of the number of blocks or tracks occupied by a file on a physical storage device. In such systems, software employed other methods to track the exact byte count (e.g., CP/M used a special control character, Ctrl-Z, to signal the end of text files).\nThe general definition of a file does not require that its size have any real meaning, however, unless the data within the file happens to correspond to data within a pool of persistent storage. A special case is a zero byte file; these files can be newly created files that have not yet had any data written to them, or may serve as some kind of flag in the file system, or are accidents (the results of aborted disk operations). For example, the file to which the link points in a typical Unix-like system probably has a defined size that seldom changes. Compare this with which is also a file, but as a character special file, its size is not meaningful.\nOrganization of data in a file.\nInformation in a computer file can consist of smaller packets of information (often called \"records\" or \"lines\") that are individually different but share some common traits. For example, a payroll file might contain information concerning all the employees in a company and their payroll details; each record in the payroll file concerns just one employee, and all the records have the common trait of being related to payroll\u2014this is very similar to placing all payroll information into a specific filing cabinet in an office that does not have a computer. A text file may contain lines of text, corresponding to printed lines on a piece of paper. Alternatively, a file may contain an arbitrary binary image (a blob) or it may contain an executable.\nThe way information is grouped into a file is entirely up to how it is designed. This has led to a plethora of more or less standardized file structures for all imaginable purposes, from the simplest to the most complex. Most computer files are used by computer programs which create, modify or delete the files for their own use on an as-needed basis. The programmers who create the programs decide what files are needed, how they are to be used and (often) their names.\nIn some cases, computer programs manipulate files that are made visible to the computer user. For example, in a word-processing program, the user manipulates document files that the user personally names. Although the content of the document file is arranged in a format that the word-processing program understands, the user is able to choose the name and location of the file and provide the bulk of the information (such as words and text) that will be stored in the file.\nMany applications pack all their data files into a single file called an archive file, using internal markers to discern the different types of information contained within. The benefits of the archive file are to lower the number of files for easier transfer, to reduce storage usage, or just to organize outdated files. The archive file must often be unpacked before next using.\nOperations.\nThe most basic operations that programs can perform on a file are:\nFiles on a computer can be created, moved, modified, grown, shrunk (truncated), and deleted. In most cases, computer programs that are executed on the computer handle these operations, but the user of a computer can also manipulate files if necessary. For instance, Microsoft Word files are normally created and modified by the Microsoft Word program in response to user commands, but the user can also move, rename, or delete these files directly by using a file manager program such as Windows Explorer (on Windows computers) or by command lines (CLI).\nIn Unix-like systems, user space programs do not operate directly, at a low level, on a file. Only the kernel deals with files, and it handles all user-space interaction with files in a manner that is transparent to the user-space programs. The operating system provides a level of abstraction, which means that interaction with a file from user-space is simply through its filename (instead of its inode). For example, rm \"filename\" will not delete the file itself, but only a link to the file. There can be many links to a file, but when they are all removed, the kernel considers that file's memory space free to be reallocated. This free space is commonly considered a security risk (due to the existence of file recovery software). Any secure-deletion program uses kernel-space (system) functions to wipe the file's data.\nFile moves within a file system complete almost immediately because the data content does not need to be rewrittten. Only the paths need to be changed.\nMoving methods.\nThere are two distinct implementations of file moves.\nWhen moving files between devices or partitions, some file managing software deletes each selected file from the source directory individually after being transferred, while other software deletes all files at once' only after every file has been transferred.\nWith the codice_1 command for instance, the former method is used when selecting files individually, possibly with the use of wildcards (example: codice_2, while the latter method is used when selecting entire directories (example: codice_3). Microsoft Windows Explorer uses the former method for mass storage filemoves, but the latter method using Media Transfer Protocol, as described in .\nThe former method (individual deletion from source) has the benefit that space is released from the source device or partition imminently after the transfer has begun, meaning after the first file is finished. With the latter method, space is only freed after the transfer of the entire selection has finished.\nIf an incomplete file transfer with the latter method is aborted unexpectedly, perhaps due to an unexpected power-off, system halt or disconnection of a device, no space will have been freed up on the source device or partition. The user would need to merge the remaining files from the source, including the incompletely written (truncated) last file.\nWith the individual deletion method, the file moving software also does not need to cumulatively keep track of all files finished transferring for the case that a user manually aborts the file transfer. A file manager using the latter (afterwards deletion) method will have to only delete the files from the source directory that have already finished transferring.\nIdentifying and organizing.\nIn modern computer systems, files are typically accessed using names (filenames). In some operating systems, the name is associated with the file itself. In others, the file is anonymous, and is pointed to by links that have names. In the latter case, a user can identify the name of the link with the file itself, but this is a false analogue, especially where there exists more than one link to the same file.\nFiles (or links to files) can be located in directories. However, more generally, a directory can contain either a list of files or a list of links to files. Within this definition, it is of paramount importance that the term \"file\" includes directories. This permits the existence of directory hierarchies, i.e., directories containing sub-directories. A name that refers to a file within a directory must be typically unique. In other words, there must be no identical names within a directory. However, in some operating systems, a name may include a specification of type that means a directory can contain an identical name for more than one type of object such as a directory and a file.\nIn environments in which a file is named, a file's name and the path to the file's directory must uniquely identify it among all other files in the computer system\u2014no two files can have the same name and path. Where a file is anonymous, named references to it will exist within a namespace. In most cases, any name within the namespace will refer to exactly zero or one file. However, any file may be represented within any namespace by zero, one or more names.\nAny string of characters may be a well-formed name for a file or a link depending upon the context of application. Whether or not a name is well-formed depends on the type of computer system being used. Early computers permitted only a few letters or digits in the name of a file, but modern computers allow long names (some up to 255 characters) containing almost any combination of unicode letters or unicode digits, making it easier to understand the purpose of a file at a glance. Some computer systems allow file names to contain spaces; others do not. Case-sensitivity of file names is determined by the file system. Unix file systems are usually case sensitive and allow user-level applications to create files whose names differ only in the case of characters. Microsoft Windows supports multiple file systems, each with different policies regarding case-sensitivity. The common FAT file system can have multiple files whose names differ only in case if the user uses a disk editor to edit the file names in the directory entries. User applications, however, will usually not allow the user to create multiple files with the same name but differing in case.\nMost computers organize files into hierarchies using folders, directories, or catalogs. The concept is the same irrespective of the terminology used. Each folder can contain an arbitrary number of files, and it can also contain other folders. These other folders are referred to as subfolders. Subfolders can contain still more files and folders and so on, thus building a tree-like structure in which one \"master folder\" (or \"root folder\" \u2014 the name varies from one operating system to another) can contain any number of levels of other folders and files. Folders can be named just as files can (except for the root folder, which often does not have a name). The use of folders makes it easier to organize files in a logical way.\nWhen a computer allows the use of folders, each file and folder has not only a name of its own, but also a path, which identifies the folder or folders in which a file or folder resides. In the path, some sort of special character\u2014such as a slash\u2014is used to separate the file and folder names. For example, in the illustration shown in this article, the path uniquely identifies a file called in a folder called , which in turn is contained in a folder called . The folder and file names are separated by slashes in this example; the topmost or root folder has no name, and so the path begins with a slash (if the root folder had a name, it would precede this first slash).\nMany computer systems use extensions in file names to help identify what they contain, also known as the file type. On Windows computers, extensions consist of a dot (period) at the end of a file name, followed by a few letters to identify the type of file. An extension of identifies a text file; a extension identifies any type of document or documentation, commonly in the Microsoft Word file format; and so on. Even when extensions are used in a computer system, the degree to which the computer system recognizes and heeds them can vary; in some systems, they are required, while in other systems, they are completely ignored if they are presented.\nProtection.\nMany modern computer systems provide methods for protecting files against accidental and deliberate damage. Computers that allow for multiple users implement file permissions to control who may or may not modify, delete, or create files and folders. For example, a given user may be granted only permission to read a file or folder, but not to modify or delete it; or a user may be given permission to read and modify files or folders, but not to execute them. Permissions may also be used to allow only certain users to see the contents of a file or folder. Permissions protect against unauthorized tampering or destruction of information in files, and keep private information confidential from unauthorized users.\nAnother protection mechanism implemented in many computers is a \"read-only flag.\" When this flag is turned on for a file (which can be accomplished by a computer program or by a human user), the file can be examined, but it cannot be modified. This flag is useful for critical information that must not be modified or erased, such as special files that are used only by internal parts of the computer system. Some systems also include a \"hidden flag\" to make certain files invisible; this flag is used by the computer system to hide essential system files that users should not alter.\nStorage.\nAny file that has any useful purpose, must have some physical manifestation. That is, a file (an abstract concept) in a real computer system must have a real physical analogue if it is to exist at all.\nIn physical terms, most computer files are stored on some type of data storage device. For example, most operating systems store files on a hard disk. Hard disks have been the ubiquitous form of non-volatile storage since the early 1960s. Where files contain only temporary information, they may be stored in RAM. Computer files can be also stored on other media in some cases, such as magnetic tapes, compact discs, Digital Versatile Discs, Zip drives, USB flash drives, etc. The use of solid state drives is also beginning to rival the hard disk drive.\nIn Unix-like operating systems, many files have no associated physical storage device. Examples are and most files under directories , and . These are virtual files: they exist as objects within the operating system kernel.\nAs seen by a running user program, files are usually represented either by a file control block or by a file handle. A file control block (FCB) is an area of memory which is manipulated to establish a filename etc. and then passed to the operating system as a parameter; it was used by older IBM operating systems and early PC operating systems including CP/M and early versions of MS-DOS. A file handle is generally either an opaque data type or an integer; it was introduced in around 1961 by the ALGOL-based Burroughs MCP running on the Burroughs B5000 but is now ubiquitous.\nFile corruption.\nWhen a file is said to be corrupted, it is because its contents have been saved to the computer in such a way that they cannot be properly read, either by a human or by software. Depending on the extent of the damage, the original file can sometimes be recovered, or at least partially understood. A file may be created corrupt, or it may be corrupted at a later point through overwriting.\nThere are many ways by which a file can become corrupted. Most commonly, the issue happens in the process of writing the file to a disk. For example, if an image-editing program unexpectedly crashes while saving an image, that file may be corrupted because the program could not save its entirety. The program itself might warn the user that there was an error, allowing for another attempt at saving the file. Some other examples of reasons for which files become corrupted include:\nAlthough file corruption usually happens accidentally, it may also be done on purpose, as to fool someone else into thinking an assignment was ready at an earlier date, potentially gaining time to finish said assignment. There are services that provide on demand file corruption, which essentially fill a given file with random data so that it cannot be opened or read, yet still seems legitimate.\nOne of the most effective countermeasures for unintentional file corruption is backing up important files. In the event of an important file becoming corrupted, the user can simply replace it with the backed up version.\nBackup.\nWhen computer files contain information that is extremely important, a \"back-up\" process is used to protect against disasters that might destroy the files. Backing up files simply means making copies of the files in a separate location so that they can be restored if something happens to the computer, or if they are deleted accidentally.\nThere are many ways to back up files. Most computer systems provide utility programs to assist in the back-up process, which can become very time-consuming if there are many files to safeguard. Files are often copied to removable media such as writable CDs or cartridge tapes. Copying files to another hard disk in the same computer protects against failure of one disk, but if it is necessary to protect against failure or destruction of the entire computer, then copies of the files must be made on other media that can be taken away from the computer and stored in a safe, distant location.\nThe grandfather-father-son backup method automatically makes three back-ups; the grandfather file is the oldest copy of the file and the son is the current copy.\nFile systems and file managers.\nThe way a computer organizes, names, stores and manipulates files is globally referred to as its \"file system.\" Most computers have at least one file system. Some computers allow the use of several different file systems. For instance, on newer MS Windows computers, the older FAT-type file systems of MS-DOS and old versions of Windows are supported, in addition to the NTFS file system that is the normal file system for recent versions of Windows. Each system has its own advantages and disadvantages. Standard FAT allows only eight-character file names (plus a three-character extension) with no spaces, for example, whereas NTFS allows much longer names that can contain spaces. You can call a file \"\" in NTFS, but in FAT you would be restricted to something like (unless you were using VFAT, a FAT extension allowing long file names).\nFile manager programs are utility programs that allow users to manipulate files directly. They allow you to move, create, delete and rename files and folders, although they do not actually allow you to read the contents of a file or store information in it. Every computer system provides at least one file-manager program for its native file system. For example, File Explorer (formerly Windows Explorer) is commonly used in Microsoft Windows operating systems, and Nautilus is common under several distributions of Linux."}
{"id": "7079", "revid": "9385", "url": "https://en.wikipedia.org/wiki?curid=7079", "title": "CID", "text": "CID may refer to:"}
{"id": "7080", "revid": "19516936", "url": "https://en.wikipedia.org/wiki?curid=7080", "title": "Christian Doppler", "text": "Christian Andreas Doppler (; 29 November 1803 \u2013 17 March 1853) was an Austrian mathematician and physicist. He is celebrated for his principle \u2013 known as the Doppler effect \u2013 that the observed frequency of a wave depends on the relative speed of the source and the observer. He used this concept to explain the color of binary stars.\nBiography.\nDoppler was born in Salzburg (today Austria) in 1803. After completing high school, Doppler studied philosophy in Salzburg and mathematics and physics at the Imperial\u2013Royal Polytechnic Institute (now TU Wien), where he became an assistant in 1829. In 1835 he began work at the Prague Polytechnic (now Czech Technical University in Prague), where he received an appointment in 1841.\nOne year later, at the age of 38, Doppler gave a lecture to the Royal Bohemian Society of Sciences and subsequently published his most notable work, \"\u00dcber das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels\" (\"On the coloured light of the binary stars and some other stars of the heavens\"). There is a facsimile edition with an English translation by Alec Eden. In this work, Doppler postulated his principle (later coined the Doppler effect) that the observed frequency of a wave depends on the relative speed of the source and the observer, and he later tried to use this concept for explaining the colour of binary stars.\nPhysicist Armand Hippolyte Louis Fizeau () also contributed to aspects of the discovery of the Doppler effect, which is known by the French as the \"Doppler-Fizeau Effect\". Fizeau contributed towards understanding its effect with light and also developed formal mathematical theorems underlying the principles of this effect. In 1848, he predicted the frequency shift of a wave when the source and receiver are moving relative to each other, therefore being the first to predict blue shifts and red shifts of spectral lines in stars.\nDoppler continued working as a professor at the Prague Polytechnic, publishing over 50 articles on mathematics, physics and astronomy, but in 1847 he left Prague for the professorship of mathematics, physics, and mechanics at the Academy of Mines and Forests (its successor is the University of Miskolc) in Selmecb\u00e1nya (then Kingdom of Hungary, now Bansk\u00e1 \u0160tiavnica, Slovakia),\n and in 1849 he moved to Vienna.\nDoppler's research was interrupted by the revolutionary incidents of 1848. During the Hungarian Revolution, he fled to Vienna. There he was appointed head of the Institute for Experimental Physics at the University of Vienna in 1850. While there, Doppler, along with Franz Unger, influenced the development of young Gregor Mendel, the founding father of genetics, who was a student at the University of Vienna from 1851 to 1853.\nDoppler died on 17 March 1853 at age 49 from a pulmonary disease in Venice (at that time part of the Austrian Empire). His tomb, found by Dr. Peter M. Schuster, is just inside the entrance of the Venetian island cemetery of San Michele.\nFull name.\nSome confusion exists about Doppler's full name. Doppler referred to himself as Christian Doppler. The records of his birth and baptism stated Christian \"Andreas\" Doppler. Forty years after Doppler's death the misnomer \"Johann\" Christian Doppler was introduced by the astronomer Julius Scheiner. Scheiner's mistake has since been copied by many.\nTribute.\nOn 29 November 2017, Google celebrated his 214th birthday with a Google Doodle."}
{"id": "7081", "revid": "525077", "url": "https://en.wikipedia.org/wiki?curid=7081", "title": "Clerihew", "text": "A clerihew () is a whimsical, four-line biographical poem invented by Edmund Clerihew Bentley. The first line is the name of the poem's subject, usually a famous person put in an absurd light, or revealing something unknown or spurious about them. The rhyme scheme is AABB, and the rhymes are often forced. The line length and metre are irregular. Bentley invented the clerihew in school and then popularized it in books. One of his best known is this (1905):\nForm.\nA clerihew has the following properties:\nClerihews are not satirical or abusive, but they target famous individuals and reposition them in an absurd, anachronistic or commonplace setting, often giving them an over-simplified and slightly garbled description.\nPractitioners.\nThe form was invented by and is named after Edmund Clerihew Bentley. When he was a 16-year-old pupil at St Paul's School in London, the lines of his first clerihew, about Humphry Davy, came into his head during a science class. Together with his schoolfriends, he filled a notebook with examples. The first use of the word in print was in 1928. Bentley published three volumes of his own clerihews: \"Biography for Beginners\" (1905), published as \"edited by E. Clerihew\"; \"More Biography\" (1929); and \"Baseless Biography\" (1939), a compilation of clerihews originally published in \"Punch\" illustrated by the author's son Nicolas Bentley.\nG. K. Chesterton, a friend of Bentley, was also a practitioner of the clerihew and one of the sources of its popularity. Chesterton provided verses and illustrations for the original schoolboy notebook and illustrated \"Biography for Beginners\". Other serious authors also produced clerihews, including W. H. Auden, and it remains a popular humorous form among other writers and the general public. Among contemporary writers, the satirist Craig Brown has made considerable use of the clerihew in his columns for \"The Daily Telegraph\".\nThere has been newfound popularity of the form on Twitter.\nExamples.\nBentley's first clerihew, published in 1905, was written about Sir Humphry Davy:\nThe original poem had the second line \"Was not fond of gravy\"; but the published version has \"Abominated gravy\".\nOther clerihews by Bentley include:\nand\nW. H. Auden's \"Academic Graffiti\" (1971) includes:\nSatirical magazine \"Private Eye\" noted Auden's work and responded:\nA second stanza aimed a jibe at Auden's publisher, Faber and Faber.\nAlan Turing, one of the founders of computing, was the subject of a clerihew written by the pupils of his \"alma mater\", Sherborne School in England:\nA clerihew appreciated by chemists is cited in \"Dark Sun\" by Richard Rhodes, and regards the inventor of the thermos bottle (or Dewar flask): \n\"Dark Sun\" also features a clerihew about the German-British physicist and Soviet nuclear spy Klaus Fuchs:\nIn 1983, \"Games\" magazine ran a contest titled \"Do You Clerihew?\" The winning entry was:\nOther uses of the form.\nThe clerihew form has also occasionally been used for non-biographical verses. Bentley opened his 1905 \"Biography for Beginners\" with an example, entitled \"Introductory Remarks\", on the theme of biography itself:\nThe third edition of the same work, published in 1925, included a \"Preface to the New Edition\" in 11 stanzas, each in clerihew form. One stanza ran:"}
{"id": "7082", "revid": "4303429", "url": "https://en.wikipedia.org/wiki?curid=7082", "title": "Central American Court of Justice", "text": ""}
{"id": "7085", "revid": "40426870", "url": "https://en.wikipedia.org/wiki?curid=7085", "title": "Civil war", "text": "A civil war, also known as an intrastate war in polemology, is a war between organized groups within the same state or country.\nThe aim of one side may be to take control of the country or a region, to achieve independence for a region or to change government policies.\nThe term is a calque of Latin \"bellum civile\" which was used to refer to the various civil wars of the Roman Republic in the 1st century BC.\nMost modern civil wars involve intervention by outside powers. According to Patrick M. Regan in his book \"Civil Wars and Foreign Powers\" (2000) about two thirds of the 138 intrastate conflicts between the end of World War II and 2000 saw international intervention, with the United States intervening in 35 of these conflicts.\nA civil war is a high-intensity conflict, often involving regular armed forces, that is sustained, organized and large-scale. Civil wars may result in large numbers of casualties and the consumption of significant resources.\nCivil wars since the end of World War II have lasted on average just over four years, a dramatic rise from the one-and-a-half-year average of the 1900\u20131944 period. While the rate of emergence of new civil wars has been relatively steady since the mid-19th century, the increasing length of those wars has resulted in increasing numbers of wars ongoing at any one time. For example, there were no more than five civil wars underway simultaneously in the first half of the 20th century while there were over 20 concurrent civil wars close to the end of the Cold War. Since 1945, civil wars have resulted in the deaths of over 25 million people, as well as the forced displacement of millions more. Civil wars have further resulted in economic collapse; Somalia, Burma (Myanmar), Uganda and Angola are examples of nations that were considered to have had promising futures before being engulfed in civil wars.\nFormal classification.\nJames Fearon, a scholar of civil wars at Stanford University, defines a civil war as \"a violent conflict within a country fought by organized groups that aim to take power at the center or in a region, or to change government policies\". Ann Hironaka further specifies that one side of a civil war is the state. The intensity at which a civil disturbance becomes a civil war is contested by academics. Some political scientists define a civil war as having more than 1,000 casualties, while others further specify that at least 100 must come from each side. The Correlates of War, a dataset widely used by scholars of conflict, classifies civil wars as having over 1000 war-related casualties per year of conflict. This rate is a small fraction of the millions killed in the Second Sudanese Civil War and Cambodian Civil War, for example, but excludes several highly publicized conflicts, such as The Troubles of Northern Ireland and the struggle of the African National Congress in Apartheid-era South Africa.\nBased on the 1,000-casualties-per-year criterion, there were 213 civil wars from 1816 to 1997, 104 of which occurred from 1944 to 1997. If one uses the less-stringent 1,000 casualties total criterion, there were over 90 civil wars between 1945 and 2007, with 20 ongoing civil wars as of 2007.\nThe Geneva Conventions do not specifically define the term \"civil war\"; nevertheless, they do outline the responsibilities of parties in \"armed conflict not of an international character\". This includes civil wars; however, no specific definition of civil war is provided in the text of the Conventions.\nNevertheless, the International Committee of the Red Cross has sought to provide some clarification through its commentaries on the Geneva Conventions, noting that the Conventions are \"so general, so vague, that many of the delegations feared that it might be taken to cover any act committed by force of arms\". Accordingly, the commentaries provide for different 'conditions' on which the application of the Geneva Convention would depend; the commentary, however, points out that these should not be interpreted as rigid conditions. The conditions listed by the ICRC in its commentary are as follows:\n(b) That it has claimed for itself the rights of a belligerent; or\n(c) That it has accorded the insurgents recognition as belligerents for the purposes only of the present Convention; or\n(d) That the dispute has been admitted to the agenda of the Security Council or the General Assembly of the United Nations as being a threat to international peace, a breach of the peace, or an act of aggression.\n(b) That the insurgent civil authority exercises de facto authority over the population within a determinate portion of the national territory.\n(c) That the armed forces act under the direction of an organized authority and are prepared to observe the ordinary laws of war.\n(d) That the insurgent civil authority agrees to be bound by the provisions of the Convention.\nCauses.\nAccording to a 2017 review study of civil war research, there are three prominent explanations for civil war: greed-based explanations which center on individuals\u2019 desire to maximize their profits, grievance-based explanations which center on conflict as a response to socioeconomic or political injustice, and opportunity-based explanations which center on factors that make it easier to engage in violent mobilization. According to the study, the most influential explanation for civil war onset is the opportunity-based explanation by James Fearon and David Laitin in their 2003 American Political Science Review article.\nGreed.\nScholars investigating the cause of civil war are attracted by two opposing theories, greed versus grievance. Roughly stated: are conflicts caused by who people are, whether that be defined in terms of ethnicity, religion or other social affiliation, or do conflicts begin because it is in the economic best interests of individuals and groups to start them? Scholarly analysis supports the conclusion that economic and structural factors are more important than those of identity in predicting occurrences of civil war.\nA comprehensive study of civil war was carried out by a team from the World Bank in the early 21st century. The study framework, which came to be called the Collier\u2013Hoeffler Model, examined 78 five-year increments when civil war occurred from 1960 to 1999, as well as 1,167 five-year increments of \"no civil war\" for comparison, and subjected the data set to regression analysis to see the effect of various factors. The factors that were shown to have a statistically significant effect on the chance that a civil war would occur in any given five-year period were:\nA high proportion of primary commodities in national exports significantly increases the risk of a conflict. A country at \"peak danger\", with commodities comprising 32% of gross domestic product, has a 22% risk of falling into civil war in a given five-year period, while a country with no primary commodity exports has a 1% risk. When disaggregated, only petroleum and non-petroleum groupings showed different results: a country with relatively low levels of dependence on petroleum exports is at slightly less risk, while a high level of dependence on oil as an export results in slightly more risk of a civil war than national dependence on another primary commodity. The authors of the study interpreted this as being the result of the ease by which primary commodities may be extorted or captured compared to other forms of wealth; for example, it is easy to capture and control the output of a gold mine or oil field compared to a sector of garment manufacturing or hospitality services.\nA second source of finance is national diasporas, which can fund rebellions and insurgencies from abroad. The study found that statistically switching the size of a country's diaspora from the smallest found in the study to the largest resulted in a sixfold increase in the chance of a civil war.\nHigher male secondary school enrollment, per capita income and economic growth rate all had significant effects on reducing the chance of civil war. Specifically, a male secondary school enrollment 10% above the average reduced the chance of a conflict by about 3%, while a growth rate 1% higher than the study average resulted in a decline in the chance of a civil war of about 1%. The study interpreted these three factors as proxies for earnings forgone by rebellion, and therefore that lower forgone earnings encourage rebellion. Phrased another way: young males (who make up the vast majority of combatants in civil wars) are less likely to join a rebellion if they are getting an education or have a comfortable salary, and can reasonably assume that they will prosper in the future.\nLow per capita income has been proposed as a cause for grievance, prompting armed rebellion. However, for this to be true, one would expect economic inequality to also be a significant factor in rebellions, which it is not. The study therefore concluded that the economic model of opportunity cost better explained the findings.\nGrievance.\nMost proxies for \"grievance\"\u2014the theory that civil wars begin because of issues of identity, rather than economics\u2014were statistically insignificant, including economic equality, political rights, ethnic polarization and religious fractionalization. Only ethnic dominance, the case where the largest ethnic group comprises a majority of the population, increased the risk of civil war. A country characterized by ethnic dominance has nearly twice the chance of a civil war. However, the combined effects of ethnic and religious fractionalization, i.e. the greater chance that any two randomly chosen people will be from separate ethnic or religious groups, the less chance of a civil war, were also significant and positive, as long as the country avoided ethnic dominance. The study interpreted this as stating that minority groups are more likely to rebel if they feel that they are being dominated, but that rebellions are more likely to occur the more homogeneous the population and thus more cohesive the rebels. These two factors may thus be seen as mitigating each other in many cases.\nCriticism of the \"greed versus grievance\" theory.\nDavid Keen, a professor at the Development Studies Institute at the London School of Economics is one of the major critics of greed vs. grievance theory, defined primarily by Paul Collier, and argues the point that a conflict, although he cannot define it, cannot be pinpointed to simply one motive. He believes that conflicts are much more complex and thus should not be analyzed through simplified methods. He disagrees with the quantitative research methods of Collier and believes a stronger emphasis should be put on personal data and human perspective of the people in conflict.\nBeyond Keen, several other authors have introduced works that either disprove greed vs. grievance theory with empirical data, or dismiss its ultimate conclusion. Authors such as Cristina Bodea and Ibrahim Elbadawi, who co-wrote the entry, \"Riots, coups and civil war: Revisiting the greed and grievance debate\", argue that empirical data can disprove many of the proponents of greed theory and make the idea \"irrelevant\". They examine a myriad of factors and conclude that too many factors come into play with conflict, which cannot be confined to simply greed or grievance.\nAnthony Vinci makes a strong argument that \"fungible concept of power and the primary motivation of survival provide superior explanations of armed group motivation and, more broadly, the conduct of internal conflicts\".\nOpportunities.\nJames Fearon and David Laitin find that ethnic and religious diversity does not make civil war more likely. They instead find that factors that make it easier for rebels to recruit foot soldiers and sustain insurgencies, such as \"poverty\u2014which marks financially &amp; bureaucratically weak states and also favors rebel recruitment\u2014political instability, rough terrain, and large populations\" make civil wars more likely.\nOther causes.\nBargaining problems.\nIn a state torn by civil war, the contesting powers often do not have the ability to commit or the trust to believe in the other side's commitment to put an end to war. When considering a peace agreement, the involved parties are aware of the high incentives to withdraw once one of them has taken an action that weakens their military, political or economical power. Commitment problems may deter a lasting peace agreement as the powers in question are aware that neither of them is able to commit to their end of the bargain in the future. States are often unable to escape conflict traps (recurring civil war conflicts) due to the lack of strong political and legal institutions that motivate bargaining, settle disputes, and enforce peace settlements.\nGovernance.\nPolitical scientist Barbara Walter suggests that most contemporary civil wars are actually repeats of earlier civil wars that often arise when leaders are not accountable to the public, when there is poor public participation in politics, and when there is a lack of transparency of information between the executives and the public. Walter argues that when these issues are properly reversed, they act as political and legal restraints on executive power forcing the established government to better serve the people. Additionally, these political and legal restraints create a standardized avenue to influence government and increase the commitment credibility of established peace treaties. It is the strength of a nation\u2019s institutionalization and good governance\u2014not the presence of democracy nor the poverty level\u2014that is the number one indicator of the chance of a repeat civil war, according to Walter.\nMilitary advantage.\nHigh levels of population dispersion and, to a lesser extent, the presence of mountainous terrain, increased the chance of conflict. Both of these factors favor rebels, as a population dispersed outward toward the borders is harder to control than one concentrated in a central region, while mountains offer terrain where rebels can seek sanctuary.\nPopulation size.\nThe various factors contributing to the risk of civil war rise increase with population size. The risk of a civil war rises approximately proportionately with the size of a country's population.\nPoverty.\nThere is a correlation between poverty and civil war, but the causality (which causes the other) is unclear. Some studies have found that in regions with lower income per capita, the likelihood of civil war is greater. Economists Simeon Djankov and Marta Reynal-Querol argue that the correlation is spurious, and that lower income and heightened conflict are instead products of other phenomena. In contrast, a study by Alex Braithwaite and colleagues showed systematic evidence of \"a causal arrow running from poverty to conflict\".\nTime.\nThe more time that has elapsed since the last civil war, the less likely it is that a conflict will recur. The study had two possible explanations for this: one opportunity-based and the other grievance-based. The elapsed time may represent the depreciation of whatever capital the rebellion was fought over and thus increase the opportunity cost of restarting the conflict. Alternatively, elapsed time may represent the gradual process of healing of old hatreds. The study found that the presence of a diaspora substantially reduced the positive effect of time, as the funding from diasporas offsets the depreciation of rebellion-specific capital.\nEvolutionary psychologist Satoshi Kanazawa has argued that an important cause of intergroup conflict may be the relative availability of women of reproductive age. He found that polygyny greatly increased the frequency of civil wars but not interstate wars. Gleditsch et al. did not find a relationship between ethnic groups with polygyny and increased frequency of civil wars but nations having legal polygamy may have more civil wars. They argued that misogyny is a better explanation than polygyny. They found that increased women's rights were associated with fewer civil wars and that legal polygamy had no effect after women's rights were controlled for.\nPolitical scholar Elisabeth Wood from Yale University offers yet another rationale for why civilians rebel and/or support civil war. Through her studies of the Salvadoran Civil War, Wood finds that traditional explanations of greed and grievance are not sufficient to explain the emergence of that insurgent movement. Instead, she argues that \"emotional engagements\" and \"moral commitments\" are the main reasons why thousand of civilians, most of them from poor and rural backgrounds, joined or supported the Farabundo Mart\u00ed National Liberation Front, despite individually facing both high risks and virtually no foreseeable gains. Wood also attributes participation in the civil war to the value that insurgents assigned to changing social relations in El Salvador, an experience she defines as the \"pleasure of agency\".\nDuration and effects.\nAnn Hironaka, author of \"Neverending Wars\", divides the modern history of civil wars into the pre-19th century, 19th century to early 20th century, and late 20th century. In 19th-century Europe, the length of civil wars fell significantly, largely due to the nature of the conflicts as battles for the power center of the state, the strength of centralized governments, and the normally quick and decisive intervention by other states to support the government. Following World War II the duration of civil wars grew past the norm of the pre-19th century, largely due to weakness of the many postcolonial states and the intervention by major powers on both sides of conflict. The most obvious commonality to civil wars are that they occur in fragile states.\nIn the 19th and early 20th centuries.\nCivil wars in the 19th century and in the early 20th century tended to be short; civil wars between 1900 and 1944 lasted on average one and half years. The state itself formed the obvious center of authority in the majority of cases, and the civil wars were thus fought for control of the state. This meant that whoever had control of the capital and the military could normally crush resistance. A rebellion which failed to quickly seize the capital and control of the military for itself normally found itself doomed to rapid destruction. For example, the fighting associated with the 1871 Paris Commune occurred almost entirely in Paris, and ended quickly once the military sided with the government at Versailles and conquered Paris.\nThe power of non-state actors resulted in a lower value placed on sovereignty in the 18th and 19th centuries, which further reduced the number of civil wars. For example, the pirates of the Barbary Coast were recognized as \"de facto\" states because of their military power. The Barbary pirates thus had no need to rebel against the Ottoman Empire \u2013 their nominal state government \u2013 to gain recognition of their sovereignty. Conversely, states such as Virginia and Massachusetts in the United States of America did not have sovereign status, but had significant political and economic independence coupled with weak federal control, reducing the incentive to secede.\nThe two major global ideologies, monarchism and democracy, led to several civil wars. However, a bi-polar world, divided between the two ideologies, did not develop, largely due to the dominance of monarchists through most of the period. The monarchists would thus normally intervene in other countries to stop democratic movements taking control and forming democratic governments, which were seen by monarchists as being both dangerous and unpredictable. The Great Powers (defined in the 1815 Congress of Vienna as the United Kingdom, Habsburg Austria, Prussia, France, and Russia) would frequently coordinate interventions in other nations' civil wars, nearly always on the side of the incumbent government. Given the military strength of the Great Powers, these interventions nearly always proved decisive and quickly ended the civil wars.\nThere were several exceptions from the general rule of quick civil wars during this period. The American Civil War (1861\u20131865) was unusual for at least two reasons: it was fought around regional identities as well as political ideologies, and it ended through a war of attrition, rather than with a decisive battle over control of the capital, as was the norm. The Spanish Civil War (1936\u20131939) proved exceptional because \"both\" sides in the struggle received support from intervening great powers: Germany, Italy, and Portugal supported opposition leader Francisco Franco, while France and the Soviet Union supported the government (see proxy war).\nSince 1945.\nIn the 1990s, about twenty civil wars were occurring concurrently during an average year, a rate about ten times the historical average since the 19th century. However, the rate of new civil wars had not increased appreciably; the drastic rise in the number of ongoing wars after World War II was a result of the tripling of the average duration of civil wars to over four years. This increase was a result of the increased number of states, the fragility of states formed after 1945, the decline in interstate war, and the Cold War rivalry.\nFollowing World War II, the major European powers divested themselves of their colonies at an increasing rate: the number of ex-colonial states jumped from about 30 to almost 120 after the war. The rate of state formation leveled off in the 1980s, at which point few colonies remained. More states also meant more states in which to have long civil wars. Hironaka statistically measures the impact of the increased number of ex-colonial states as increasing the post-World War II incidence of civil wars by +165% over the pre-1945 number.\nWhile the new ex-colonial states appeared to follow the blueprint of the idealized state\u2014centralized government, territory enclosed by defined borders, and citizenry with defined rights\u2014as well as accessories such as a national flag, an anthem, a seat at the United Nations and an official economic policy, they were in actuality far weaker than the Western states they were modeled after. In Western states, the structure of governments closely matched states' actual capabilities, which had been arduously developed over centuries. The development of strong administrative structures, in particular those related to extraction of taxes, is closely associated with the intense warfare between predatory European states in the 17th and 18th centuries, or in Charles Tilly's famous formulation: \"War made the state and the state made war\". For example, the formation of the modern states of Germany and Italy in the 19th century is closely associated with the wars of expansion and consolidation led by Prussia and Sardinia-Piedmont, respectively. The Western process of forming effective and impersonal bureaucracies, developing efficient tax systems, and integrating national territory continued into the 20th century. Nevertheless, Western states that survived into the latter half of the 20th century were considered \"strong\" by simple reason that they had managed to develop the institutional structures and military capability required to survive predation by their fellow states.\nIn sharp contrast, decolonization was an entirely different process of state formation. Most imperial powers had not foreseen a need to prepare their colonies for independence; for example, Britain had given limited self-rule to India and Sri Lanka, while treating British Somaliland as little more than a trading post, while all major decisions for French colonies were made in Paris and Belgium prohibited any self-government up until it suddenly granted independence to its colonies in 1960. Like Western states of previous centuries, the new ex-colonies lacked autonomous bureaucracies, which would make decisions based on the benefit to society as a whole, rather than respond to corruption and nepotism to favor a particular interest group. In such a situation, factions manipulate the state to benefit themselves or, alternatively, state leaders use the bureaucracy to further their own self-interest. The lack of credible governance was compounded by the fact that most colonies were economic loss-makers at independence, lacking both a productive economic base and a taxation system to effectively extract resources from economic activity. Among the rare states profitable at decolonization was India, to which scholars credibly argue that Uganda, Malaysia and Angola may be included. Neither did imperial powers make territorial integration a priority, and may have discouraged nascent nationalism as a danger to their rule. Many newly independent states thus found themselves impoverished, with minimal administrative capacity in a fragmented society, while faced with the expectation of immediately meeting the demands of a modern state. Such states are considered \"weak\" or \"fragile\". The \"strong\"-\"weak\" categorization is not the same as \"Western\"-\"non-Western\", as some Latin American states like Argentina and Brazil and Middle Eastern states like Egypt and Israel are considered to have \"strong\" administrative structures and economic infrastructure.\nHistorically, the international community would have targeted weak states for territorial absorption or colonial domination or, alternatively, such states would fragment into pieces small enough to be effectively administered and secured by a local power. However, international norms towards sovereignty changed in the wake of World War II in ways that support and maintain the existence of weak states. Weak states are given \"de jure\" sovereignty equal to that of other states, even when they do not have \"de facto\" sovereignty or control of their own territory, including the privileges of international diplomatic recognition and an equal vote in the United Nations. Further, the international community offers development aid to weak states, which helps maintain the facade of a functioning modern state by giving the appearance that the state is capable of fulfilling its implied responsibilities of control and order. The formation of a strong international law regime and norms against territorial aggression is strongly associated with the dramatic drop in the number of interstate wars, though it has also been attributed to the effect of the Cold War or to the changing nature of economic development. Consequently, military aggression that results in territorial annexation became increasingly likely to prompt international condemnation, diplomatic censure, a reduction in international aid or the introduction of economic sanction, or, as in the case of 1990 invasion of Kuwait by Iraq, international military intervention to reverse the territorial aggression. Similarly, the international community has largely refused to recognize secessionist regions, while keeping some secessionist self-declared states such as Somaliland in diplomatic recognition limbo. While there is not a large body of academic work examining the relationship, Hironaka's statistical study found a correlation that suggests that every major international anti-secessionist declaration increased the number of ongoing civil wars by +10%, or a total +114% from 1945 to 1997. The diplomatic and legal protection given by the international community, as well as economic support to weak governments and discouragement of secession, thus had the unintended effect of encouraging civil wars.\nInterventions by outside powers.\nThere has been an enormous amount of international intervention in civil wars since 1945 that some have argued served to extend wars. According to Patrick M. Regan in his book \"Civil Wars and Foreign Powers\" (2000) about 2/3rds of the 138 intrastate conflicts between the end of World War II and 2000 saw international intervention, with the United States intervening in 35 of these conflicts. While intervention has been practiced since the international system has existed, its nature changed substantially. It became common for both the state and opposition group to receive foreign support, allowing wars to continue well past the point when domestic resources had been exhausted. Superpowers, such as the European great powers, had always felt no compunction in intervening in civil wars that affected their interests, while distant regional powers such as the United States could declare the interventionist Monroe Doctrine of 1821 for events in its Central American \"backyard\". However, the large population of weak states after 1945 allowed intervention by former colonial powers, regional powers and neighboring states who themselves often had scarce resources.\nEffectiveness of intervention.\nThe effectiveness of intervention is widely debated, in part because the data suffers from selection bias; as Fortna has argued, peacekeepers select themselves into difficult cases. When controlling for this effect, Forta holds that peacekeeping is resoundingly successful in shortening wars. However, other scholars disagree. Knaus and Stewart are extremely skeptical as to the effectiveness of interventions, holding that they can only work when they are performed with extreme caution and sensitivity to context, a strategy they label 'principled incrementalism'. Few interventions, for them, have demonstrated such an approach. Other scholars offer more specific criticisms; Dube and Naidu, for instance, show that US military aid, a less conventional form of intervention, seems to be siphoned off to paramilitaries thus exacerbating violence. Weinstein holds more generally that interventions might disrupt processes of 'autonomous recovery' whereby civil war contributes to state-building.\nOn average, a civil war with interstate intervention was 300% longer than those without. When disaggregated, a civil war with intervention on only one side is 156% longer, while when intervention occurs on both sides the average civil war is longer by an additional 92%. If one of the intervening states was a superpower, a civil war is a further 72% longer; a conflict such as the Angolan Civil War, in which there is two-sided foreign intervention, including by a superpower (actually, two superpowers in the case of Angola), would be 538% longer on average than a civil war without any international intervention.\nEffect of the Cold War.\nThe Cold War (1947\u20131991) provided a global network of material and ideological support that often helped perpetuate civil wars, which were mainly fought in weak ex-colonial states rather than the relatively strong states that were aligned with the Warsaw Pact and North Atlantic Treaty Organization. In some cases, superpowers would superimpose Cold War ideology onto local conflicts, while in others local actors using Cold War ideology would attract the attention of a superpower to obtain support. Using a separate statistical evaluation than used above for interventions, civil wars that included pro- or anti-communist forces lasted 141% longer than the average non-Cold War conflict, while a Cold War civil war that attracted superpower intervention resulted in wars typically lasting over three times as long as other civil wars. Conversely, the end of the Cold War marked by the fall of the Berlin Wall in 1989 resulted in a reduction in the duration of Cold War civil wars of 92% or, phrased another way, a roughly ten-fold increase in the rate of resolution of Cold War civil wars. Lengthy Cold War-associated civil conflicts that ground to a halt include the wars of Guatemala (1960\u20131996), El Salvador (1979\u20131991) and Nicaragua (1970\u20131990).\nPost-2003.\nAccording to Barbara F. Walter, \"post-2003 civil wars are different from previous civil wars in three striking ways. First, most of them are situated in Muslim-majority countries. Second, most of the rebel groups fighting these wars espouse radical Islamist ideas and goals. Third, most of these radical groups are pursuing transnational rather than national aims.\" She argues \"that the transformation of information technology, especially the advent of the Web 2.0 in the early 2000s, is the big new innovation that is likely driving many of these changes.\"\nEffects.\nCivil wars often have severe economic consequences: two studies estimate that each year of civil war reduces a country's GDP growth by about 2%. It also has a regional effect, reducing the GDP growth of neighboring countries. Civil wars also have the potential to lock the country in a conflict trap, where each conflict increases the likelihood of future conflict."}
{"id": "7088", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=7088", "title": "List of cryptographers", "text": "This is a list of cryptographers. Cryptography is the practice and study of techniques for secure communication in the presence of third parties called adversaries.\nModern.\nSee also: for a more exhaustive list."}
{"id": "7089", "revid": "6908984", "url": "https://en.wikipedia.org/wiki?curid=7089", "title": "Chocolate", "text": "Chocolate is a preparation of roasted and ground cacao seeds that is made in the form of a liquid, paste, or in a block, which may also be used as a flavoring ingredient in other foods. The earliest signs of use are associated with Olmec sites (within what would become Mexico\u2019s post-colonial territory) suggesting consumption of chocolate beverages, dating from the 19th century BC. The majority of Mesoamerican people made chocolate beverages, including the Maya and Aztecs. The English word \"chocolate\" comes, via Spanish, from the Classical Nahuatl word .\nThe seeds of the cacao tree have an intense bitter taste and must be fermented to develop the flavor. After fermentation, the beans are dried, cleaned, and roasted. The shell is removed to produce cacao nibs, which are then ground to cocoa mass, unadulterated chocolate in rough form. Once the cocoa mass is liquefied by heating, it is called chocolate liquor. The liquor may also be cooled and processed into its two components: cocoa solids and cocoa butter. Baking chocolate, also called bitter chocolate, contains cocoa solids and cocoa butter in varying proportions, without any added sugar. Powdered baking cocoa, which contains more fiber than cocoa butter, can be processed with alkali to produce dutch cocoa. Much of the chocolate consumed today is in the form of sweet chocolate, a combination of cocoa solids, cocoa butter or added vegetable oils, and sugar. Milk chocolate is sweet chocolate that additionally contains milk powder or condensed milk. White chocolate contains cocoa butter, sugar, and milk, but no cocoa solids.\nChocolate is one of the most popular food types and flavors in the world, and many foodstuffs involving chocolate exist, particularly desserts, including cakes, pudding, mousse, chocolate brownies, and chocolate chip cookies. Many candies are filled with or coated with sweetened chocolate. Chocolate bars, either made of solid chocolate or other ingredients coated in chocolate, are eaten as snacks. Gifts of chocolate molded into different shapes (such as eggs, hearts, coins) are traditional on certain Western holidays, including Christmas, Easter, Valentine's Day, and Hanukkah. Chocolate is also used in cold and hot beverages, such as chocolate milk and hot chocolate, and in some alcoholic drinks, such as creme de cacao.\nAlthough cocoa originated in the Americas, West African countries, particularly C\u00f4te d'Ivoire and Ghana, are the leading producers of cocoa in the 21st century, accounting for some 60% of the world cocoa supply.\nWith some two million children involved in the farming of cocoa in West Africa, child slavery and trafficking were major concerns in 2018. International attempts to improve conditions for children were failing because of persistent poverty, absence of schools, increasing world cocoa demand, more intensive farming of cocoa, and continued exploitation of child labor.\nHistory.\nMesoamerican usage.\nChocolate has been prepared as a drink for nearly all of its history. For example, one vessel found at an Olmec archaeological site on the Gulf Coast of Veracruz, Mexico, dates chocolate's preparation by pre-Olmec peoples as early as 1750 BC. On the Pacific coast of Chiapas, Mexico, a Mokaya archaeological site provides evidence of cacao beverages dating even earlier, to 1900 BC. The residues and the kind of vessel in which they were found indicate the initial use of cacao was not simply as a beverage, but the white pulp around the cacao beans was likely used as a source of fermentable sugars for an alcoholic drink.\nAn early Classic-period (460\u2013480 AD) Mayan tomb from the site in Rio Azul had vessels with the Maya glyph for cacao on them with residue of a chocolate drink, suggests the Maya were drinking chocolate around 400\u00a0AD. Documents in Maya hieroglyphs stated chocolate was used for ceremonial purposes, in addition to everyday life. The Maya grew cacao trees in their backyards, and used the cacao seeds the trees produced to make a frothy, bitter drink.\nBy the 15th century, the Aztecs gained control of a large part of Mesoamerica and adopted cacao into their culture. They associated chocolate with Quetzalcoatl, who, according to one legend, was cast away by the other gods for sharing chocolate with humans, and identified its extrication from the pod with the removal of the human heart in sacrifice. In contrast to the Maya, who liked their chocolate warm, the Aztecs drank it cold, seasoning it with a broad variety of additives, including the petals of the \"Cymbopetalum penduliflorum\" tree, chile pepper, allspice, vanilla, and honey.\nThe Aztecs were unable to grow cacao themselves, as their home in the Mexican highlands was unsuitable for it, so chocolate was a luxury imported into the empire. Those who lived in areas ruled by the Aztecs were required to offer cacao seeds in payment of the tax they deemed \"tribute\". Cocoa beans were often used as currency. For example, the Aztecs used a system in which one turkey cost 100 cacao beans and one fresh avocado was worth three beans.\nThe Maya and Aztecs associated cacao with human sacrifice, and chocolate drinks specifically with sacrificial human blood.\nThe Spanish royal chronicler Gonzalo Fern\u00e1ndez de Oviedo y Vald\u00e9s described a chocolate drink he had seen in Nicaragua in 1528, mixed with achiote: \"because those people are fond of drinking human blood, to make this beverage seem like blood, they add a little achiote, so that it then turns red. ... and part of that foam is left on the lips and around the mouth, and when it is red for having achiote, it seems a horrific thing, because it seems like blood itself.\"\nEuropean adaptation.\nUntil the 16th century, no European had ever heard of the popular drink from the Central American peoples. Christopher Columbus and his son Ferdinand encountered the cacao bean on Columbus's fourth mission to the Americas on 15 August 1502, when he and his crew seized a large native canoe that proved to contain cacao beans among other goods for trade. Spanish conquistador Hern\u00e1n Cort\u00e9s may have been the first European to encounter it, as the frothy drink was part of the after-dinner routine of Montezuma. Jos\u00e9 de Acosta, a Spanish Jesuit missionary who lived in Peru and then Mexico in the later 16th century, wrote of its growing influence on the Spaniards:\nLoathsome to such as are not acquainted with it, having a scum or froth that is very unpleasant to taste. Yet it is a drink very much esteemed among the Indians, wherewith they feast noble men who pass through their country. The Spaniards, both men, and women, that are accustomed to the country, are very greedy of this Chocolat\u00e9. They say they make diverse sorts of it, some hot, some cold, and some temperate, and put therein much of that 'chili'; yea, they make paste thereof, the which they say is good for the stomach and against the catarrh.\nWhile Columbus had taken cacao beans with him back to Spain, chocolate made no impact until Spanish friars introduced it to the Spanish court. After the Spanish conquest of the Aztecs, chocolate was imported to Europe. There, it quickly became a court favorite. It was still served as a beverage, but the Spanish added sugar, as well as honey (the original sweetener used by the Aztecs for chocolate), to counteract the natural bitterness. Vanilla, another indigenous American introduction, was also a popular additive, with pepper and other spices sometimes used to give the illusion of a more potent vanilla flavor. Unfortunately, these spices tended to unsettle the European constitution; the \"Encyclop\u00e9die\" states, \"The pleasant scent and sublime taste it imparts to chocolate have made it highly recommended; but a long experience having shown that it could potentially upset one's stomach\", which is why chocolate without vanilla was sometimes referred to as \"healthy chocolate\". By 1602, chocolate had made its way from Spain to Austria. By 1662, Pope Alexander VII had declared that religious fasts were not broken by consuming chocolate drinks. Within about a hundred years, chocolate established a foothold throughout Europe.\nThe new craze for chocolate brought with it a thriving slave market, as between the early 1600s and late 1800s, the laborious and slow processing of the cacao bean was manual. Cacao plantations spread, as the English, Dutch, and French colonized and planted. With the depletion of Mesoamerican workers, largely to disease, cacao production was often the work of poor wage laborers and African slaves. Wind-powered and horse-drawn mills were used to speed production, augmenting human labor. Heating the working areas of the table-mill, an innovation that emerged in France in 1732, also assisted in extraction.\nNew processes that sped the production of chocolate emerged early in the Industrial Revolution. In 1815, Dutch chemist Coenraad van Houten introduced alkaline salts to chocolate, which reduced its bitterness. A few years thereafter, in 1828, he created a press to remove about half the natural fat (cocoa butter or cacao butter) from chocolate liquor, which made chocolate both cheaper to produce and more consistent in quality. This innovation introduced the modern era of chocolate.\nKnown as \"Dutch cocoa\", this machine-pressed chocolate was instrumental in the transformation of chocolate to its solid form when, in 1847, English chocolatier Joseph Fry discovered a way to make chocolate moldable when he mixed the ingredients of cocoa powder and sugar with melted cocoa butter. Subsequently, his chocolate factory, Fry's of Bristol, England, began mass-producing chocolate bars, Fry's Chocolate Cream, launched in 1866, and they became very popular. Milk had sometimes been used as an addition to chocolate beverages since the mid-17th century, but in 1875 Swiss chocolatier Daniel Peter invented milk chocolate by mixing a powdered milk developed by Henri Nestl\u00e9 with the liquor. In 1879, the texture and taste of chocolate was further improved when Rudolphe Lindt invented the conching machine.\nBesides Nestl\u00e9, several notable chocolate companies had their start in the late 19th and early 20th centuries. Rowntree's of York set up and began producing chocolate in 1862, after buying out the Tuke family business. Cadbury was manufacturing boxed chocolates in England by 1868. Manufacturing their first Easter egg in 1875, Cadbury created the modern chocolate Easter egg after developing a pure cocoa butter that could easily be molded into smooth shapes. In 1893, Milton S. Hershey purchased chocolate processing equipment at the World's Columbian Exposition in Chicago, and soon began the career of Hershey's chocolates with chocolate-coated caramels.\nIntroduction to the United States.\nThe Baker Chocolate Company, which makes Baker's Chocolate, is the oldest producer of chocolate in the United States. In 1765 Dr. James Baker and John Hannon founded the company in Boston. Using cocoa beans from the West Indies, the pair built their chocolate business, which is still in operation.\nWhite chocolate was first introduced to the U.S. in 1946 by Frederick E. Hebert of Hebert Candies in Shrewsbury, Massachusetts, near Boston, after he had tasted \"white coat\" candies while traveling in Europe.\nEtymology.\nCacao, pronounced by the Olmecs as \"kakawa\", dates to 1000\u00a0BC or earlier. The word \"chocolate\" entered the English language from Spanish in about 1600. The word entered Spanish from the word \"xocol\u0101tl\" in Nahuatl, the language of the Aztecs. The origin of the Nahuatl word is uncertain, as it does not appear in any early Nahuatl source, where the word for chocolate drink is \"cacahuatl\", \"cacao water\". It is possible that the Spaniards coined the word (perhaps in order to avoid \"caca\", a vulgar Spanish word for \"faeces\") by combining the Yucatec Mayan word \"chocol\", \"hot\", with the Nahuatl word \"atl\", \"water\". Another proposed etymology derives it from the word \"chicolatl\", meaning \"beaten drink\", which may derive from the word for the frothing stick, \"chicoli\". The term \"chocolatier\", for a chocolate confection maker, is attested from 1888.\nTypes.\nSeveral types of chocolate can be distinguished. Pure, unsweetened chocolate, often called \"baking chocolate\", contains primarily cocoa solids and cocoa butter in varying proportions. Much of the chocolate consumed today is in the form of sweet chocolate, which combines chocolate with sugar.\nMilk.\nMilk chocolate is sweet chocolate that also contains milk powder or condensed milk. In the UK and Ireland, milk chocolate must contain a minimum of 20% total dry cocoa solids; in the rest of the European Union, the minimum is 25%.\nWhite.\nWhite chocolate, although similar in texture to that of milk and dark chocolate, does not contain any cocoa solids that impart a dark color. In 2002, the US Food and Drug Administration established a standard for white chocolate as the \"common or usual name of products made from cacao fat (i.e., cocoa butter), milk solids, nutritive carbohydrate sweeteners, and other safe and suitable ingredients, but containing no nonfat cacao solids\".\nDark.\nDark chocolate is produced by adding fat and sugar to the cacao mixture. The U.S. Food and Drug Administration calls this \"sweet chocolate\", and requires a 15% concentration of chocolate liquor. European rules specify a minimum of 35% cocoa solids. A higher amount of cocoa solids indicates more bitterness. Semisweet chocolate is dark chocolate with low sugar content. Bittersweet chocolate is chocolate liquor to which some sugar (typically a third), more cocoa butter and vanilla are added. It has less sugar and more liquor than semisweet chocolate, but the two are interchangeable in baking. It is also known to last for two years if stored properly. , there is no high-quality evidence that dark chocolate affects blood pressure significantly or provides other health benefits.\nUnsweetened.\nUnsweetened chocolate is pure chocolate liquor, also known as bitter or baking chocolate. It is unadulterated chocolate: the pure, ground, roasted chocolate beans impart a strong, deep chocolate flavor. It is typically used in baking or other products to which sugar and other ingredients are added. Raw chocolate, often referred to as raw cacao, is always dark and a minimum of 75% cacao.\nPoorly tempered or untempered chocolate may have whitish spots on the dark chocolate part, called chocolate bloom; it is an indication that sugar or fat has separated due to poor storage. It is not toxic and can be safely consumed.\nProduction.\nRoughly two-thirds of the entire world's cocoa is produced in West Africa, with 43% sourced from C\u00f4te d'Ivoire, where, , child labor is a common practice to obtain the product. According to the World Cocoa Foundation, in 2007 some 50\u00a0million people around the world depended on cocoa as a source of livelihood. in the UK, most chocolatiers purchase their chocolate from them, to melt, mold and package to their own design. According to the WCF's 2012 report, the Ivory Coast is the largest producer of cocoa in the world. The two main jobs associated with creating chocolate candy are chocolate makers and chocolatiers. Chocolate makers use harvested cacao beans and other ingredients to produce couverture chocolate (covering). Chocolatiers use the finished couverture to make chocolate candies (bars, truffles, etc.).\nProduction costs can be decreased by reducing cocoa solids content or by substituting cocoa butter with another fat. Cocoa growers object to allowing the resulting food to be called \"chocolate\", due to the risk of lower demand for their crops.\nGenome.\nThe sequencing in 2010 of the genome of the cacao tree may allow yields to be improved. Due to concerns about global warming effects on lowland climate in the narrow band of latitudes where cacao is grown (20 degrees north and south of the equator), the commercial company Mars, Incorporated and the University of California, Berkeley are conducting genomic research in 2017\u201318 to improve the survivability of cacao plants in hot climates.\nCacao varieties.\nChocolate is made from cocoa beans, the dried and fermented seeds of the cacao tree (\"Theobroma cacao\"), a small, 4\u20138 m tall (15\u201326\u00a0ft tall) evergreen tree native to the deep tropical region of the Americas. Recent genetic studies suggest the most common genotype of the plant originated in the Amazon basin and was gradually transported by humans throughout South and Central America. Early forms of another genotype have also been found in what is now Venezuela. The scientific name, \"Theobroma\", means \"food of the gods\". The fruit, called a cacao pod, is ovoid, long and wide, ripening yellow to orange, and weighing about when ripe.\nCacao trees are small, understory trees that need rich, well-drained soils. They naturally grow within 20\u00b0 of either side of the equator because they need about 2000\u00a0mm of rainfall a year, and temperatures in the range of . Cacao trees cannot tolerate a temperature lower than .\nThe three main varieties of cacao beans used in chocolate are criollo, forastero, and trinitario.\nCriollo.\nRepresenting only 5% of all cocoa beans grown , criollo is the rarest and most expensive cocoa on the market, and is native to Central America, the Caribbean islands and the northern tier of South American states. The genetic purity of cocoas sold today as criollo is disputed, as most populations have been exposed to the genetic influence of other varieties.\nCriollos are particularly difficult to grow, as they are vulnerable to a variety of environmental threats and produce low yields of cocoa per tree. The flavor of criollo is described as delicate yet complex, low in classic chocolate flavor, but rich in \"secondary\" notes of long duration.\nForastero.\nThe most commonly grown bean is forastero, a large group of wild and cultivated cacaos, most likely native to the Amazon basin. The African cocoa crop is entirely made up of forastero. They are significantly hardier and of higher yield than criollo. The source of most chocolate marketed, forastero cocoas are typically strong in classic \"chocolate\" flavor, but have a short duration and are unsupported by secondary flavors, producing \"quite bland\" chocolate.\nTrinitario.\nTrinitario is a natural hybrid of criollo and forastero. Trinitario originated in Trinidad after an introduction of forastero to the local criollo crop. Nearly all cacao produced over the past five decades is of the forastero or lower-grade trinitario varieties.\nProcessing.\nCacao pods are harvested by cutting them from the tree using a machete, or by knocking them off the tree using a stick. The beans with their surrounding pulp are removed from the pods and placed in piles or bins, allowing access to micro-organisms so fermentation of the pectin-containing material can begin. Yeasts produce ethanol, lactic acid bacteria produce lactic acid, and acetic acid bacteria produce acetic acid. The fermentation process, which takes up to seven days, also produces several flavor precursors, eventually resulting in the familiar chocolate taste.\nIt is important to harvest the pods when they are fully ripe, because if the pod is unripe, the beans will have a low cocoa butter content, or sugars in the white pulp will be insufficient for fermentation, resulting in a weak flavor. After fermentation, the beans must be quickly dried to prevent mold growth. Climate and weather permitting, this is done by spreading the beans out in the sun from five to seven days.\nThe dried beans are then transported to a chocolate manufacturing facility. The beans are cleaned (removing twigs, stones, and other debris), roasted, and graded. Next, the shell of each bean is removed to extract the nib. Finally, the nibs are ground and liquefied, resulting in pure chocolate in fluid form: chocolate liquor. The liquor can be further processed into two components: cocoa solids and cocoa butter.\nBlending.\nChocolate liquor is blended with the cocoa butter in varying quantities to make different types of chocolate or couverture. The basic blends of ingredients for the various types of chocolate (in order of highest quantity of cocoa liquor first), are:\nUsually, an emulsifying agent, such as soy lecithin, is added, though a few manufacturers prefer to exclude this ingredient for purity reasons and to remain GMO-free, sometimes at the cost of a perfectly smooth texture. Some manufacturers are now using PGPR, an artificial emulsifier derived from castor oil that allows them to reduce the amount of cocoa butter while maintaining the same mouthfeel.\nThe texture is also heavily influenced by processing, specifically conching (see below). The more expensive chocolate tends to be processed longer and thus has a smoother texture and mouthfeel, regardless of whether emulsifying agents are added.\nDifferent manufacturers develop their own \"signature\" blends based on the above formulas, but varying proportions of the different constituents are used. The finest, plain dark chocolate couverture contains at least 70% cocoa (both solids and butter), whereas milk chocolate usually contains up to 50%. High-quality white chocolate couverture contains only about 35% cocoa butter.\nProducers of high-quality, small-batch chocolate argue that mass production produces bad-quality chocolate. Some mass-produced chocolate contains much less cocoa (as low as 7% in many cases), and fats other than cocoa butter. Vegetable oils and artificial vanilla flavor are often used in cheaper chocolate to mask poorly fermented and/or roasted beans.\nIn 2007, the Chocolate Manufacturers Association in the United States, whose members include Hershey, Nestl\u00e9, and Archer Daniels Midland, lobbied the Food and Drug Administration (FDA) to change the legal definition of chocolate to let them substitute partially hydrogenated vegetable oils for cocoa butter, in addition to using artificial sweeteners and milk substitutes. Currently, the FDA does not allow a product to be referred to as \"chocolate\" if the product contains any of these ingredients.\nIn the EU a product can be sold as chocolate if it contains up to 5% vegetable oil, and must be labeled as \"family milk chocolate\" rather than \"milk chocolate\" if it contains 20% milk.\nAccording to Canadian Food and Drug Regulations, a \"chocolate product\" is a food product that is sourced from at least one \"cocoa product\" and contains at least one of the following: \"chocolate, bittersweet chocolate, semi-sweet chocolate, dark chocolate, sweet chocolate, milk chocolate, or white chocolate\". A \"cocoa product\" is defined as a food product that is sourced from cocoa beans and contains \"cocoa nibs, cocoa liquor, cocoa mass, unsweetened chocolate, bitter chocolate, chocolate liquor, cocoa, low-fat cocoa, cocoa powder, or low-fat cocoa powder\".\nConching.\nThe penultimate process is called conching. A conche is a container filled with metal beads, which act as grinders. The refined and blended chocolate mass is kept in a liquid state by frictional heat. Chocolate before conching has an uneven and gritty texture. The conching process produces cocoa and sugar particles smaller than the tongue can detect, hence the smooth feel in the mouth. The length of the conching process determines the final smoothness and quality of the chocolate. High-quality chocolate is conched for about 72\u00a0hours, and lesser grades about four to six hours. After the process is complete, the chocolate mass is stored in tanks heated to about until final processing.\nTempering.\nThe final process is called tempering. Uncontrolled crystallization of cocoa butter typically results in crystals of varying size, some or all large enough to be seen with the naked eye. This causes the surface of the chocolate to appear mottled and matte, and causes the chocolate to crumble rather than snap when broken. The uniform sheen and crisp bite of properly processed chocolate are the results of consistently small cocoa butter crystals produced by the tempering process.\nThe fats in cocoa butter can crystallize in six different forms (polymorphous crystallization). The primary purpose of tempering is to assure that only the best form is present. The six different crystal forms have different properties.\nAs a solid piece of chocolate, the cocoa butter fat particles are in a crystalline rigid structure that gives the chocolate its solid appearance. Once heated, the crystals of the polymorphic cocoa butter can break apart from the rigid structure and allow the chocolate to obtain a more fluid consistency as the temperature increases \u2013 the melting process. When the heat is removed, the cocoa butter crystals become rigid again and come closer together, allowing the chocolate to solidify.\nThe temperature in which the crystals obtain enough energy to break apart from their rigid conformation would depend on the milk fat content in the chocolate and the shape of the fat molecules, as well as the form of the cocoa butterfat. Chocolate with a higher fat content will melt at a lower temperature.\nMaking chocolate considered \"good\" is about forming as many type V crystals as possible. This provides the best appearance and texture and creates the most stable crystals, so the texture and appearance will not degrade over time. To accomplish this, the temperature is carefully manipulated during the crystallization.\nGenerally, the chocolate is first heated to to melt all six forms of crystals. Next, the chocolate is cooled to about , which will allow crystal types IV and V to form. At this temperature, the chocolate is agitated to create many small crystal \"seeds\" which will serve as nuclei to create small crystals in the chocolate. The chocolate is then heated to about to eliminate any type IV crystals, leaving just type V. After this point, any excessive heating of the chocolate will destroy the temper and this process will have to be repeated. Other methods of chocolate tempering are used as well. The most common variant is introducing already tempered, solid \"seed\" chocolate. The temper of chocolate can be measured with a chocolate temper meter to ensure accuracy and consistency. A sample cup is filled with the chocolate and placed in the unit which then displays or prints the results.\nTwo classic ways of manually tempering chocolate are:\nChocolate tempering machines (or temperers) with computer controls can be used for producing consistently tempered chocolate. In particular, continuous tempering machines are used in large volume applications. Various methods and apparatuses for continuous flow tempering have been described by Aasted, Sollich and Buhler, three manufacturers of commercial chocolate equipment, with a focus now on energy efficiency. In general, molten chocolate coming in at 40\u201350\u00a0\u00b0C is cooled in heat exchangers to crystallization temperates of about 26\u201330\u00a0\u00b0C, passed through a tempering column consisting of spinning plates to induce shear, then warmed slightly to re-melt undesirable crystal formations.\nStorage.\nChocolate is very sensitive to temperature and humidity. Ideal storage temperatures are between , with a relative humidity of less than 50%. If refrigerated or frozen without containment, chocolate can absorb enough moisture to cause a whitish discoloration, the result of fat or sugar crystals rising to the surface. Various types of \"blooming\" effects can occur if chocolate is stored or served improperly.\nChocolate bloom is caused by storage temperature fluctuating or exceeding , while sugar bloom is caused by temperature below or excess humidity. To distinguish between different types of bloom, one can rub the surface of the chocolate lightly, and if the bloom disappears, it is fat bloom. Moving chocolate between temperature extremes, can result in an oily texture. Although visually unappealing, chocolate suffering from bloom is safe for consumption and taste unaffected. Bloom can be reversed by retempering the chocolate or using it for any use that requires melting the chocolate.\nChocolate is generally stored away from other foods, as it can absorb different aromas. Ideally, chocolates are packed or wrapped, and placed in proper storage with the correct humidity and temperature. Additionally, chocolate is frequently stored in a dark place or protected from light by wrapping paper. The glossy shine, snap, aroma, texture, and taste of the chocolate can show the quality and if it was stored well.\nComposition.\nNutrition.\nA 100-gram serving of milk chocolate supplies 540 calories. It is 59% carbohydrates (52% as sugar and 3% as dietary fiber), 30% fat and 8% protein (table). Approximately 65% of the fat in milk chocolate is saturated, mainly palmitic acid and stearic acid, while the predominant unsaturated fat is oleic acid (table).\n100-grams of milk chocolate is an \"excellent source\" (over 19% of the Daily Value, DV) of riboflavin, vitamin B12 and the dietary minerals, manganese, phosphorus and zinc. Chocolate is a \"good source\" (10\u201319% DV) of calcium, magnesium and iron.\nEffects on health.\nChocolate may be a factor for heartburn in some people because one of its constituents, theobromine, may affect the esophageal sphincter muscle in a way that permits stomach acids to enter the esophagus. Theobromine poisoning is an overdosage reaction to the bitter alkaloid, which happens more frequently in domestic animals than humans. However, daily intake of 50\u2013100 g cocoa (0.8\u20131.5 g theobromine) by humans has been associated with sweating, trembling and severe headache. Chocolate contains alkaloids such as theobromine and phenethylamine, which have physiological effects in humans, but the presence of theobromine renders it toxic to some animals, including dogs and cats.\nAccording to a 2005 study, the average lead concentration of cocoa beans is \u2264 0.5 ng/g, which is one of the lowest reported values for a natural food. However, during cultivation and production, chocolate may absorb lead from the environment (such as in atmospheric emissions of leaded gasoline, which is still being used in Nigeria). Reports from 2014 indicate that \"chocolate might be a significant source\" of lead ingestion for children if consumption is high (with dark chocolate containing higher amounts), and \"one 10\u00a0g cube of dark chocolate may contain as much as 20% of the daily lead oral limit.\"\nChocolate and cocoa contain moderate to high amounts of oxalate, which may increase the risk of kidney stones. \nA few studies have documented allergic reactions from chocolate in children. Other research has shown that dark chocolate can aggravate acne in men who are prone to it. Research has also shown that consuming dark chocolate does not substantially affect blood pressure. Chocolate and cocoa are under preliminary research to determine if consumption affects the risk of certain cardiovascular diseases or cognitive abilities.\nOne tablespoonful (5 grams) of dry unsweetened cocoa powder has 12.1\u00a0mg of caffeine and a 25-g single serving of dark chocolate has 22.4\u00a0mg of caffeine. Although a single 7\u00a0oz. serving of coffee may contain 80\u2013175\u00a0mg, studies have shown psychoactive effects in caffeine doses as low as 9\u00a0mg, and a dose as low as 12.5\u00a0mg was shown to have effects on cognitive performance.\nExcessive consumption of large quantities of any energy-rich food, such as chocolate, without a corresponding increase in activity to expend the associated calories, can cause weight gain and possibly lead to obesity. Raw chocolate is high in cocoa butter, a fat which is removed during chocolate refining and then added back in varying proportions during the manufacturing process. Manufacturers may add other fats, sugars, and milk, all of which increase the caloric content of chocolate.\nPhytochemicals.\nIn comparison to most foods, cocoa contains more phenolic antioxidants. Cocoa solids are a source of flavonoids and alkaloids, such as theobromine, phenethylamine, and caffeine.\nLabeling.\nSome manufacturers provide the percentage of chocolate in a finished chocolate confection as a label quoting percentage of \"cocoa\" or \"cacao\". This refers to the combined percentage of both cocoa solids and cocoa butter in the bar, not just the percentage of cocoa solids. The Belgian AMBAO certification mark indicates that no non-cocoa vegetable fats have been used in making the chocolate. A long-standing dispute between Britain on the one hand and Belgium and France over British use of vegetable fats in chocolate ended in 2000 with the adoption of new standards which permitted the use of up to five percent vegetable fats in clearly labelled products. This British style of chocolate has sometimes been pejoratively referred to as \"vegelate\".\nChocolates that are organic or fair trade certified carry labels accordingly.\nIn the United States, some large chocolate manufacturers lobbied the federal government to permit confections containing cheaper hydrogenated vegetable oil in place of cocoa butter to be sold as \"chocolate\". In June 2007, in response to consumer concern about the proposal, the FDA reiterated \"Cacao fat, as one of the signature characteristics of the product, will remain a principal component of standardized chocolate.\"\nIndustry.\nChocolate, prevalent throughout the world, is a steadily growing, US$50\u00a0billion-a-year worldwide business. Europe accounts for 45% of the world's chocolate revenue, and the US spent $20\u00a0billion in 2013. Big Chocolate is the grouping of major international chocolate companies in Europe and the U.S. U.S. companies Mars and Hershey's alone generated $13\u00a0billion a year in chocolate sales and account for two-thirds of U.S. production in 2004. Despite the expanding reach of the chocolate industry internationally, cocoa farmers and labourers in the Ivory Coast are unaware of the uses of the beans; the high cost of chocolate products in the Ivory Coast make it inaccessible to the majority of the population, who do not know what it tastes like.\nManufacturers.\nChocolate manufacturers produce a range of products from chocolate bars to fudge. Large manufacturers of chocolate products include Cadbury (the world's largest confectionery manufacturer), Ferrero, Guylian, The Hershey Company, Lindt &amp; Spr\u00fcngli, Mars, Incorporated, Milka, Neuhaus and Suchard.\nGuylian is best known for its chocolate sea shells; Cadbury for its Dairy Milk and Creme Egg. The Hershey Company, the largest chocolate manufacturer in North America, produces the Hershey Bar and Hershey's Kisses. Mars Incorporated, a large privately owned U.S. corporation, produces Mars Bar, Milky Way, M&amp;M's, Twix, and Snickers. Lindt is known for its truffle balls and gold foil-wrapped Easter bunnies.\nFood conglomerates Nestl\u00e9 SA and Kraft Foods both have chocolate brands. Nestl\u00e9 acquired Rowntree's in 1988 and now markets chocolates under their brand, including Smarties (a chocolate candy) and Kit Kat (a chocolate bar); Kraft Foods through its 1990 acquisition of Jacobs Suchard, now owns Milka and Suchard. In February 2010, Kraft also acquired British-based Cadbury; Fry's, Trebor Basset and the fair trade brand Green &amp; Black's also belongs to the group.\nChild labor in cocoa harvesting.\nThe widespread use of children in cocoa production is controversial, not only for the concerns about child labor and exploitation, but also because up to 12,000 of the 200,000 children working in C\u00f4te d'Ivoire, the world's biggest producer of cocoa, may be victims of trafficking or slavery. Most attention on this subject has focused on West Africa, which collectively supplies 69 percent of the world's cocoa, and C\u00f4te d'Ivoire in particular, which supplies 35 percent of the world's cocoa. Thirty percent of children under age 15 in sub-Saharan Africa are child laborers, mostly in agricultural activities including cocoa farming. Major chocolate producers, such as Nestl\u00e9, buy cocoa at commodities exchanges where Ivorian cocoa is mixed with other cocoa.\nIn 2009, Salvation Army International Development (SAID) UK stated that 12,000 children have been trafficked on cocoa farms in the Ivory Coast of Africa, where half of the world's chocolate is made. SAID UK states that it is these child slaves who are likely to be working in \"harsh and abusive\" conditions for the production of chocolate, and an increasing number of health-food and anti-slavery organisations are highlighting and campaigning against the use of trafficking in the chocolate industry.\nAs of 2017, approximately 2.1\u00a0million children in Ghana and C\u00f4te d'Ivoire were involved in farming cocoa, carrying heavy loads, clearing forests, and being exposed to pesticides. According to Sona Ebai, the former secretary-general of the Alliance of Cocoa Producing Countries: \"I think child labor cannot be just the responsibility of industry to solve. I think it's the proverbial all-hands-on-deck: government, civil society, the private sector. And there, you need leadership.\" Reported in 2018, a 3-year pilot program \u2013 conducted by Nestl\u00e9 with 26,000 farmers mostly located in C\u00f4te d'Ivoire \u2013 observed a 51% decrease in the number of children doing hazardous jobs in cocoa farming. The US Department of Labor formed the Child Labor Cocoa Coordinating Group as a public-private partnership with the governments of Ghana and C\u00f4te d'Ivoire to address child labor practices in the cocoa industry. The International Cocoa Initiative involving major cocoa manufacturers established the Child Labor Monitoring and Remediation System intended to monitor thousands of farms in Ghana and C\u00f4te d'Ivoire for child labor conditions, but the program reached less than 20% of the child laborers. Despite these efforts, goals to reduce child labor in West Africa by 70% before 2020 are frustrated by persistent poverty, absence of schools, expansion of cocoa farmland, and increased demand for cocoa.\nIn April 2018, the Cocoa Barometer report stated: \"Not a single company or government is anywhere near reaching the sector-wide objective of the elimination of child labor, and not even near their commitments of a 70% reduction of child labor by 2020\".\nFair trade.\nIn the 2000s, some chocolate producers began to engage in fair trade initiatives, to address concerns about the marginalization of cocoa laborers in developing countries. Traditionally, Africa and other developing countries received low prices for their exported commodities such as cocoa, which caused poverty to abound. Fairtrade seeks to establish a system of direct trade from developing countries to counteract this unfair system. One solution for fair labor practices is for farmers to become part of an Agricultural cooperative. Cooperatives pay farmers a fair price for their cocoa so farmers have enough money for food, clothes, and school fees. One of the main tenets of fair trade is that farmers receive a fair price, but this does not mean that the larger amount of money paid for fair trade cocoa goes directly to the farmers. The effectiveness of fair trade has been questioned. In a 2014 article, \"The Economist\" stated that workers on fair trade farms have a lower standard of living than on similar farms outside the fair trade system.\nUsage and consumption.\nBars.\nChocolate is sold in chocolate bars, which come in dark chocolate, milk chocolate and white chocolate varieties. Some bars that are mostly chocolate have other ingredients blended into the chocolate, such as nuts, raisins, or crisped rice. Chocolate is used as an ingredient in a huge variety of bars, which typically contain various confectionary ingredients (e.g., nougat, wafers, caramel, nuts, etc.) which are coated in chocolate.\nCoating and filling.\nChocolate is used as a flavouring product in many desserts, such as chocolate cakes, chocolate brownies, chocolate mousse and chocolate chip cookies. Numerous types of candy and snacks contain chocolate, either as a filling (e.g., M&amp;M's) or as a coating (e.g., chocolate-coated raisins or chocolate-coated peanuts).\nBeverages.\nSome non-alcoholic beverages contain chocolate, such as chocolate milk, hot chocolate and chocolate milkshakes. Some alcoholic liqueurs are flavoured with chocolate, such as chocolate liqueur and creme de cacao. Chocolate is a popular flavour of ice cream and pudding, and chocolate sauce is a commonly added as a topping on ice cream sundaes. The caff\u00e8 mocha is an espresso beverage containing chocolate."}
{"id": "7093", "revid": "59", "url": "https://en.wikipedia.org/wiki?curid=7093", "title": "Cetaceans", "text": ""}
{"id": "7094", "revid": "19382112", "url": "https://en.wikipedia.org/wiki?curid=7094", "title": "Cetacean", "text": ""}
{"id": "7098", "revid": "900004532", "url": "https://en.wikipedia.org/wiki?curid=7098", "title": "COPPA", "text": ""}
{"id": "7099", "revid": "22105", "url": "https://en.wikipedia.org/wiki?curid=7099", "title": "Child Online Privacy Protection Act", "text": ""}
{"id": "7100", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=7100", "title": "Cornet", "text": "The cornet (, ) is a brass instrument similar to the trumpet but distinguished from it by its conical bore, more compact shape, and mellower tone quality. The most common cornet is a transposing instrument in B, though there is also a soprano cornet in E and cornets in A and C. All are unrelated to the Renaissance and early Baroque cornett.\nHistory.\nThe cornet derived from the posthorn, by applying rotary valves to it in the 1820s in France. However by the 1830s, Parisian makers were using piston valves. Cornets first appeared as separate instrumental parts in 19th-century French compositions.\nThis instrument could not have been developed without the improvement of piston valves by Silesian horn player Friedrich Bl\u00fchmel and Heinrich St\u00f6lzel in the early 19th century. These two instrument makers almost simultaneously invented valves, though it is likely that Bl\u00fchmel (or Bl\u00fcmel) was the inventor, and St\u00f6lzel who developed a practical instrument. They jointly applied for a patent and were granted this for a period of ten years. Later, and most importantly, Fran\u00e7ois P\u00e9rinet received a patent in 1838 for an improved valve which is the basis of all modern brass instrument piston valves. The first notable virtuoso player was Jean-Baptiste Arban, who studied the cornet extensively and published \"La grande m\u00e9thode compl\u00e8te de cornet \u00e0 piston et de saxhorn\", commonly referred to as the \"Arban method\", in 1864. Up until the early 20th century, the trumpet and cornet co-existed in musical ensembles. Symphonic repertoire often involves separate parts for trumpet and cornet. As several instrument builders made improvements to both instruments, they started to look and sound more alike. The modern-day cornet is used in brass bands, concert bands, and in specific orchestral repertoire that requires a more mellow sound.\nThe name cornet derives from corne, meaning \"horn\", itself from Latin 'cornu'. While not musically related, instruments of the Zink family (which includes serpents) are named \"cornetto\" or \"cornett\" in modern English to distinguish them from the valved cornet described here. The 11th edition of the \"Encyclop\u00e6dia Britannica\" referred to serpents as \"old wooden cornets\". The Roman/Etruscan cornu (or simply \"horn\") is the lingual ancestor of these. It is a predecessor of the post horn from which the cornet evolved, and was used like a bugle to signal orders on the battlefield.\nRelationship to trumpet.\nThe cornet's valves allowed for melodic playing throughout the register of the cornet. Trumpets were slower to adopt the new valve technology, so for 100 years or more, composers often wrote separate parts for trumpet and cornet. The trumpet would play fanfare-like passages, while the cornet played more melodic passages. The modern trumpet has valves that allow it to play the same notes and fingerings as the cornet.\nCornets and trumpets made in a given key (usually the key of B) play at the same pitch, and the technique for playing the instruments is nearly identical. However, cornets and trumpets are not entirely interchangeable, as they differ in timbre. Also available, but usually seen only in the brass band, is an E soprano model, pitched a fourth above the standard B.\nUnlike the trumpet, which has a cylindrical bore up to the bell section, the tubing of the cornet has a mostly conical bore, starting very narrow at the mouthpiece and gradually widening towards the bell. Cornets following the 1913 patent of E.A. Couturier can have a continuously conical bore. The conical bore of the cornet is primarily responsible for its characteristic warm, mellow tone, which can be distinguished from the more penetrating sound of the trumpet. The conical bore of the cornet also makes it more agile than the trumpet when playing fast passages, but correct pitching is often less assured. The cornet is often preferred for young beginners as it is easier to hold, with its centre of gravity much closer to the player.\nThe cornet mouthpiece has a shorter and narrower shank than that of a trumpet so it can fit the cornet's smaller mouthpiece receiver. The cup size is often deeper than that of a trumpet mouthpiece.\nOne variety is the short model traditional cornet, also known as a \"Shepherd's Crook\" shaped model. These are most often large\u2013bore instruments with a rich mellow sound. There is also a long-model or \"American-wrap\" cornet, often with a smaller bore and a brighter sound, which is produced in a variety of different tubing wraps and is closer to a trumpet in appearance. The Shepherd's Crook model is preferred by cornet traditionalists. The long-model cornet is generally used in concert bands in the United States, but has found little following in British-style brass and concert bands.\nA third and relatively rare variety\u2014distinct from the long-model or \"American-wrap\" cornet\u2014is the \"long cornet\", which was produced in the mid-20th Century by C.G. Conn and F.E. Olds and visually is nearly indistinguishable from a trumpet except that it has a receiver fashioned to accept cornet mouthpieces.\nEcho cornet.\nThe echo cornet has been called an obsolete variant. It has a mute chamber (or echo chamber) mounted to the side acting as a second bell when the fourth valve is pressed. The second bell has a sound similar to that of a Harmon mute and is typically used to play echo phrases, whereupon the player imitates the sound from the primary bell using the echo chamber.\nPlaying technique.\nLike the trumpet and all other modern brass wind instruments, the cornet makes a sound when the player vibrates (\"buzzes\") the lips in the mouthpiece, creating a vibrating column of air in the tubing. The frequency of the air column's vibration can be modified by changing the lip tension and aperture or \"embouchure\", and by altering the tongue position to change the shape of the oral cavity, thereby increasing or decreasing the speed of the airstream. In addition, the column of air can be lengthened by engaging one or more valves, thus lowering the pitch. Double and triple tonguing are also possible.\nWithout valves, the player could produce only a harmonic series of notes like those played by the bugle and other \"natural\" brass instruments. These notes are far apart for most of the instrument's range, making diatonic and chromatic playing impossible except in the extreme high register. The valves change the length of the vibrating column and provide the cornet with the ability to play chromatically.\nEnsembles with cornets.\nBrass band.\nBritish brass bands consist only of brass instruments and a percussion section. The cornet is the leading melodic instrument in this ensemble; trumpets are never used. The ensemble consists of about thirty musicians, including nine B cornets and one E cornet (soprano cornet). In the UK, companies such as Besson and Boosey &amp; Hawkes specialized in instrument for brass bands. In America, 19th-century manufacturers such as Graves and Company, Hall and Quinby, E.G. Wright and the Boston Musical Instrument Manufactury made instruments for this ensemble.\nConcert band.\nThe cornet features in the British-style concert band, and early American concert band pieces, particularly those written or transcribed before 1960, often feature distinct, separate parts for trumpets and cornets. Cornet parts are rarely included in later American pieces, however, and cornets are replaced in modern American bands by the trumpet. This slight difference in instrumentation derives from the British concert band's heritage in military bands, where the highest brass instrument is always the cornet. There are usually four to six B cornets present in a British concert band, but no E instrument, as this role is taken by the E clarinet.\nFanfare orkest.\nFanfare orkesten (\"fanfare orchestras\"), found in only the Netherlands, Belgium, Northern France and Lithuania, use the complete saxhorn family of instruments. The standard instrumentation includes both the cornet and the trumpet; however, in recent decades, the cornet has largely been replaced by the trumpet.\nJazz ensemble.\nIn old style jazz bands, the cornet was preferred to the trumpet, but from the swing era onwards, it has been largely replaced by the louder, more piercing trumpet. Likewise the cornet has been largely phased out of big bands by a growing taste for louder and more aggressive instruments, especially since the advent of bebop in the post World War II era.\nJazz pioneer Buddy Bolden played the cornet, and Louis Armstrong started off on the cornet but his switch to the trumpet is often credited with beginning of the trumpet's dominance in jazz. Cornetists such as Bubber Miley and Rex Stewart contributed substantially to the Duke Ellington Orchestra's early sound. Other influential jazz cornetists include Freddie Keppard, King Oliver, Bix Beiderbecke, Ruby Braff, Bobby Hackett, and Nat Adderley. Notable performances on cornet by players generally associated with the trumpet include Freddie Hubbard's on \"Empyrean Isles\" by Herbie Hancock and Don Cherry's on \"The Shape of Jazz to Come\" by Ornette Coleman.\nSymphony orchestra.\nSoon after its invention, the cornet was introduced into the symphony orchestra, supplementing the trumpets. The use of valves meant they could play a full chromatic scale in contrast with trumpets, which were still restricted to the harmonic series. In addition, their tone was found to unify the horn and trumpet sections. Hector Berlioz was the first significant composer to use them in these ways, and his orchestral works often use pairs of both trumpets and cornets, the latter playing more of the melodic lines. In his \"Symphonie fantastique\" (1830), he added a counter-melody for a solo cornet in the second movement (\"Un Bal\").\nCornets continued to be used, particularly in French compositions, well after the valve trumpet was common. They blended well with other instruments, and were held to be better suited to certain types of melody. Tchaikovsky used them effectively this way in his \"Capriccio Italien\" (1880).\nFrom the early 20th century, the cornet and trumpet combination was still favored by some composers, including Edward Elgar and Igor Stravinsky, but tended to be used for occasions when the composer wanted the specific mellower and more agile sound. The sounds of cornet and trumpet have grown closer together over time and the former is now rarely used as an ensemble instrument: in the first version of his ballet \"Petrushka\" (1911), Stravinsky gives a celebrated solo to the cornet; in the 1946 revision he removed cornets from the orchestration and instead assigned the solo to the trumpet."}
{"id": "7102", "revid": "1159327", "url": "https://en.wikipedia.org/wiki?curid=7102", "title": "CAMP", "text": "CAMP, cAMP or camP may stand for:"}
{"id": "7103", "revid": "16860921", "url": "https://en.wikipedia.org/wiki?curid=7103", "title": "CGMP", "text": "CGMP is an initialism. It can refer to:"}
{"id": "7104", "revid": "16422414", "url": "https://en.wikipedia.org/wiki?curid=7104", "title": "Cotton Mather", "text": "Cotton Mather (February 12, 1663 \u2013 February 13, 1728) was a New England Puritan minister, prolific author, and pamphleteer. One of the most important intellectual figures in English-speaking colonial America, Mather is remembered today chiefly for his \"Magnalia Christi Americana\" (1702) and other works of history, for his scientific contributions to plant hybridization and to the promotion of inoculation as a means of preventing smallpox and other infectious diseases, and for his involvement in the events surrounding the Salem witch trials of 1692\u20133. He also promoted the new Newtonian science in America and sent many scientific reports to the Royal Society of London, which formally elected him as a fellow in 1723. A controversial figure in his own day, he sought unsuccessfully the presidency of Harvard College, which had been held by his father Increase, another important Puritan intellectual.\nLife and work.\nMather was born in Boston, Massachusetts Bay Colony, the son of Maria (n\u00e9e Cotton) and Increase Mather, and grandson of both John Cotton and Richard Mather, all also prominent Puritan ministers. Mather was named after his maternal grandfather John Cotton. He attended Boston Latin School, where his name was posthumously added to its Hall of Fame, and graduated from Harvard in 1678 at age 15. After completing his post-graduate work, he joined his father as assistant pastor of Boston's original North Church (not to be confused with the Anglican/Episcopal Old North Church of Paul Revere fame). In 1685, Mather assumed full responsibilities as pastor of the church.\nMather wrote more than 450 books and pamphlets, and his ubiquitous literary works made him one of the most influential religious leaders in America. He set the moral tone in the colonies and sounded the call for second- and third-generation Puritans, whose parents had left England for the New England colonies, to return to the theological roots of Puritanism. The most important of these was \"Magnalia Christi Americana\" (1702) which comprises seven distinct books, many of which depict biographical and historical narratives.\nMather influenced early American science. In 1716, he conducted one of the first recorded experiments with plant hybridization based on his observations of corn varieties. This observation was memorialized in a letter to his friend James Petiver:\nIn November 1713, Mather's wife, newborn twins, and two-year-old daughter all succumbed during a measles epidemic. He was twice widowed, and only two of his 15 children survived him; he died on the day after his 65th birthday and was buried on Copp's Hill, near Old North Church.\nBoyle's influence on Mather.\nRobert Boyle was a huge influence throughout Mather's career. He read Boyle's \"The Usefulness of Experimental Natural Philosophy\" closely throughout the 1680s, and his own early works on science and religion borrowed greatly from it, using almost identical language to Boyle.\nIncrease Mather.\nMather's relationship with his father Increase Mather is thought by some to have been strained and difficult. Increase was a pastor of the North Square Church and president of Harvard College; he led an accomplished life. Despite Cotton's efforts, he never became quite as well known and successful in politics as his father. He did surpass his father's output as a writer, writing more than 400 books. One of the most public displays of their strained relationship emerged during the witch trials, which Increase Mather reportedly did not support.\nYale College.\nCotton Mather helped convince Elihu Yale to make a donation to a new college in New Haven, which became Yale College.\nSalem witch trials of 1692, the Mather influence.\nPre-trials.\nIn 1689, Mather published \"Memorable Providences\" detailing the supposed afflictions of several children in the Goodwin family in Boston. Mather had a prominent role in the witchcraft case against Catholic washerwoman Goody Glover, which ultimately resulted in her conviction and execution. Besides praying for the children, which also included fasting and meditation, he would also observe and record their activities. The children were subject to hysterical fits, which he detailed in \"Memorable Providences\". In his book, Mather argued that since there are witches and devils, there are \"immortal souls.\" He also claimed that witches appear spectrally as themselves. He opposed any natural explanations for the fits; he believed that people who confessed to using witchcraft were sane; he warned against performing magic due to its connection with the devil; and he argued that spectral evidence should not be used as evidence for witchcraft.\nRobert Calef was a contemporary of Mather and critical of him, and he considered this book responsible for laying the groundwork for the Salem witch trials three years later:\nNineteenth-century historian Charles Wentworth Upham shared the view that the afflicted in Salem were imitating the Goodwin children, but he put the blame on both Cotton and his father Increase Mather:\nThe court.\nMather was influential in the construction of the court for the trials from the beginning. Sir William Phips, governor of the newly chartered Province of Massachusetts Bay, appointed his lieutenant governor, William Stoughton, as head of a special witchcraft tribunal and then as chief justice of the colonial courts, where he presided over the witch trials. According to George Bancroft, Mather had been influential in gaining the politically unpopular Stoughton his appointment as lieutenant governor under Phips through the intervention of Mather's own politically powerful father, Increase. \"Intercession had been made by Cotton Mather for the advancement of Stoughton, a man of cold affections, proud, self-willed and covetous of distinction.\" Apparently Mather saw in Stoughton, a bachelor who had never wed, an ally for church-related matters. Bancroft quotes Mather's reaction to Stoughton's appointment as follows:\nMather claimed not to have attended the trials in Salem (although his father attended the trial of George Burroughs). His contemporaries Calef and Thomas Brattle place him at the executions (see below). Mather began to publicize and celebrate the trials well before they were put to an end: \"If in the midst of the many Dissatisfaction among us, the publication of these Trials may promote such a pious Thankfulness unto God, for Justice being so far executed among us, I shall Re-joyce that God is Glorified.\" Mather called himself a historian not an advocate but, according to one modern writer, his writing largely presumes the guilt of the accused and includes such comments as calling Martha Carrier \"a rampant hag\". Mather referred to George Burroughs as a \"very puny man\" whose \"tergiversations, contradictions, and falsehoods\" made his testimony not \"worth considering\".\nCaution on the use of spectral evidence.\nThe afflicted girls claimed that the semblance of a defendant, invisible to any but themselves, was tormenting them; this was considered evidence of witchcraft, despite the defendant's denial and profession of strongly held Christian beliefs. On May 31, 1692, Mather wrote to one of the judges, John Richards, a member of his congregation, expressing his support of the prosecutions, but cautioning; \"do not lay more stress on pure spectral evidence than it will bear \u2026 It is very certain that the Devils have sometimes represented the Shapes of persons not only innocent, but also very virtuous. Though I believe that the just God then ordinarily provides a way for the speedy vindication of the persons thus abused.\"\nAn opinion on the matter was sought from the ministers of the area and a response was submitted June 15, 1692. Cotton Mather seems to take credit for the varied responses when anonymously celebrating himself years later: \"drawn up at their desire, by Cotton Mather the younger, as I have been informed.\" The \"Return of the Several Ministers\" ambivalently discussed whether or not to allow spectral evidence. The original full version of the letter was reprinted in late 1692 in the final two pages of Increase Mather's \"Cases of Conscience\". It is a curious document and remains a source of confusion and argument. Calef calls it \"perfectly Ambidexter, giving as great as greater Encouragement to proceed in those dark methods, then cautions against them\u2026 indeed the Advice then given, looks most like a thing of his Composing, as carrying both Fire to increase and Water to quench the Conflagration.\" It seems likely that the \"Several\" ministers consulted did not agree, and thus Cotton Mather's construction and presentation of the advice could have been crucial to its interpretation.\nThomas Hutchinson summarized the Return, \"The two first and the last sections of this advice took away the force of all the others, and the prosecutions went on with more vigor than before.\" Reprinting the Return five years later in his anonymously published \"Life of Phips\" (1697), Cotton Mather omitted the fateful \"two first and the last\" sections, though they were the ones he had already given most attention in his \"Wonders of the Invisible World\" rushed into publication in the summer and early autumn of 1692.\nOn August 19, 1692, Mather attended the execution of George Burroughs (and four others who were executed after Mather spoke) and Robert Calef presents him as playing a direct and influential role:\nOn September 2, 1692, after eleven of the accused had been executed, Cotton Mather wrote a letter to Chief Justice William Stoughton congratulating him on \"extinguishing of as wonderful a piece of devilism as has been seen in the world\" and claiming that \"one half of my endeavors to serve you have not been told or seen.\"\nRegarding spectral evidence, Upham concludes that \"Cotton Mather never in any public writing 'denounced the admission' of it, never advised its absolute exclusion; but on the contrary recognized it as a ground of 'presumption' \u2026 [and once admitted] nothing could stand against it. Character, reason, common sense, were swept away.\" In a letter to an English clergyman in 1692, Boston intellectual Thomas Brattle, criticizing the trials, said of the judges' use of spectral evidence:\nThe later exclusion of spectral evidence from trials by Governor Phips, around the same time his own wife's (Lady Mary Phips) name coincidentally started being bandied about in connection with witchcraft, began in January 1693. This immediately brought about a sharp decrease in convictions. Due to a reprieve by Phips, there were no further executions. Phips's actions were vigorously opposed by William Stoughton.\nBancroft notes that Mather considered witches \"among the poor, and vile, and ragged beggars upon Earth\", and Bancroft asserts that Mather considered the people against the witch trials to be witch advocates.\nPost-trials.\nIn the years after the trials, of the principal actors in the trial, whose lives are recorded after, neither he nor Stoughton admitted strong misgivings. For several years after the trials, Cotton Mather continued to defend them and seemed to hold out a hope for their return.\n\"Wonders of the Invisible World\" contained a few of Mather's sermons, the conditions of the colony and a description of witch trials in Europe. He somewhat clarified the contradictory advice he had given in \"Return of the Several Ministers\", by defending the use of spectral evidence. \"Wonders of the Invisible World\" appeared around the same time as Increase Mather's \"Cases of Conscience.\"\"\nMather did not sign his name or support his father's book initially:\nThe last major events in Mather's involvement with witchcraft were his interactions with Mercy Short in December 1692 and Margaret Rule in September 1693. The latter brought a five year campaign by Boston merchant Robert Calef against the influential and powerful Mathers. Calef's book \"More Wonders of the Invisible World\" was inspired by the fear that Mather would succeed in once again stirring up new witchcraft trials, and the need to bear witness to the horrible experiences of 1692. He quotes the public apologies of the men on the jury and one of the judges. Increase Mather was said to have publicly burned Calef's book in Harvard Yard around the time he was removed from the head of the college and replaced by Samuel Willard.\nPoole vs. Upham.\nCharles Wentworth Upham wrote \"Salem Witchcraft Volumes I and II With an Account of Salem Village and a History of Opinions on Witchcraft and Kindred Subjects\", which runs to almost 1,000 pages. It came out in 1867 and cites numerous criticisms of Mather by Robert Calef.\nWilliam Frederick Poole defended Mather from these criticisms.\nIn 1869, Poole quoted from various school textbooks of the time demonstrating they were in agreement on Cotton Mather's role in the Witch Trial\n&lt;poem&gt;\nIf anyone imagines that we are stating the case too strongly, let him try an experiment with the first bright boy he meets by asking...\n'Who got up Salem Witchcraft?'... he will reply, 'Cotton Mather'. Let him try another boy...\n'Who was Cotton Mather?' and the answer will come, 'The man who was on horseback, and hung witches.'&lt;/poem&gt;\nPoole was a librarian, and a lover of literature, including Mather's \"Magnalia\" \"and other books and tracts, numbering nearly 400 [which] were never so prized by collectors as today.\" Poole announced his intention to redeem Mather's name, using as a springboard a harsh critique of Upham's book, via his own book \"Cotton Mather and Salem witchcraft\". A quick search of the name Mather in Upham's book (referring to either father, son, or ancestors) shows that it occurs 96 times. Poole's critique runs less than 70 pages but the name \"Mather\" occurs many more times than the other book, which is more than ten times as long. Upham shows a balanced and complicated view of Cotton Mather, such as this first mention: \"One of Cotton Mather's most characteristic productions is the tribute to his venerated master. It flows from a heart warm with gratitude.\"\nUpham's book refers to Robert Calef no fewer than 25 times with the majority of these regarding documents compiled by Calef in the mid-1690s and stating: \"Although zealously devoted to the work of exposing the enormities connected with the witchcraft prosecutions, there is no ground to dispute the veracity of Calef as to matters of fact.\" He goes on to say that Calef's collection of writings \"gave a shock to Mather's influence, from which it never recovered.\"\nCalef produced only the one book; he is self-effacing and apologetic for his limitations, and on the title page he is listed not as author but \"collector\". Poole, champion of literature, could not accept Calef whose \"faculties, as indicated by his writings appear to us to have been of an inferior order;\u2026\", and his book \"in our opinion, has a reputation much beyond its merits.\" Poole refers to Calef as Mather's \"personal enemy\" and opens a line, \"Without discussing the character and motives of Calef\u2026\" but does not follow up on this suggestive comment to discuss any actual or purported motive or reason to impugn Calef. Upham responded to Poole (referring to Poole as \"the Reviewer\") in a book running five times as long and sharing the same title but with the clauses reversed: \"Salem Witchcraft and Cotton Mather\". Many of Poole's arguments were addressed, but both authors emphasize the importance of Cotton Mather's difficult and contradictory view on spectral evidence, as copied in the final pages, called \"The Return of Several Ministers\", of Increase Mather's \"Cases of Conscience\".\nThe debate continues: Kittredge vs. Burr.\nEvidenced by the published opinion in the years that followed the Poole vs Upham debate, it would seem Upham was considered the clear winner (see Sibley, GH Moore, WC Ford, and GH Burr below.). In 1891, Harvard English professor Barrett Wendall wrote \"Cotton Mather, The Puritan Priest\". His book often expresses agreement with Upham but also announces an intention to show Cotton Mather in a more positive light. \"[Cotton Mather] gave utterance to many hasty things not always consistent with fact or with each other\u2026\" And some pages later: \"[Robert] Calef's temper was that of the rational Eighteenth century; the Mathers belonged rather to the Sixteenth, the age of passionate religious enthusiasm.\"\nIn 1907, George Lyman Kittredge published an essay that would become foundational to a major change in the 20th-century view of witchcraft and Mather culpability therein. Kittredge is dismissive of Robert Calef, and sarcastic toward Upham, but shows a fondness for Poole and a similar soft touch toward Cotton Mather. Responding to Kittredge in 1911, George Lincoln Burr, a historian at Cornell, published an essay that begins in a professional and friendly fashion toward both Poole and Kittredge, but quickly becomes a passionate and direct criticism, stating that Kittredge in the \"zeal of his apology\u2026 reached results so startlingly new, so contradictory of what my own lifelong study in this field has seemed to teach, so unconfirmed by further research\u2026 and withal so much more generous to our ancestors than I can find it in my conscience to deem fair, that I should be less than honest did I not seize this earliest opportunity share with you the reasons for my doubts\u2026\" (In referring to \"ancestors\" Burr primarily means the Mathers, as is made clear in the substance of the essay.) The final paragraph of Burr's 1911 essay pushes these men's debate into the realm of a progressive creed\n\u2026 I fear that they who begin by excusing their ancestors may end by excusing themselves.\nPerhaps as a continuation of his argument, in 1914, George Lincoln Burr published a large compilation \"Narratives\". This book arguably continues to be the single most cited reference on the subject. Unlike Poole and Upham, Burr avoids forwarding his previous debate with Kittredge directly into his book and mentions Kittredge only once, briefly in a footnote citing both of their essays from 1907 and 1911, but without further comment. But in addition to the viewpoint displayed by Burr's selections, he weighs in on the Poole vs Upham debate at various times, including siding with Upham in a note on Thomas Brattle's letter, \"The strange suggestion of W. F. Poole that Brattle here means Cotton Mather himself, is adequately answered by Upham\u2026\" Burr's \"Narratives\" reprint a lengthy but abridged portion of Calef's book and introducing it he digs deep into the historical record for information on Calef and concludes \"\u2026that he had else any grievance against the Mathers or their colleagues there is no reason to think.\" Burr finds that a comparison between Calef's work and original documents in the historical record collections \"testify to the care and exactness\u2026\"\n20th century revision: The Kittredge lineage at Harvard.\n1920\u20133 Kenneth B. Murdock wrote a doctoral dissertation on Increase Mather advised by Chester Noyes Greenough and Kittredge. Murdock's father was a banker hired in 1920 to run the Harvard Press and he published his son's dissertation as a handsome volume in 1925: \"Increase Mather, The Foremost American Puritan\" (Harvard University Press). Kittredge was right hand man to the elder Murdock at the Press. This work focuses on Increase Mather and is more critical of the son, but the following year he published a selection of Cotton Mather's writings with an introduction that claims Cotton Mather was \"not less but more humane than his contemporaries. Scholars have demonstrated that his advice to the witch judges was always that they should be more cautious in accepting evidence\" against the accused. Murdock's statement seems to claim a majority view. But one wonders who Murdock would have meant by \"scholars\" at this time other than Poole, Kittredge, and TJ Holmes (below) and Murdock's obituary calls him a pioneer \"in the reversal of a movement among historians of American culture to discredit the Puritan and colonial period\u2026\"\n1924 Thomas J. Holmes was an Englishman with no college education, but he apprenticed in bookbinding and emigrated to the U.S. and became the librarian at the William G. Mather Library in Ohio where he likely met Murdock. In 1924, Holmes wrote an essay for the Bibliographical Society of America identifying himself as part of the Poole-Kittredge lineage and citing Kenneth B. Murdock's still unpublished dissertation. In 1932 Holmes published a bibliography of Increase Mather followed by \"Cotton Mather, A Bibliography\" (1940). Holmes often cites Murdock and Kittredge and is highly knowledgeable about the construction of books. Holmes' work also includes Cotton Mather's October 20, 1692 letter (see above) to his uncle opposing an end to the trials.\n 1930 Samuel Eliot Morison published \"Builders of the Bay Colony.\" Morison chose not to include anyone with the surname Mather or Cotton in his collection of twelve \"builders\" and in the bibliography writes \"I have a higher opinion than most historians of Cotton Mather's \"Magnalia\"\u2026 Although Mather is inaccurate, pedantic, and not above \"suppresio veri\", he does succeed in giving a living picture of the person he writes about.\" Whereas Kittredge and Murdock worked from the English department, Morison was from Harvard's history department. Morison's view seems to have evolved over the course of the 1930s, as can be seen in \"Harvard College in the Seventeenth Century\" (1936) published while Kittredge ran the Harvard press, and in a year that coincided with the tercentary of the college: \"Since the appearance of Professor Kittredge's work, it is not necessary to argue that a man of learning\u2026\" of that era should be judged on his view of witchcraft. In \"The Intellectual Life of Colonial New England\" (1956), Morison writes that Cotton Mather found balance and level-thinking during the witchcraft trials. Like Poole, Morison suggests Calef had an agenda against Mather, without providing supporting evidence.\n1953 Perry Miller published \"The New England Mind: From Colony to Province\" (Belknap Press of Harvard University Press). Miller worked from the Harvard English Department and his expansive prose contains few citations, but the \"Bibliographical Notes\" for Chapter XIII \"The Judgement of the Witches\" references the bibliographies of TJ Holmes (above) calling Holmes portrayal of Cotton Mather's composition of \"Wonders\" \"an epoch in the study of Salem Witchcraft.\" However, following the discovery of the authentic holograph of the September 2, 1692 letter, in 1985, David Levin writes that the letter demonstrates that the timeline employed by TJ Holmes and Perry Miller, is off by \"three weeks.\" Contrary to the evidence in the later arriving letter, Miller portrays Phips and Stoughton as pressuring Cotton Mather to write the book (p.201): \"If ever there was a false book produced by a man whose heart was not in it, it is \"The Wonders\"\u2026.he was insecure, frightened, sick at heart\u2026\" The book \"has ever since scarred his reputation,\" Perry Miller writes. Miller seems to imagine Cotton Mather as sensitive, tender, and a good vehicle for his jeremiad thesis: \"His mind was bubbling with every sentence of the jeremiads, for he was heart and soul in the effort to reorganize them.\n1969 Chadwick Hansen \"Witchcraft at Salem\". Hansen states a purpose to \"set the record straight\" and reverse the \"traditional interpretation of what happened at Salem\u2026\" and names Poole and Kittredge as like-minded influences. (Hansen reluctantly keys his footnotes to Burr's anthology for the reader's convenience, \"in spite of [Burr's] anti-Puritan bias\u2026\") Hansen presents Mather as a positive influence on the Salem Trials and considers Mather's handling of the Goodwin children sane and temperate. Hansen posits that Mather was a moderating influence by opposing the death penalty for those who confessed\u2014or feigned confession\u2014such as Tituba and Dorcas Good, and that most negative impressions of him stem from his \"defense\" of the ongoing trials in \"Wonders of the Invisible World\". Writing an introduction to a facsimile of Robert Calef's book in 1972, Hansen compares Robert Calef to Joseph Goebbels, and also explains that, in Hansen's opinion, women \"are more subject to hysteria than men.\"\n1971 \"The Admirable Cotton Mather\" by James Playsted Wood. A young adult book. In the preface, Wood discusses the Harvard-based revision and writes that Kittredge and Murdock \"added to a better understanding of a vital and courageous man\u2026\" \n1985 David Hall writes, \"With [Kittredge] one great phase of interpretation came to a dead end.\" Hall writes that whether the old interpretation favored by \"antiquarians\" had begun with the \"malice of Robert Calef or deep hostility to Puritanism,\" either way \"such notions are no longer\u2026 the concern of the historian.\" But David Hall notes \"one minor exception. Debate continues on the attitude and role of Cotton Mather\u2026\" \nTercentenary of the trials and ongoing scholarship.\nToward the later half of the twentieth century, a number of historians at universities far from New England seemed to find inspiration in the Kittredge lineage. In \"Selected Letters of Cotton Mather\" Ken Silverman writes, \"Actually, Mather had very little to do with the trials.\" Twelve pages later Silverman publishes, for the first time, a letter to chief judge William Stoughton on September 2, 1692, in which Cotton Mather writes \"\u2026 I hope I can may say that one half of my endeavors to serve you have not been told or seen \u2026 I have labored to divert the thoughts of my readers with something of a designed contrivance\u2026\"\nWriting in the early 1980s, historian John Demos imputed to Mather a purportedly moderating influence on the trials.\nCoinciding with the tercentary of the trials in 1992, there was a flurry of publications.\nHistorian Larry Gregg highlights Mather's cloudy thinking and confusion between sympathy for the possessed, and the boundlessness of spectral evidence when Mather stated, \"the devil have sometimes represented the shapes of persons not only innocent, but also the very virtuous.\"\nSmallpox inoculation controversy.\nThe practice of smallpox inoculation (as opposed to the later practice of vaccination) was developed possibly in 8th-century India or 10th-century China. Spreading its reach in seventeenth-century Turkey, inoculation or, rather, variolation, involved infecting a person via a cut in the skin with exudate from a patient with a relatively mild case of smallpox (variola), to bring about a manageable and recoverable infection that would provide later immunity. By the beginning of the 18th century, the Royal Society in England was discussing the practice of inoculation, and the smallpox epidemic in 1713 spurred further interest. It was not until 1721, however, that England recorded its first case of inoculation.\nEarly New England.\nSmallpox was a serious threat in colonial America, most devastating to Native Americans, but also to Anglo-American settlers. New England suffered smallpox epidemics in 1677, 1689\u201390, and 1702. It was highly contagious, and mortality could reach as high as 30 percent. Boston had been plagued by smallpox outbreaks in 1690 and 1702. During this era, public authorities in Massachusetts dealt with the threat primarily by means of quarantine. Incoming ships were quarantined in Boston Harbor, and any smallpox patients in town were held under guard or in a \"pesthouse\".\nIn 1706, Onesimus, one of Mather's slaves, explained to Mather how he had been inoculated as a child in Africa. Mather was fascinated by the idea. By July 1716, he had read an endorsement of inoculation by Dr Emanuel Timonius of Constantinople in the \"Philosophical Transactions\". Mather then declared, in a letter to Dr John Woodward of Gresham College in London, that he planned to press Boston's doctors to adopt the practice of inoculation should smallpox reach the colony again.\nBy 1721, a whole generation of young Bostonians was vulnerable and memories of the last epidemic's horrors had by and large disappeared. Smallpox returned on April 22 of that year, when HMS \"Seahorse\" arrived from the West Indies carrying smallpox on board. Despite attempts to protect the town through quarantine, nine known cases of smallpox appeared in Boston by May 27, and by mid-June, the disease was spreading at an alarming rate. As a new wave of smallpox hit the area and continued to spread, many residents fled to outlying rural settlements. The combination of exodus, quarantine, and outside traders' fears disrupted business in the capital of the Bay Colony for weeks. Guards were stationed at the House of Representatives to keep Bostonians from entering without special permission. The death toll reached 101 in September, and the Selectmen, powerless to stop it, \"severely limited the length of time funeral bells could toll.\" As one response, legislators delegated a thousand pounds from the treasury to help the people who, under these conditions, could no longer support their families.\nOn June 6, 1721, Mather sent an abstract of reports on inoculation by Timonius and Jacobus Pylarinus to local physicians, urging them to consult about the matter. He received no response. Next, Mather pleaded his case to Dr. Zabdiel Boylston, who tried the procedure on his youngest son and two slaves\u2014one grown and one a boy. All recovered in about a week. Boylston inoculated seven more people by mid-July. The epidemic peaked in October 1721, with 411 deaths; by February 26, 1722, Boston was again free from smallpox. The total number of cases since April 1721 came to 5,889, with 844 deaths\u2014more than three-quarters of all the deaths in Boston during 1721. Meanwhile, Boylston had inoculated 287 people, with six resulting deaths.\nInoculation debate.\nBoylston and Mather's inoculation crusade \"raised a horrid Clamour\" among the people of Boston. Both Boylston and Mather were \"Object[s] of their Fury; their furious Obloquies and Invectives\", which Mather acknowledges in his diary. Boston's Selectmen, consulting a doctor who claimed that the practice caused many deaths and only spread the infection, forbade Boylston from performing it again.\n\"The New-England Courant\" published writers who opposed the practice. The editorial stance was that the Boston populace feared that inoculation spread, rather than prevented, the disease; however, some historians, notably H. W. Brands, have argued that this position was a result of the contrarian positions of editor-in-chief James Franklin (a brother of Benjamin Franklin). Public discourse ranged in tone from organized arguments by John Williams from Boston, who posted that \"several arguments proving that inoculating the smallpox is not contained in the law of Physick, either natural or divine, and therefore unlawful\", to those put forth in a pamphlet by Dr. William Douglass of Boston, entitled \"The Abuses and Scandals of Some Late Pamphlets in Favour of Inoculation of the Small Pox\" (1721), on the qualifications of inoculation's proponents. (Douglass was exceptional at the time for holding a medical degree from Europe.) At the extreme, in November 1721, someone hurled a lighted grenade into Mather's home.\nMedical opposition.\nSeveral opponents of smallpox inoculation, among them Rev. John Williams, stated that there were only two laws of physick (medicine): sympathy and antipathy. In his estimation, inoculation was neither a sympathy toward a wound or a disease, or an antipathy toward one, but the creation of one. For this reason, its practice violated the natural laws of medicine, transforming health care practitioners into those who harm rather than heal.\nAs with most colonists, Williams' Puritan beliefs were enmeshed in every aspect of his life, and he used the Bible to state his case. He quoted , when Jesus said: \"It is not the healthy who need a doctor, but the sick.\" William Douglass proposed a more secular argument against inoculation, stressing the importance of reason over passion and urging the public to be pragmatic in their choices. In addition, he demanded that ministers leave the practice of medicine to physicians, and not meddle in areas where they lacked expertise. According to Douglass, smallpox inoculation was \"a medical experiment of consequence,\" one not to be undertaken lightly. He believed that not all learned individuals were qualified to doctor others, and while ministers took on several roles in the early years of the colony, including that of caring for the sick, they were now expected to stay out of state and civil affairs. Douglass felt that inoculation caused more deaths than it prevented. The only reason Mather had had success in it, he said, was because Mather had used it on children, who are naturally more resilient. Douglass vowed to always speak out against \"the wickedness of spreading infection\". Speak out he did: \"The battle between these two prestigious adversaries [Douglass and Mather] lasted far longer than the epidemic itself, and the literature accompanying the controversy was both vast and venomous.\"\nPuritan resistance.\nGenerally, Puritan pastors favored the inoculation experiments. Increase Mather, Cotton's father, was joined by prominent pastors Benjamin Colman and William Cooper in openly propagating the use of inoculations. \"One of the classic assumptions of the Puritan mind was that the will of God was to be discerned in nature as well as in revelation.\" Nevertheless, Williams questioned whether the smallpox \"is not one of the strange works of God; and whether inoculation of it be not a fighting with the most High.\" He also asked his readers if the smallpox epidemic may have been given to them by God as \"punishment for sin,\" and warned that attempting to shield themselves from God's fury (via inoculation), would only serve to \"provoke him more\".\nPuritans found meaning in affliction, and they did not yet know why God was showing them disfavor through smallpox. Not to address their errant ways before attempting a cure could set them back in their \"errand\". Many Puritans believed that creating a wound and inserting poison was doing violence and therefore was antithetical to the healing art. They grappled with adhering to the Ten Commandments, with being proper church members and good caring neighbors. The apparent contradiction between harming or murdering a neighbor through inoculation and the Sixth Commandment\u2014\"thou shalt not kill\"\u2014seemed insoluble and hence stood as one of the main objections against the procedure. Williams maintained that because the subject of inoculation could not be found in the Bible, it was not the will of God, and therefore \"unlawful.\" He explained that inoculation violated The Golden Rule, because if one neighbor voluntarily infected another with disease, he was not doing unto others as he would have done to him. With the Bible as the Puritans' source for all decision-making, lack of scriptural evidence concerned many, and Williams vocally scorned Mather for not being able to reference an inoculation edict directly from the Bible.\nInoculation defended.\nWith the smallpox epidemic catching speed and racking up a staggering death toll, a solution to the crisis was becoming more urgently needed by the day. The use of quarantine and various other efforts, such as balancing the body's humors, did not slow the spread of the disease. As news rolled in from town to town and correspondence arrived from overseas, reports of horrific stories of suffering and loss due to smallpox stirred mass panic among the people. \"By circa 1700, smallpox had become among the most devastating of epidemic diseases circulating in the Atlantic world.\"\nMather strongly challenged the perception that inoculation was against the will of God and argued the procedure was not outside of Puritan principles. He wrote that \"whether a Christian may not employ this Medicine (let the matter of it be what it will) and humbly give Thanks to God's good Providence in discovering of it to a miserable World; and humbly look up to His Good Providence (as we do in the use of any other Medicine) It may seem strange, that any wise Christian cannot answer it. And how strangely do Men that call themselves Physicians betray their Anatomy, and their Philosophy, as well as their Divinity in their invectives against this Practice?\" The Puritan minister began to embrace the sentiment that smallpox was an inevitability for anyone, both the good and the wicked, yet God had provided them with the means to save themselves. Mather reported that, from his view, \"none that have used it ever died of the Small Pox, tho at the same time, it were so malignant, that at least half the People died, that were infected With it in the Common way.\"\nWhile Mather was experimenting with the procedure, prominent Puritan pastors Benjamin Colman and William Cooper expressed public and theological support for them. The practice of smallpox inoculation was eventually accepted by the general population due to first-hand experiences and personal relationships. Although many were initially wary of the concept, it was because people were able to witness the procedure's consistently positive results, within their own community of ordinary citizens, that it became widely utilized and supported. One important change in the practice after 1721 was regulated quarantine of innoculees.\nThe aftermath.\nAlthough Mather and Boylston were able to demonstrate the efficacy of the practice, the debate over inoculation would continue even beyond the epidemic of 1721\u201322. They almost caught the disease themselves. After overcoming considerable difficulty and achieving notable success, Boylston traveled to London in 1725, where he published his results and was elected to the Royal Society in 1726, with Mather formally receiving the honor two years prior.\nSermons against pirates and piracy.\nThroughout his career Mather was also keen to minister to convicted pirates. He produced a number of pamphlets and sermons concerning piracy, including \"Faithful Warnings to prevent Fearful Judgments\"; \"Instructions to the Living, from the Condition of the Dead\"; \"The Converted Sinner ... A Sermon Preached in Boston, May 31, 1724, In the Hearing and at the Desire of certain Pirates\"; \"A Brief Discourse occasioned by a Tragical Spectacle of a Number of Miserables under Sentence of Death for Piracy\"; \"Useful Remarks. An Essay upon Remarkables in the Way of Wicked Men\" and \"The Vial Poured Out Upon the Sea\". His father Increase had preached at the trial of Dutch pirate Peter Roderigo; Cotton Mather in turn preached at the trials and sometimes executions of pirate Captains (or the crews of) William Fly, John Quelch, Samuel Bellamy, William Kidd, Charles Harris, and John Phillips. He also ministered to Thomas Hawkins, Thomas Pound, and William Coward; having been convicted of piracy, they were jailed alongside \"Mary Glover the Irish Catholic witch,\" daughter of witch \"Goody\" Ann Glover at whose trial Mather had also preached.\nIn his conversations with William Fly and his crew Mather scolded them: \"You have something within you, that will compell you to confess, That the Things which you have done, are most Unreasonable and Abominable. The Robberies and Piracies, you have committed, you can say nothing to Justify them. \u2026 It is a most hideous Article in the Heap of Guilt lying on you, that an Horrible Murder is charged upon you; There is a cry of Blood going up to Heaven against you.\"\nWorks.\n\"Boston Ephemeris\".\nThe Boston Ephemeris was an almanac written by Mather in 1686. The content was similar to what is known today as the \"Farmer's Almanac\". This was particularly important because it shows that Cotton Mather had influence in mathematics during the time of Puritan New England. This almanac contained a significant amount of astronomy, celestial within the text of the almanac the positions and motions of these celestial bodies, which he must have calculated by hand. \n\"The Biblia Americana\".\nWhen Mather died, he left behind an abundance of unfinished writings, including one entitled \"The Biblia Americana\". Mather believed that \"Biblia Americana\" was the best thing he had ever written; his masterwork. \"Biblia Americana\" contained Mather's thoughts and opinions on the Bible and how he interpreted it. \"Biblia Americana\" is incredibly large, and Mather worked on it from 1693 until 1728, when he died. Mather tried to convince others that philosophy and science could work together with religion instead of against it. People did not have to choose one or the other. In \"Biblia Americana\", Mather looked at the Bible through a scientific perspective, completely opposite to his perspective in \"The Christian Philosopher\", in which he approached science in a religious manner.\n\"Pillars of Salt\".\nMather's first published sermon, printed in 1686, concerned the execution of James Morgan, convicted of murder. Thirteen years later, Mather published the sermon in a compilation, along with other similar works, called \"Pillars of Salt\".\n\"Magnalia Christi Americana\".\n\"Magnalia Christi Americana\", considered Mather's greatest work, was published in 1702, when he was 39. The book includes several biographies of saints and describes the process of the New England settlement. In this context \"saints\" does not refer to the canonized saints of the Catholic church, but to those Puritan divines about whom Mather is writing. It comprises seven total books, including \"Pietas in Patriam: The life of His Excellency Sir William Phips\", originally published anonymously in London in 1697. Despite being one of Mather's best-known works, some have openly criticized it, labeling it as hard to follow and understand, and poorly paced and organized. However, other critics have praised Mather's work, citing it as one of the best efforts at properly documenting the establishment of America and growth of the people.\n\"The Christian Philosopher\".\nIn 1721, Mather published \"The Christian Philosopher\", the first systematic book on science published in America. Mather attempted to show how Newtonian science and religion were in harmony. It was in part based on Robert Boyle's \"The Christian Virtuoso\" (1690). Mather reportedly took inspiration from \"Hayy ibn Yaqdhan\", by the 12th-century Islamic philosopher Abu Bakr Ibn Tufail.\nDespite condemning the \"Mahometans\" as infidels, Mather viewed the novel's protagonist, Hayy, as a model for his ideal Christian philosopher and monotheistic scientist. Mather viewed Hayy as a noble savage and applied this in the context of attempting to understand the Native American Indians, in order to convert them to Puritan Christianity. Mather's short treatise on the Lord's Supper was later translated by his cousin Josiah Cotton.\nIn popular culture.\nThe Handsome Family's 2006 album \"Last Days of Wonder\" is named in reference to Mather's 1693 book \"Wonders of the Invisible World\", which lyricist Rennie Sparks found intriguing because of what she called its \"madness brimming under the surface of things.\""}
{"id": "7105", "revid": "1010269550", "url": "https://en.wikipedia.org/wiki?curid=7105", "title": "Cordwainer Smith", "text": " \nPaul Myron Anthony Linebarger (July 11, 1913 \u2013 August 6, 1966), better known by his pen-name Cordwainer Smith, was an American author known for his science fiction works. Linebarger was a US Army officer, a noted East Asia scholar, and an expert in psychological warfare. Although his career as a writer was shortened by his death at the age of 53, he is considered one of the more talented and influential science fiction authors.\nEarly life and education.\nLinebarger's father, Paul Myron Wentworth Linebarger, was a lawyer and political activist, an advisor to Sun Yat-sen, and had close ties to the leaders of the Chinese revolution of 1911.\nIn order for Linebarger to be eligible to become president of the United States, his father sent Linebarger's mother to Milwaukee, Wisconsin, to give birth to him there.\nLinebarger's godfather was Sun Yat-sen, who was considered the father of Chinese nationalism.\nHis young life was unsettled as his father moved the family to a succession of places in Asia, Europe, and the United States. He was sometimes sent to boarding schools for safety. In all, Linebarger attended more than 30 schools. In 1919, while at a boarding school in Hawaii, he was blinded in his right eye and it was replaced by a glass eye. The vision in his remaining eye was impaired by infection.\nLinebarger was familiar with six languages by adulthood. At the age of 23, he received a PhD in political science from Johns Hopkins University.\nCareer.\nFrom 1937 to 1946, Linebarger held a faculty appointment at Duke University, where he began producing highly regarded works on Far Eastern affairs.\nWhile retaining his professorship at Duke after the beginning of World War II, Linebarger began serving as a second lieutenant of the United States Army, where he was involved in the creation of the Office of War Information and the Operation Planning and Intelligence Board. He also helped organize the army's first psychological warfare section. In 1943, he was sent to China to coordinate military intelligence operations. When he later pursued his interest in China, Linebarger became a close confidant of Chiang Kai-shek. By the end of the war, he had risen to the rank of major.\nIn 1947, Linebarger moved to the Johns Hopkins University's School of Advanced International Studies in Washington, DC, where he served as Professor of Asiatic Studies. He used his experiences in the war to write the book \"Psychological Warfare\" (1948), regarded by many in the field as a classic text.\nHe eventually rose to the rank of colonel in the reserves. He was recalled to advise the British forces in the Malayan Emergency and the U.S. Eighth Army in the Korean War. While he was known to call himself a \"visitor to small wars\", he refrained from becoming involved in the Vietnam War, but is known to have done work for the Central Intelligence Agency. In 1969 CIA officer Miles Copeland Jr. wrote that Linebarger was \"perhaps the leader practitioner of 'black' and 'gray' propaganda in the Western world\". According to Joseph Burkholder Smith, a former CIA operative, he conducted classes in psychological warfare for CIA agents at his home in Washington under cover of his position at the School of Advanced International Studies. He traveled extensively and became a member of the Foreign Policy Association, and was called upon to advise President John F. Kennedy.\nMarriage and family.\nIn 1936, Linebarger married Margaret Snow. They had a daughter in 1942 and another in 1947. They divorced in 1949.\nIn 1950, Linebarger married again to Genevieve Collins; they had no children. They remained married until his death from a heart attack in 1966, at Johns Hopkins University Medical Center in Baltimore, Maryland, at age 53. Linebarger had expressed a wish to retire to Australia, which he had visited in his travels. He is buried in Arlington National Cemetery, Section 35, Grave Number 4712. His widow, Genevieve Collins Linebarger, was interred with him on November 16, 1981.\nCase history debate.\nLinebarger is long rumored to have been \"Kirk Allen\", the fantasy-haunted subject of \"The Jet-Propelled Couch,\" a chapter in psychologist Robert M. Lindner's best-selling 1954 collection \"The Fifty-Minute Hour.\" According to Cordwainer Smith scholar Alan C. Elms, this speculation first reached print in Brian Aldiss's 1973 history of science fiction, \"Billion Year Spree\"; Aldiss, in turn, claimed to have received the information from science fiction fan and scholar Leon Stover. More recently, both Elms and librarian Lee Weinstein have gathered circumstantial evidence to support the case for Linebarger's being Allen, but both concede there is no direct proof that Linebarger was ever a patient of Lindner's or that he suffered from a disorder similar to that of Kirk Allen.\nScience fiction style.\nAccording to Frederik Pohl\nLinebarger's identity as \"Cordwainer Smith\" was secret until his death. (\"Cordwainer\" is an archaic word for \"a worker in cordwain or cordovan leather; a shoemaker\", and a \"smith\" is \"one who works in iron or other metals; esp. a blacksmith or farrier\": two kinds of skilled workers with traditional materials.)\nLinebarger also employed the literary pseudonyms \"Carmichael Smith\" (for his political thriller \"Atomsk\"), \"Anthony Bearden\" (for his poetry) and \"Felix C. Forrest\" (for the novels \"Ria\" and \"Carola\").\nSmith's stories are unusual, sometimes being written in narrative styles closer to traditional Chinese stories than to most English-language fiction, as well as reminiscent of the Genji tales of Lady Murasaki. The total volume of his science fiction output is relatively small, because of his time-consuming profession and his early death.\nSmith's works consist of one novel, originally published in two volumes in edited form as \"The Planet Buyer\", also known as \"The Boy Who Bought Old Earth\" (1964) and \"The Underpeople\" (1968), and later restored to its original form as \"Norstrilia\" (1975); and 32 short stories (collected in \"The Rediscovery of Man\" (1993), including two versions of the short story \"War No. 81-Q\").\nLinebarger's cultural links to China are partially expressed in the pseudonym \"Felix C. Forrest\", which he used in addition to \"Cordwainer Smith\": his godfather Sun Yat-Sen suggested to Linebarger that he adopt the Chinese name \"Lin Bai-lo\" (), which may be roughly translated as \"Forest of Incandescent Bliss\". (\"Felix\" is Latin for \"happy\".) In his later years, Linebarger proudly wore a tie with the Chinese characters for this name embroidered on it.\nAs an expert in psychological warfare, Linebarger was very interested in the newly developing fields of psychology and psychiatry. He used many of their concepts in his fiction. His fiction often has religious overtones or motifs, particularly evident in characters who have no control over their actions. James B. Jordan argued for the importance of Anglicanism to Smith's works back to 1949. But Linebarger's daughter Rosana Hart has indicated that he did not become an Anglican until 1950, and was not strongly interested in religion until later still. The introduction to the collection \"Rediscovery of Man\" notes that from around 1960 Linebarger became more devout and expressed this in his writing. Linebarger's works are sometimes included in analyses of Christianity in fiction, along with the works of authors such as C. S. Lewis and J.R.R. Tolkien.\nMost of Smith's stories are set in the far future, between 4,000 and 14,000 years from now. After the Ancient Wars devastate Earth, humans, ruled by the Instrumentality of Mankind, rebuild and expand to the stars in the Second Age of Space around 6000 AD. Over the next few thousand years, mankind spreads to thousands of worlds and human life becomes safe but sterile, as robots and the animal-derived Underpeople take over many human jobs and humans themselves are genetically programmed as embryos for specified duties. Towards the end of this period, the Instrumentality attempts to revive old cultures and languages in a process known as the Rediscovery of Man, where humans emerges from their mundane utopia and Underpeople are freed from slavery.\nFor years, Linebarger had a pocket notebook which he had filled with ideas about The Instrumentality and additional stories in the series. But while in a small boat in a lake or bay in the mid 60s, he leaned over the side, and his notebook fell out of his breast pocket into the water, where it was lost forever. Another story claims that he accidentally left the notebook in a restaurant in Rhodes in 1965. With the book gone, he felt empty of ideas, and decided to start a new series which was an allegory of Mid-Eastern politics.\nSmith's stories describe a long future history of Earth. The settings range from a postapocalyptic landscape with walled cities, defended by agents of the Instrumentality, to a state of sterile utopia, in which freedom can be found only deep below the surface, in long-forgotten and buried anthropogenic strata. These features may place Smith's works within the Dying Earth subgenre of science fiction. They are ultimately more optimistic and distinctive.\nSmith's most celebrated short story is his first-published, \"Scanners Live in Vain\", which led many of its earliest readers to assume that \"Cordwainer Smith\" was a new pen name for one of the established giants of the genre. It was selected as one of the best science fiction short stories of the pre-Nebula Award period by the Science Fiction and Fantasy Writers of America, appearing in \"The Science Fiction Hall of Fame Volume One, 1929-1964\". \"The Ballad of Lost C'Mell\" was similarly honored, appearing in \"The Science Fiction Hall of Fame, Volume Two\".\nAfter \"Scanners Live in Vain\", Smith's next story did not appear for several years, but from 1955 until his death in 1966 his stories appeared regularly, for the most part in \"Galaxy Science Fiction\". His universe featured strange and vivid creations, such as:\nPublished fiction.\nShort stories.\nTitles marked with an asterisk * are independent stories not related to the Instrumentality universe."}
{"id": "7110", "revid": "27853216", "url": "https://en.wikipedia.org/wiki?curid=7110", "title": "CSS (disambiguation)", "text": "CSS, or Cascading Style Sheets, is a language used to describe the style of document presentations in web development.\nCSS may also refer to:"}
{"id": "7112", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=7112", "title": "Colorado Front Range", "text": ""}
{"id": "7114", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=7114", "title": "Channel Isles", "text": ""}
{"id": "7118", "revid": "14703151", "url": "https://en.wikipedia.org/wiki?curid=7118", "title": "Churnsike Lodge", "text": "Churnsike Lodge is an early Victorian hunting lodge situated in the parish of Greystead, West Northumberland, England. Constructed in 1850 by the Charlton family, descendants of the noted Border Reivers family of the English Middle March, the lodge formed part of the extensive Hesleyside estate, located some 10 miles from Hesleyside Hall itself.\nConsisting of the main house, stable block, hunting-dog kennels and gamekeepers bothy, when the property was acquired by the Chesters Estate in 1887 the 'Cairnsyke' estate consisted of several thousand acres of moorland, much of which was managed to support shooting of the formerly populous black grouse. Although much of this land has now reverted to fellside or has been otherwise managed as part of the commercial timber plantations of Kielder Forest, areas of heather moorland persist, dotted with remnants of the shooting butts. It is with reference to these fells that the 1887 sale catalogue described the estate as being the \"Finest grouse moor in the Kingdom\".\nHistorically, the Lodge was home to the Irthing Head and Kielder hounds, regionally renowned and headed by the locally famed fox hunter William Dodd. Dodd, and his hounds, are repeatedly referenced in the traditional Northumbrian ballads of James Armstrong's 'Wanny Blossoms'.\nHaving fallen into ruin by the 1980s, the property fell into the care of the Forestry Commission and was slated for demolition, as many properties in the area were, until being privately purchased. The former gamekeepers bothy now serves as a holiday-home."}
{"id": "7119", "revid": "1013010316", "url": "https://en.wikipedia.org/wiki?curid=7119", "title": "William Kidd", "text": "William Kidd, also known as Captain William Kidd or simply Captain Kidd (c. 1655 \u2013 23 May 1701), was a Scottish sailor who was tried and executed for piracy after returning from a voyage to the Indian Ocean. Some modern historians, for example Sir Cornelius Neale Dalton, deem his piratical reputation unjust.\nBiography.\nEarly life and education.\nKidd was born in Dundee, Scotland. Greenock was given as his place of birth (although some say it was Dundee, and others say Belfast), and his age as 41 in testimony under oath at the High Court of the Admiralty in October 1694 or 1695. A local society supported the family financially after the death of the father. The myth that his \"father was thought to have been a Church of Scotland minister\" has been discounted, insofar as there is no mention of the name in comprehensive Church of Scotland records for the period. Others still hold the contrary view.\nEarly voyages.\nKidd later settled in the newly anglicized New York City, where he befriended many prominent colonial citizens, including three governors. Some published information suggests that he was a seaman's apprentice on a pirate ship during this time, before partaking in his more famous seagoing exploits.\nBy 1689, Kidd was a member of a French\u2013English pirate crew sailing the Caribbean under Captain Jean Fantin. During one of their voyages, Kidd and other crew members mutinied, ousting the captain and sailing to the British colony of Nevis. There they renamed the ship \"Blessed William\", and Kidd became captain either as a result of election by the ship's crew, or by appointment of Christopher Codrington, governor of the island of Nevis. Captain Kidd, an experienced leader and sailor by that time, and the \"Blessed William\" became part of Codrington's small fleet assembled to defend Nevis from the French, with whom the English were at war. The governor did not pay the sailors for their defensive services, telling them instead to take their pay from the French. Kidd and his men attacked the French island of Marie-Galante, destroying its only town and looting the area, and gathering for themselves around 2,000 pounds sterling.\nLater, during the War of the Grand Alliance, on commissions from the provinces of New York and Massachusetts Bay, Kidd captured an enemy privateer off the New England coast. Shortly afterwards, he was awarded \u00a3150 for successful privateering in the Caribbean, and one year later, Captain Robert Culliford, a notorious pirate, stole Kidd's ship while he was ashore at Antigua in the West Indies. In 1695, William III of England appointed Richard Coote, 1st Earl of Bellomont, governor in place of the corrupt Benjamin Fletcher, who was known for accepting bribes to allow illegal trading of pirate loot. In New York City, Kidd was active in the building of Trinity Church, New York.\nOn 16 May 1691, Kidd married Sarah Bradley Cox Oort, an English woman in her early twenties, who had already been twice widowed and was one of the wealthiest women in New York, largely because of her inheritance from her first husband.\nPreparing his expedition.\nOn 11 December 1695, Bellomont was governing New York, Massachusetts, and New Hampshire, and he asked the \"trusty and well beloved Captain Kidd\" to attack Thomas Tew, John Ireland, Thomas Wake, William Maze, and all others who associated themselves with pirates, along with any enemy French ships. It would have been viewed as disloyalty to the crown to turn down this request, carrying much social stigma, making it difficult for Kidd to say no. The request preceded the voyage which established Kidd's reputation as a pirate and marked his image in history and folklore.\nFour-fifths of the cost for the venture was paid for by noble lords, who were among the most powerful men in England: the Earl of Orford, the Baron of Romney, the Duke of Shrewsbury, and Sir John Somers. Kidd was presented with a letter of marque, signed personally by King William III of England. This letter reserved 10% of the loot for the Crown, and Henry Gilbert's \"The Book of Pirates\" suggests that the King may have fronted some of the money for the voyage himself. Kidd and his acquaintance Colonel Robert Livingston orchestrated the whole plan; they sought additional funding from a merchant named Sir Richard Blackham. Kidd also had to sell his ship \"Antigua\" to raise funds.\nThe new ship, \"Adventure Galley\", was well suited to the task of catching pirates, weighing over 284 tons burthen and equipped with 34 cannon, oars, and 150 men. The oars were a key advantage, as they enabled \"Adventure Galley\" to manoeuvre in a battle when the winds had calmed and other ships were dead in the water. Kidd took pride in personally selecting the crew, choosing only those whom he deemed to be the best and most loyal officers.\nBecause of Kidd's refusal to salute, the Navy vessel's captain retaliated by pressing much of Kidd's crew into naval service, despite rampant protests. Thus short-handed, Kidd sailed for New York City, capturing a French vessel en route (which was legal under the terms of his commission). To make up for the lack of officers, Kidd picked up replacement crew in New York, the vast majority of whom were known and hardened criminals, some undoubtedly former pirates.\nAmong Kidd's officers was his quartermaster Hendrick van der Heul. The quartermaster was considered \"second in command\" to the captain in pirate culture of this era. It is not clear, however, if Van der Heul exercised this degree of responsibility because Kidd was nominally a privateer. Van der Heul is also noteworthy because he may have been African or of African descent. A contemporary source describes him as a \"small black Man\". If Van der Heul was indeed of African ancestry, this fact would make him the highest-ranking black pirate so far identified. Van der Heul went on to become a master's mate on a merchant vessel and was never convicted of piracy.\nHunting for pirates.\nIn September 1696, Kidd weighed anchor and set course for the Cape of Good Hope. A third of his crew died on the Comoros due to an outbreak of cholera, the brand-new ship developed many leaks, and he failed to find the pirates whom he expected to encounter off Madagascar.\nAs it became obvious that his ambitious enterprise was failing, Kidd became desperate to cover its costs. But, once again, he failed to attack several ships when given a chance, including a Dutchman and a New York privateer. Some of the crew deserted Kidd the next time that \"Adventure Galley\" anchored offshore, and those who decided to stay on made constant open threats of mutiny.\nKidd killed one of his own crewmen on 30 October 1697. Kidd's gunner William Moore was on deck sharpening a chisel when a Dutch ship appeared. Moore urged Kidd to attack the Dutchman, an act not only piratical but also certain to anger Dutch-born King William. Kidd refused, calling Moore a lousy dog. Moore retorted, \"If I am a lousy dog, you have made me so; you have brought me to ruin and many more.\" Kidd snatched up and heaved an ironbound bucket at Moore. Moore fell to the deck with a fractured skull and died the following day.\nSeventeenth-century English admiralty law allowed captains great leeway in using violence against their crew, but outright murder was not permitted. Yet Kidd seemed unconcerned, later explaining to his surgeon that he had \"good friends in England, that will bring me off for that\".\nAccusations of piracy.\nActs of savagery on Kidd's part were reported by escaped prisoners, who told stories of being hoisted up by the arms and \"drubbed\" (thrashed) with a drawn cutlass. On one occasion, crew members ransacked the trading ship \"Mary\" and tortured several of its crew members while Kidd and the other captain, Thomas Parker, conversed privately in Kidd's cabin. When Kidd found out what had happened, he was outraged and forced his men to return most of the stolen property.\nKidd was declared a pirate very early in his voyage by a Royal Navy officer, to whom he had promised \"thirty men or so\". Kidd sailed away during the night to preserve his crew, rather than subject them to Royal Navy impressment.\nOn 30 January 1698, Kidd raised French colours and took his greatest prize, the 400-ton \"Quedagh Merchant\", an Indian ship hired by Armenian merchants that was loaded with satins, muslins, gold, silver, an incredible variety of East Indian merchandise, as well as extremely valuable silks. The captain of \"Quedagh Merchant\" was an Englishman named Wright, who had purchased passes from the French East India Company promising him the protection of the French Crown. After realising the captain of the taken vessel was an Englishman, Kidd tried to persuade his crew to return the ship to its owners, but they refused, claiming that their prey was perfectly legal, as Kidd was commissioned to take French ships, and that an Armenian ship counted as French if it had French passes. In an attempt to maintain his tenuous control over his crew, Kidd relented and kept the prize. When this news reached England, it confirmed Kidd's reputation as a pirate, and various naval commanders were ordered to \"pursue and seize the said Kidd and his accomplices\" for the \"notorious piracies\" they had committed.\nKidd kept the French sea passes of the \"Quedagh Merchant\", as well as the vessel itself. While the passes were at best a dubious defence of his capture, British admiralty and vice-admiralty courts (especially in North America) heretofore had often winked at privateers' excesses into piracy, and Kidd may have been hoping that the passes would provide the legal fig leaf that would allow him to keep \"Quedagh Merchant\" and her cargo. Renaming the seized merchantman \"Adventure Prize\", he set sail for Madagascar.\nOn 1 April 1698, Kidd reached Madagascar. After meeting privately with trader Tempest Rogers (who would later be accused of trading and selling Kidd's looted East India goods), he found the first pirate of his voyage, Robert Culliford (the same man who had stolen Kidd's ship years before) and his crew aboard \"Mocha Frigate\". Two contradictory accounts exist of how Kidd reacted to his encounter with Culliford. According to \"The General History of the Pirates\", published more than 25 years after the event by an author whose identity remains in dispute, Kidd made peaceful overtures to Culliford: he \"drank their Captain's health\", swearing that \"he was in every respect their Brother\", and gave Culliford \"a Present of an Anchor and some Guns\". This account appears to be based on the testimony of Kidd's crewmen Joseph Palmer and Robert Bradinham at his trial. The other version was presented by Richard Zacks in his 2002 book \"The Pirate Hunter: The True Story of Captain Kidd\". According to Zacks, Kidd was unaware that Culliford had only about 20 crew with him, and felt ill-manned and ill-equipped to take \"Mocha Frigate\" until his two prize ships and crews arrived, so he decided not to molest Culliford until these reinforcements came. After \"Adventure Prize\" and \"Rouparelle\" came in, Kidd ordered his crew to attack Culliford's \"Mocha Frigate\". However, his crew, despite their previous eagerness to seize any available prize, refused to attack Culliford and threatened instead to shoot Kidd. Zacks does not refer to any source for his version of events.\nBoth accounts agree that most of Kidd's men now abandoned him for Culliford. Only 13 remained with \"Adventure Galley\". Deciding to return home, Kidd left the \"Adventure Galley\" behind, ordering her to be burnt because she had become worm-eaten and leaky. Before burning the ship, he was able to salvage every last scrap of metal, such as hinges. With the loyal remnant of his crew, he returned to the Caribbean aboard the \"Adventure Prize\". Some of his crew later returned to America on their own as passengers aboard Giles Shelley's ship \"Nassau\".\nTrial and execution.\nPrior to returning to New York City, Kidd knew that he was a wanted pirate and that several English men-of-war were searching for him. Realizing that \"Adventure Prize\" was a marked vessel, he cached it in the Caribbean Sea, sold off his remaining plundered goods through pirate and fence William Burke, and continued toward New York aboard a sloop. He deposited some of his treasure on Gardiners Island, hoping to use his knowledge of its location as a bargaining tool. Kidd found himself in Oyster Bay, as a way of avoiding his mutinous crew who gathered in New York. In order to avoid them, Kidd sailed around the eastern tip of Long Island, and then doubled back along the Sound to Oyster Bay. He felt this was a safer passage than the highly trafficked Narrows between Staten Island and Brooklyn.\nBellomont (an investor) was away in Boston, Massachusetts. Aware of the accusations against Kidd, Bellomont was justifiably afraid of being implicated in piracy himself and knew that presenting Kidd to England in chains was his best chance to save himself. He lured Kidd into Boston with false promises of clemency, then ordered him arrested on 6 July 1699. Kidd was placed in Stone Prison, spending most of the time in solitary confinement. His wife, Sarah, was also imprisoned. The conditions of Kidd's imprisonment were extremely harsh, and appear to have driven him at least temporarily insane. By then, Bellomont had turned against Kidd and other pirates, writing that the inhabitants of Long Island were \"a lawless and unruly people\" protecting pirates who had \"settled among them\".\nAfter over a year, Kidd was sent to England for questioning by the Parliament of England. The new Tory ministry hoped to use Kidd as a tool to discredit the Whigs who had backed him, but Kidd refused to name names, naively confident his patrons would reward his loyalty by interceding on his behalf. There is speculation that he probably would have been spared had he talked. Finding Kidd politically useless, the Tory leaders sent him to stand trial before the High Court of Admiralty in London, for the charges of piracy on high seas and the murder of William Moore. Whilst awaiting trial, Kidd was confined in the infamous Newgate Prison, and wrote several letters to King William requesting clemency.\nKidd had two lawyers to assist in his defence. He was shocked to learn at his trial that he was charged with murder. He was found guilty on all charges (murder and five counts of piracy) and sentenced to death. He was hanged in a public execution on 23 May 1701, at Execution Dock, Wapping, in London. He was hanged twice. On the first attempt, the hangman's rope broke and Kidd survived. Although some in the crowd called for Kidd's release, claiming the breaking of the rope was a sign from God, Kidd was hanged again minutes later, this time successfully. His body was gibbeted over the River Thames at Tilbury Point \u2013 as a warning to future would-be pirates \u2013 for three years.\nKidd's associates Richard Barleycorn, Robert Lamley, William Jenkins, Gabriel Loffe, Able Owens, and Hugh Parrot were also convicted, but pardoned just prior to hanging at Execution Dock.\nKidd's Whig backers were embarrassed by his trial. Far from rewarding his loyalty, they participated in the effort to convict him by depriving him of the money and information which might have provided him with some legal defence. In particular, the two sets of French passes he had kept were missing at his trial. These passes (and others dated 1700) resurfaced in the early twentieth century, misfiled with other government papers in a London building. These passes call the extent of Kidd's guilt into question. Along with the papers, many goods were brought from the ships and soon auctioned off as \"pirate plunder\". They were never mentioned in the trial.\nAs to the accusations of murdering Moore, on this he was mostly sunk on the testimony of the two former crew members, Palmer and Bradinham, who testified against him in exchange for pardons. A deposition Palmer gave, when he was captured in Rhode Island two years earlier, contradicted his testimony and may have supported Kidd's assertions, but Kidd was unable to obtain the deposition.\nA broadside song, \"Captain Kidd's Farewell to the Seas, or, the Famous Pirate's Lament\", was printed shortly after his execution and popularised the common belief that Kidd had confessed to the charges.\nMythology and legend.\nThe belief that Kidd had left buried treasure contributed considerably to the growth of his legend. The 1701 broadside song \"Captain Kid's Farewell to the Seas, or, the Famous Pirate's Lament\" lists \"Two hundred bars of gold, and rix dollars manifold, we seized uncontrolled\". This belief made its contributions to literature in Edgar Allan Poe's \"The Gold-Bug\"; Washington Irving's \"The Devil and Tom Walker\"; Robert Louis Stevenson's \"Treasure Island\" and Nelson DeMille's \"Plum Island\". It also gave impetus to the constant treasure hunts conducted on Oak Island in Nova Scotia; in Suffolk County, Long Island in New York where Gardiner's Island is located; Charles Island in Milford, Connecticut; the Thimble Islands in Connecticut; Cockenoe Island in Westport, Connecticut; and on the island of Grand Manan in the Bay of Fundy.\nCaptain Kidd did bury a small cache of treasure on Gardiners Island in a spot known as Cherry Tree Field; however, it was removed by Governor Bellomont and sent to England to be used as evidence against Kidd.\nSome time during the 1690s Kidd visited Block Island, where he was supplied by Mrs. Mercy (Sands) Raymond, daughter of the mariner James Sands. The story has it that, for her hospitality, Mrs. Raymond was bid to hold out her apron, into which Kidd threw gold and jewels until it was full. After her husband Joshua Raymond died, Mercy moved with her family to northern New London, Connecticut (later Montville), where she bought much land. The Raymond family was thus said to have been \"enriched by the apron\".\nOn Grand Manan in the Bay of Fundy, as early as 1875, reference was made to searches on the west side of the island for treasure allegedly buried by Kidd during his time as a privateer. For nearly 200 years, this remote area of the island has been called \"Money Cove\".\nIn 1983, Cork Graham and Richard Knight went looking for Captain Kidd's buried treasure off the Vietnamese island of Ph\u00fa Qu\u1ed1c. Knight and Graham were caught, convicted of illegally landing on Vietnamese territory, and assessed each a $10,000 fine. They were imprisoned for 11 months until they paid the fine.\n\"Quedagh Merchant\" found.\nFor years, people and treasure hunters have tried to locate \"Quedagh Merchant\". It was reported on 13 December 2007 that \"wreckage of a pirate ship abandoned by Captain Kidd in the 17th century has been found by divers in shallow waters off the Dominican Republic.\" The waters in which the ship was found were less than ten feet deep and were only off Catalina Island, just to the south of La Romana on the Dominican coast. The ship is believed to be \"the remains of \"Quedagh Merchant\"\". Charles Beeker, the director of Academic Diving and Underwater Science Programs in Indiana University (Bloomington)'s School of Health, Physical Education, and Recreation, was one of the experts leading the Indiana University diving team. He said that it was \"remarkable that the wreck has remained undiscovered all these years given its location,\" and given that the ship has been the subject of so many prior failed searches. Captain Kidd's cannon, an artifact from the shipwreck, was added to a permanent exhibit at The Children's Museum of Indianapolis in 2011.\nFalse find.\nIn May 2015, a ingot expected to be silver was found in a wreck off the coast of \u00cele Sainte-Marie in Madagascar by a team led by marine archaeologist Barry Clifford, and was believed to be part of Captain Kidd's treasure. Clifford handed the booty to Hery Rajaonarimampianina, President of Madagascar. However, in July 2015, a UNESCO scientific and technical advisory body revealed that the ingot consisted of 95% lead, and speculated that the wreck in question might be a broken part of the Sainte-Marie port constructions.\nPortrayals in popular culture.\nEdgar Allan Poe uses the legend of Kidd's buried treasure in his seminal detective story The Gold Bug.\nCharles Laughton played Kidd twice on film, first in 1945 in \"Captain Kidd\", then in 1952 in \"Abbott and Costello Meet Captain Kidd\".\nKidd is mentioned in the 1945 song \"Captain Kidd\" by singer Ella Mae Morse with Billy May and his orchestra.\nKidd is also name-checked in the song The Land of Make Believe by Bucks Fizz, which was a number-one hit in the United Kingdom, the Netherlands, Belgium and Ireland in 1982. The lyric is, \"Captain Kidd's on the sand, With the treasure close at hand.\"\nJohn Crawford played Kidd in the 1953 Columbia film serial \"The Great Adventures of Captain Kidd\".\nThe most recent film portrayal was by Love Nystrom in the 2006 mini-series \"Blackbeard\".\nCaptain Kidd is mentioned in Bob Dylan's song Bob Dylan's 115th Dream from his fifth album Bringing It All Back Home.\nCaptain Kidd appears in \"Persona 5\", as one of the eponymous Personas belonging to Ryuji Sakamoto. The game's theme is about outcasts/outlaws, as well as rebellion against the established order/society, and portrays several individuals (mostly from fiction) that fit this theme, such as Ars\u00e8ne Lupin, Pope Joan and Ishikawa Goemon.\nCaptain Kidd appears in \"The Devil and Daniel Webster\".\nThe 1957 children's book \"Captain Kidd's Cat\" by Robert Lawson is a largely fictionalized account of Kidd's last voyage, trial and execution, told from the point of view of his loyal ship's cat. The book portrays Kidd as an innocent privateer who was framed by corrupt officials as a scapegoat for their own crimes.\nThe song \"Ballad of William Kidd\" by the heavy metal band Running Wild is based on Kidd's life, particularly the events surrounding his trial and execution.\nA version of \"Ballad of William Kidd\" is sung by Commander Klaes Ashford in Seasons 3 and 4 of \"The Expanse\"\n\"The Ballad of William Kidd\" is performed by Blackbeard's Tea Party, making reference to his life throughout the song.\n\"\" portrays a highly fictionalized version of Captain Kidd as a pirate in the Pacific with more of an American accent than a Scottish one who comes across the Doraemon and his friends that were caught in a time distortion. The combined group finds an island with secret treasure and is being used by a business from the future for his own benefit. Eventually, the businessman is captured by the Time Patrol and Kidd continues his voyage.\nThe character of Ogin in the anime Girls und Panzer models herself heavily on Kidd, particularly with regard to her personality and leadership style. She is the commander of a British Mark IV tank in \"Das Finale\".\nThe band Scissorfight had a song called \"The Gibbeted Captain Kidd\" on their 1998 album \"Balls Deep\".\nIn the manga and anime series \"One Piece\", there is a character named Eustass \"Captain\" Kid, whose name and epithet are inspired by the real-life Captain Kidd.\nFour missions in Assassin's Creed III involve finding map pieces that Captain Kidd had given to four of his crew members for safekeeping. Finding all four pieces would reveal the location of Kidd's treasure, which in the Assassin's Creed series is a Piece of Eden that gives the player the ability to repel bullets.\nCaptain Kidd inspired the creation of a pirate character by the same name in various video games published by SNK. The character makes appearances in the games World Heroes 2, World Heroes 2 Jet, World Heroes Perfect, and ."}
{"id": "7120", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=7120", "title": "Calreticulin", "text": "Calreticulin also known as calregulin, CRP55, CaBP3, calsequestrin-like protein, and endoplasmic reticulum resident protein 60 (ERp60) is a protein that in humans is encoded by the \"CALR\" gene.\nCalreticulin is a multifunctional soluble protein that binds Ca2+ ions (a second messenger in signal transduction), rendering it inactive. The Ca2+ is bound with low affinity, but high capacity, and can be released on a signal (see inositol trisphosphate). Calreticulin is located in storage compartments associated with the endoplasmic reticulum and is considered an ER resident protein.\nThe term \"Mobilferrin\" is considered to be the same as calreticulin by some sources.\nFunction.\nCalreticulin binds to misfolded proteins and prevents them from being exported from the endoplasmic reticulum to the Golgi apparatus.\nA similar quality-control molecular chaperone, calnexin, performs the same service for soluble proteins as does calreticulin, however it is a membrane-bound protein. Both proteins, calnexin and calreticulin, have the function of binding to oligosaccharides containing terminal glucose residues, thereby targeting them for degradation. Calreticulin and Calnexin's ability to bind carbohydrates associates them with the lectin protein family. In normal cellular function, trimming of glucose residues off the core oligosaccharide added during N-linked glycosylation is a part of protein processing. If \"overseer\" enzymes note that residues are misfolded, proteins within the rER will re-add glucose residues so that other calreticulin/calnexin can bind to these proteins and prevent them from proceeding to the Golgi. This leads these aberrantly folded proteins down a path whereby they are targeted for degradation.\nStudies on transgenic mice reveal that calreticulin is a cardiac embryonic gene that is essential during development.\nCalreticulin and calnexin are also integral proteins in the production of MHC class I Proteins. As newly synthesized MHC class I \u03b1-chains enter the endoplasmic reticulum, calnexin binds on to them retaining them in a partly folded state. After the \u03b22-microglobulin binds to the peptide-loading complex (PLC), calreticulin (along with ERp57) takes over the job of chaperoning the MHC class I protein while the tapasin links the complex to the transporter associated with antigen processing (TAP) complex. This association prepares the MHC class I for binding an antigen for presentation on the cell surface.\nTranscription regulation.\nCalreticulin is also found in the nucleus, suggesting that it may have a role in transcription regulation. Calreticulin binds to the synthetic peptide KLGFFKR, which is almost identical to an amino acid sequence in the DNA-binding domain of the superfamily of nuclear receptors. The amino terminus of calreticulin interacts with the DNA-binding domain of the glucocorticoid receptor and prevents the receptor from binding to its specific glucocorticoid response element. Calreticulin can inhibit the binding of androgen receptor to its hormone-responsive DNA element and can inhibit androgen receptor and retinoic acid receptor transcriptional activities in vivo, as well as retinoic acid-induced neuronal differentiation. Thus, calreticulin can act as an important modulator of the regulation of gene transcription by nuclear hormone receptors.\nClinical significance.\nCalreticulin binds to antibodies in certain area of systemic lupus and Sj\u00f6gren patients that contain anti-Ro/SSA antibodies. Systemic lupus erythematosus is associated with increased autoantibody titers against calreticulin, but calreticulin is not a Ro/SS-A antigen. Earlier papers referred to calreticulin as an Ro/SS-A antigen, but this was later disproven. Increased autoantibody titer against human calreticulin is found in infants with complete congenital heart block of both the IgG and IgM classes.\nIn 2013, two groups detected calreticulin mutations in a majority of JAK2-negative/MPL-negative patients with essential thrombocythemia and primary myelofibrosis, which makes \"CALR\" mutations the second most common in myeloproliferative neoplasms. All mutations (insertions or deletions) affected the last exon, generating a reading frame shift of the resulting protein, that creates a novel terminal peptide and causes a loss of endoplasmic reticulum KDEL retention signal.\nRole in cancer.\nCalreticulin (CRT) is expressed in many cancer cells and plays a role to promote macrophages to engulf hazardous cancerous cells. The reason why most of the cells are not destroyed is the presence of another molecule with signal CD47, which blocks CRT. Hence antibodies that block CD47 might be useful as a cancer treatment. In mice models of myeloid leukemia and non-Hodgkin lymphoma, anti-CD47 were effective in clearing cancer cells while normal cells were unaffected.\nInteractions.\nCalreticulin has been shown to interact with Perforin and NK2 homeobox 1."}
{"id": "7122", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=7122", "title": "Crannog", "text": "A crannog (; ; ) is typically a partially or entirely artificial island, usually built in lakes and estuarine waters of Scotland, Wales, and Ireland. Unlike the prehistoric pile dwellings around the Alps, which were built on the shores and not inundated until later, crannogs were built in the water, thus forming artificial islands.\nCrannogs were used as dwellings over five millennia, from the European Neolithic Period to as late as the 17th/early 18th century. In Scotland there is no convincing evidence in the archaeological record of Early and Middle Bronze Age or Norse Period use. The radiocarbon dating obtained from key sites such as Oakbank and Redcastle indicates at a 95.4 per cent confidence level that they date to the Late Bronze Age to Early Iron Age. The date ranges fall \"after\" around 800 BC and so could be considered Late Bronze Age by only the narrowest of margins. \nCrannogs have been variously interpreted as free-standing wooden structures, as at Loch Tay, although more commonly they are composed of brush, stone or timber mounds that can be revetted with timber piles. However, in areas such as the Outer Hebrides of Scotland, timber was unavailable from the Neolithic era onwards. As a result, crannogs made completely of stone and supporting drystone architecture are common there. Today, crannogs typically appear as small, circular islets, often in diameter, covered in dense vegetation due to their inaccessibility to grazing livestock.\nEtymology and uncertain meanings.\nThe Irish word derives from Old Irish , which referred to a wooden structure or vessel, stemming from \"crann\", which means \"tree\", suffixed with \"-\u00f3g\" which is a diminutive ending ultimately borrowed from Welsh. The suffix \"-\u00f3g\" is sometimes misunderstood by non-native Irish-speakers as \"\u00f3g\", which is a separate word that means \"young\". This misunderstanding leads to a folk etymology whereby \"crann\u00f3g\" is misanalysed as \"crann \u00f3g\", which is pronounced differently and means \"a young tree\". The modern sense of the term first appears sometime around the 12th century; its popularity spread in the medieval period along with the terms \"isle\", \"ylle\", \"inis\", \"eilean\" or \"oile\u00e1n\". There is some confusion on what the term \"crannog\" originally referred to, as the structure atop the island or the island itself. The additional meanings of Irish can be variously related as 'structure/piece of wood', including 'crow's nest', 'pulpit', or 'driver's box on a coach'; 'vessel/box/chest' more generally; and 'wooden pin'. The Scottish Gaelic form is and has the additional meanings of 'pulpit' and 'churn'. Thus, there is no real consensus on what the term \"crannog\" actually implies, although the modern adoption in the English language broadly refers to a partially or completely artificial islet that saw use from the prehistoric to the Post-Medieval period in Ireland and Scotland.\nLocation.\nCrannogs are widespread in Ireland, with an estimated 1,200 examples, while Scotland has 347 sites officially listed as such. The actual number in Scotland varies considerably depending on definition\u2014between about 350 to 500, due to the use of the term \"island dun\" for well over one hundred Hebridean examples\u2014a distinction that has created a divide between mainland Scottish crannog and Hebridean islet settlement studies. Previously unknown crannogs in Scotland and Ireland are still being found as underwater surveys continue to investigate loch beds for completely submerged examples. The largest concentrations of crannogs in Ireland are found in the Drumlin Belt of the Midlands, North and Northwest. In Scotland, crannogs are mostly found on the western coast, with high concentrations in Argyll and Dumfries and Galloway. In reality, the Western Isles contain the highest density of lake-settlements in Scotland, yet they are recognised under varying terms besides \"crannog\". One lone Welsh example at Llangorse Lake exists, likely a product of Irish influence across the Irish Sea.\nReconstructed Irish crann\u00f3gs are located in Craggaunowen, County Clare, Ireland; the Irish National Heritage Park, in Wexford, Ireland; and in Scotland at the \"Scottish Crannog Centre\" at Loch Tay, Perthshire. This centre offers guided tours and hands-on activities, including wool-spinning, wood-turning and making fire, holds events to celebrate wild cooking and crafts, and hosts yearly Midsummer, Lughnasadh and Samhain festivals.\nTypes and problems with definition.\nCrannogs took on many different forms and methods of construction based on what was available in the immediate landscape. The classic image of a prehistoric crannog stems from both post-medieval illustrations and highly influential excavations, such as Milton Loch in Scotland by C. M. Piggot after World War II. The Milton Loch interpretation is of a small islet surrounded or defined at its edges by timber piles and a gangway, topped by a typical Iron Age roundhouse. The choice of a small islet as a home may seem odd today, yet waterways were the main channels for both communication and travel until the 19th century in much of Ireland and, especially, Highland Scotland. Crannogs are traditionally interpreted as simple prehistorical farmsteads. They are also interpreted as boltholes in times of danger, as status symbols with limited access, and as inherited locations of power that imply a sense of legitimacy and ancestry towards ownership of the surrounding landscape.\nA strict definition of a crannog, which has long been debated, requires the use of timber. Sites in the Western Isles do not satisfy this criterion, although their inhabitants shared the common habit of living on water. If not classed as \"true\" crannogs, small occupied islets (often at least partially artificial in nature) may be referred to as \"island duns., But, rather confusingly, 22 islet-based sites are classified as \"proper\" crannogs due to the different interpretations of the inspectors or excavators who drew up field reports. Hebridean island dwellings or crannogs were commonly built on both natural and artificial islets, usually reached by a stone causeway. The visible structural remains are traditionally interpreted as duns or, in more recent terminology, as \"Atlantic roundhouses\". This terminology has recently become popular when describing the entire range of robust, drystone structures that existed in later prehistoric Atlantic Scotland.\nThe majority of crannog excavations were poorly conducted (by modern standards) in the late 19th and early 20th centuries by early antiquarians, or were purely accidental finds as lochs were drained during the improvements to increase usable farmland or pasture. In some early digs, labourers hauled away tons of materials, with little regard to anything that was not of immediate economic value. Conversely, the vast majority of early attempts at proper excavation failed to accurately measure or record stratigraphy, thereby failing to provide a secure context for artefact finds. Thus only extremely limited interpretations are possible. Preservation and conservation techniques for waterlogged materials such as logboats or structural material were all but non-existent, and a number of extremely important finds were destroyed as a result: in some instances dried out for firewood. \nFrom about 1900 to the late 1940s there was very little crannog excavation in Scotland, while some important and highly influential contributions were made in Ireland. In contrast, relatively few crannogs have been excavated since the Second World War. But this number has steadily grown, especially since the early 1980s, and may soon surpass pre-war totals. The overwhelming majority of crannogs show multiple phases of occupation and re-use, often extending over centuries. Thus the re-occupiers may have viewed crannogs as a legacy that was alive in local tradition and memory. Crannog reoccupation is important and significant, especially in the many instances of crannogs built near natural islets, which were often completely unused. This long chronology of use has been verified by both radiocarbon dating and more precisely by dendrochronology. \nInterpretations of crannog function have not been static; instead they appear to have changed in both the archaeological and historic records. Rather than the simple domestic residences of prehistory, the medieval crannogs were increasingly seen as strongholds of the upper class or regional political players, such as the Gaelic chieftains of the O'Boylans and McMahons in County Monaghan and the Kingdom of Airg\u00edalla, until the 17th century. In Scotland, the medieval and post-medieval use of crannogs is also documented into the early 18th century. Whether this increase in status is real, or just a by-product of increasingly complex material assemblages, remains to be convincingly validated.\nHistory.\nThe earliest-known constructed crannog is the completely artificial Neolithic islet of Eilean D\u00f2mhnuill, Loch Olabhat on North Uist in Scotland. Eilean Domhnuill has produced radiocarbon dates ranging from 3650 to 2500 BC. Irish crannogs appear in middle Bronze Age layers at Ballinderry (1200\u2013600 BC). Recent radiocarbon dating of worked timber found in Loch Bhorghastail on the Isle of Lewis has produced evidence of crannogs as old as 3380-3630 BC. Prior to the Bronze Age, the existence of artificial island settlement in Ireland is not as clear. While lakeside settlements are evident in Ireland from 4500 BC, these settlements are not crannogs, as they were not intended to be islands. Despite having a lengthy chronology, their use was not at all consistent or unchanging. \nCrannog construction and occupation was at its peak in Scotland from about 800 BC to AD 200. Not surprisingly, crannogs have useful defensive properties, although there appears to be more significance to prehistoric use than simple defense, as very few weapons or evidence for destruction appear in excavations of prehistoric crannogs. In Ireland, crannogs were at their zenith during the Early Historic period, when they were the homes and retreats of kings, lords, prosperous farmers and, occasionally, socially marginalised groups, such as monastic hermits or metalsmiths who could work in isolation. Despite scholarly concepts supporting a strict Early Historic evolution, Irish excavations are increasingly uncovering examples that date from the \"missing\" Iron Age in Ireland.\nConstruction.\nThe construction techniques for a crannog (prehistoric or otherwise) are as varied as the multitude of finished forms that make up the archaeological record. Island settlement in Scotland and Ireland is manifest through the entire range of possibilities ranging from entirely natural, small islets to completely artificial islets, therefore definitions remain contentious. For crannogs in the strict sense, typically the construction effort began on a shallow reef or rise in the lochbed.\nWhen timber was available, many crannogs were surrounded by a circle of wooden piles, with axe-sharpened bases that were driven into the bottom, forming a circular enclosure that helped to retain the main mound and prevent erosion. The piles could also be joined together by mortise and tenon, or large holes cut to carefully accept specially shaped timbers designed to interlock and provide structural rigidity. On other examples, interior surfaces were built up with any mixture of clay, peat, stone, timber or brush \u2013 whatever was available. In some instances, more than one structure was built on crannogs.\nIn other types of crannogs, builders and occupants added large stones to the waterline of small natural islets, extending and enlarging them over successive phases of renewal. Larger crannogs could be occupied by extended families or communal groups, and access was either by logboats or coracles. Evidence for timber or stone causeways exists on a large number of crannogs. The causeways may have been slightly submerged; this has been interpreted as a device to make access difficult but may also be a result of loch level fluctuations over the ensuing centuries or millennia. Organic remains are often found in excellent condition on these water-logged sites. The bones of cattle, deer, and swine have been found in excavated crannogs, while remains of wooden utensils and even dairy products have been completely preserved for several millennia."}
{"id": "7123", "revid": "1009560086", "url": "https://en.wikipedia.org/wiki?curid=7123", "title": "Calendar date", "text": "A calendar date is a reference to a particular day represented within a calendar system. The calendar date allows the specific day to be identified. The number of days between two dates may be calculated. For example, \"24 \" is ten days after \"14 \" in the Gregorian calendar. The date of a particular event depends on the observed time zone. For example, the air attack on Pearl Harbor that began at 7:48\u00a0a.m. Hawaiian time on 7 December 1941 took place at 3:18\u00a0a.m. Japan Standard Time, 8 December in Japan.\nA particular day may be represented by a different date in another calendar as in the Gregorian calendar and the Julian calendar, which have been used simultaneously in different places. In most calendar systems, the date consists of three parts: the \"day of the month\", the \"month\", and the \"year\". There may also be additional parts, such as the \"day of the week\". Years are usually counted from a particular starting point, usually called the epoch, with era referring to the particular period of time (note the different use of the terms in geology).\nThe most widely used epoch is a conventional birthdate of Jesus (which was established by Dionysius Exiguus in the sixth century). A date without the year may also be referred to as a \"date\" or \"calendar date\" (such as \" \" rather than \" \"). As such, it is either shorthand for the current year (commonly seen on social media websites) or it defines the day of an annual event, such as a birthday on 31 May, a holiday on 1 September, or Christmas on 25 December.\nMany computer systems internally store points in time in Unix time format or some other system time format.\nThe date (Unix) command\u2014internally using the C date and time functions\u2014can be used to convert that internal representation of a point in time\nto most of the date representations shown here.\nThe current date in the Gregorian calendar is . If this is not really the current date, then to update it.\nDate format.\nThere is a large variety of formats for dates in use, which differ in the order of date components. These variations use the sample date of 31 May 2006: (e.g. 31/05/2006, 05/31/2006, 2006/05/31), component separators (e.g. 31.05.2006, 31/05/2006, 31-05-2006), whether leading zeros are included (e.g. 31/5/2006 vs. 31/05/2006), whether all four digits of the year are written (e.g., 31.05.2006 vs. 31.05.06), and whether the month is represented in Arabic or Roman numerals or by name (e.g. 31.05.2006, 31.V.2006 vs. 31 May 2006).\nGregorian, day\u2013month\u2013year (DMY).\nThis little-endian sequence is used by a majority of the world and is the preferred form by the United Nations when writing the full date format in official documents. This date format originates from the custom of writing the date as \"the Nth day of [month] in the year of our Lord [year]\" in Western religious and legal documents. The format has shortened over time but the order of the elements has remained constant. The following examples use the date of 9 November 2006. (With the years 2000\u20132009, care must be taken to ensure that two digit years do not intend to be 1900\u20131909 or other similar years.)\nGregorian, year\u2013month\u2013day (YMD).\nIn this format, the most significant data item is written before lesser data items i.e. the year before the month before the day. It is consistent with the big-endianness of the Hindu\u2013Arabic numeral system, which progresses from the highest to the lowest order magnitude. That is, using this format textual orderings and chronological orderings are identical. This form is standard in East Asia, Iran, Lithuania, Hungary, and Sweden; and some other countries to a limited extent.\nExamples for the 9th of November 2003:\nIt is also extended through the universal big-endian format clock time: 9 November 2003, 18h 14m 12s, or 2003/11/9/18:14:12 or (ISO 8601) 2003-11-09T18:14:12.\nGregorian, month\u2013day\u2013year (MDY).\nThis sequence is used primarily in the Philippines and the United States. This date format was commonly used alongside the little-endian form in the United Kingdom until the mid-20th century and can be found in both defunct and modern print media such as the \"London Gazette\" and \"The Times\", respectively. This format was also commonly used by several English-language print media in many former British colonies and also one of two formats commonly used in India during British Raj era until the mid-20th century. In the United States, it is said as of Sunday, November 9, for example, although usage of \"the\" isn't uncommon (e.g. \"Sunday, November the 9th\", and even \"November the 9th, Sunday\", are also possible and readily understood).\nThe modern convention is to avoid using the ordinal (th, st, rd, nd) form of numbers when the day follows the month (July 4 or July 4, 2006). The ordinal was common in the past and is still sometimes used ([the] 4th [of] July or July 4th).\nGregorian, year\u2013day\u2013month (YDM).\nThis date format is used in Kazakhstan, Latvia, Nepal, and Turkmenistan. According to the official rules of documenting dates by governmental authorities, the date format in Kazakh is written as YYYY.DD.MM, e.g. 2006.05.04 or 2006 5 April ().\nStandards.\nThere are several standards that specify date formats:\nUsage overloading.\nMany numerical forms can create confusion when used in international correspondence, particularly when abbreviating the year to its final two digits, with no context.\nFor example, \"07/08/06\" could refer to either 7 August 2006 or July 8, 2006 (or 1906, or the sixth year of any century), or 6 August 2007. In the United States, dates are rarely written in purely numerical forms in formal writing, although they are very common elsewhere; when numerical forms are used, the month appears first. In the United Kingdom, while it is regarded as acceptable albeit less common to write \"month-name day, year\", this order is never used when written numerically. However, as an exception, the American shorthand \"9/11\" is widely understood as referring to the September 11, 2001 terrorist attacks.\nWhen numbers are used to represent months, a significant amount of confusion can arise from the ambiguity of a date order; especially when the numbers representing the day, month, or year are low, it can be impossible to tell which order is being used. This can be clarified by using four digits to represent years, and naming the month; for example, \"Feb\" instead of \"02\". The ISO 8601 date order with four-digit years: YYYY-MM-DD (introduced in ISO 2014), is specifically chosen to be unambiguous. The ISO 8601 standard also has the advantage of being language independent and is therefore useful when there may be no language context and a universal application is desired (expiration dating on export products, for example). Many Internet sites use YYYY-MM-DD, and those using other conventions often use -MMM- for the month to further clarify and avoid ambiguity (2001-MAY-09, 9-MAY-2001, MAY 09 2001, etc.).\nIn addition, the International Organization for Standardization considers its ISO 8601 standard to make sense from a logical perspective. Mixed units, for example, feet and inches, or pounds and ounces, are normally written with the largest unit first, in decreasing order. Numbers are also written in that order, so the digits of 2006 indicate, in order, the millennium, the century within the millennium, the decade within the century, and the year within the decade. The only date order that is consistent with these well-established conventions is year\u2013month\u2013day. A plain text list of dates with this format can be easily sorted by file managers, word processors, spreadsheets, and other software tools with built-in sorting functions. Some database systems use an eight-digit YYYYMMDD representation to handle date values. Naming folders with YYYY-MM-DD at the beginning allows them to be listed in date order when sorting by name \u2013 especially useful for organizing document libraries.\nAn early U.S. Federal Information Processing Standard recommended 2-digit years. This is now widely recognized as extremely problematic, because of the year 2000 problem. Some U.S. government agencies now use ISO 8601 with 4-digit years.\nWhen transitioning from one date notation to another, people often write both styles; for example Old Style and New Style dates in the transition from the Julian to the Gregorian calendar.\nAdvantages for ordering in sequence.\nOne of the advantages of using the ISO 8601 date format is that the lexicographical order (ASCIIbetical) of the representations is equivalent to the chronological order of the dates, assuming that all dates are in the same time zone. Thus dates can be sorted using simple string comparison algorithms, and indeed by any left to right collation. For example:\n 1998-02-28 (28 February 1998) sorts before\n 2006-03-01 (1 March 2006) which sorts before\n 2015-01-30 (30 January 2015)\nThe YYYY-MM-DD layout is the only common format that can provide this. Sorting other date representations involves some parsing of the date strings. This also works when a time in 24-hour format is included after the date, as long as all times are understood to be in the same time zone.\nISO 8601 is used widely where concise, human-readable yet easily computable and unambiguous dates are required, although many applications store dates internally as UNIX time and only convert to ISO 8601 for display. It is worth noting that all modern computer Operating Systems retain date information of files outside of their titles, allowing the user to choose which format they prefer and have them sorted thus, irrespective of the files' names.\nSpecialized usage.\nDay and year only.\nThe U.S. military sometimes uses a system, which they call \"Julian date format\" that indicates the year and the actual day out of the 365 days of the year (and thus a designation of the month would not be needed). For example, \"11 December 1999\" can be written in some contexts as \"1999345\" or \"99345\", for the 345th day of 1999. This system is most often used in US military logistics since it makes the process of calculating estimated shipping and arrival dates easier. For example: say a tank engine takes an estimated 35 days to ship by sea from the US to South Korea. If the engine is sent on 06104 (Friday, 14 April 2006), it should arrive on 06139 (Friday, 19 May). Note that outside of the US military and some US government agencies, including the Internal Revenue Service, this format is usually referred to as \"ordinal date\", rather than \"Julian date\".\nSuch ordinal date formats are also used by many computer programs (especially those for mainframe systems). Using a three-digit Julian day number saves one byte of computer storage over a two-digit month plus two-digit day, for example, \"January 17\" is 017 in Julian versus 0117 in month-day format. OS/390 or its successor, z/OS, display dates in yy.ddd format for most operations.\nUNIX time stores time as a number in seconds since the beginning of the UNIX Epoch (1970-01-01).\nAnother \"ordinal\" date system (\"ordinal\" in the sense of advancing in value by one as the date advances by one day) is in common use in astronomical calculations and referencing and uses the same name as this \"logistics\" system. The continuity of representation of period regardless of the time of year being considered is highly useful to both groups of specialists. The astronomers describe their system as also being a \"Julian date\" system.\nWeek number used.\nCompanies in Europe often use year, week number, and day for planning purposes.\nSo, for example, an event in a project can happen on w43 (week 43) or w43-1 (Monday, week 43) or, if the year needs to be indicated, on w0643 or w643 (the year 2006 week 43; i.e., Monday 23 October-Sunday 29 October 2006).\nThe ISO does present a standard for identifying weeks, but as it does not match up with the Gregorian calendar (the beginning and ending days of a given year do not match up), this standard is somewhat more problematic than the other standards for dates.\nExpressing dates in spoken English.\nIn English-language outside North America (mostly in Anglophone Europe and some countries in Australasia), full dates are written as \"7 December 1941\" (or \"7th December 1941\") and spoken as \"the seventh of December, nineteen forty-one\" (exceedingly common usage of \"the\" and \"of\"), with the occasional usage of \"December 7, 1941\" (\"December the seventh, nineteen forty-one\"). In common with most continental European usage, however, all-numeric dates are invariably ordered dd/mm/yyyy.\nIn Canada and the United States, the usual written form is \"December 7, 1941\", spoken as \"December seventh, nineteen forty-one\" or colloquially \"December the seventh, nineteen forty-one\". Ordinal numerals, however, are not always used when writing and pronouncing dates, and \"December seven, nineteen forty-one\" is also an accepted pronunciation of the date written \"December 7, 1941\". A notable exception to this rule is the Fourth of July (U.S. Independence Day)."}
{"id": "7124", "revid": "8372814", "url": "https://en.wikipedia.org/wiki?curid=7124", "title": "Cist", "text": "A cist ( or ; also kist ;\nfrom , Middle Welsh \"Kist\" or Germanic \"Kiste\") is a small stone-built coffin-like box or ossuary used to hold the bodies of the dead. Examples can be found across Europe and in the Middle East.\nA cist may have been associated with other monuments, perhaps under a cairn or long barrow. Several cists are sometimes found close together within the same cairn or barrow. Often ornaments have been found within an excavated cist, indicating the wealth or prominence of the interred individual.\nThis old word is preserved in the Swedish language, where \"kista\" is the word for a funerary coffin."}
{"id": "7125", "revid": "84951", "url": "https://en.wikipedia.org/wiki?curid=7125", "title": "Center (group theory)", "text": "In abstract algebra, the center of a group, , is the set of elements that commute with every element of . It is denoted , from German \"Zentrum,\" meaning \"center\". In set-builder notation,\nThe center is a normal subgroup, . As a subgroup, it is always characteristic, but is not necessarily fully characteristic. The quotient group, , is isomorphic to the inner automorphism group, .\nA group is abelian if and only if . At the other extreme, a group is said to be centerless if is trivial; i.e., consists only of the identity element.\nThe elements of the center are sometimes called central.\nAs a subgroup.\nThe center of \"G\" is always a subgroup of . In particular:\nFurthermore, the center of is always a normal subgroup of . Since all elements of commute, it is closed under conjugation.\nConjugacy classes and centralizers.\nBy definition, the center is the set of elements for which the conjugacy class of each element is the element itself; i.e., }.\nThe center is also the intersection of all the centralizers of each element of . As centralizers are subgroups, this again shows that the center is a subgroup.\nConjugation.\nConsider the map, , from to the automorphism group of defined by , where is the automorphism of defined by \nThe function, is a group homomorphism, and its kernel is precisely the center of , and its image is called the inner automorphism group of , denoted . By the first isomorphism theorem we get,\nThe cokernel of this map is the group of outer automorphisms, and these form the exact sequence\nHigher centers.\nQuotienting out by the center of a group yields a sequence of groups called the upper central series:\nThe kernel of the map is the th center of (second center, third center, etc.) and is denoted . Concretely, the ()-st center are the terms that commute with all elements up to an element of the th center. Following this definition, one can define the 0th center of a group to be the identity subgroup. This can be continued to transfinite ordinals by transfinite induction; the union of all the higher centers is called the hypercenter.\nThe ascending chain of subgroups\nstabilizes at \"i\" (equivalently, ) if and only if is centerless."}
{"id": "7127", "revid": "1754504", "url": "https://en.wikipedia.org/wiki?curid=7127", "title": "Cut-and-cover", "text": ""}
{"id": "7129", "revid": "82835", "url": "https://en.wikipedia.org/wiki?curid=7129", "title": "Commonwealth of England", "text": "The Commonwealth was the political structure during the period from 1649 to 1660 when England and Wales, later along with Ireland and Scotland, were governed as a republic after the end of the Second English Civil War and the trial and execution of Charles I. The republic's existence was declared through \"An Act declaring England to be a Commonwealth\", adopted by the Rump Parliament on 19 May 1649. Power in the early Commonwealth was vested primarily in the Parliament and a Council of State. During the period, fighting continued, particularly in Ireland and Scotland, between the parliamentary forces and those opposed to them, as part of what is now generally referred to as the Third English Civil War.\nIn 1653, after dissolution of the Rump Parliament, the Army Council adopted the Instrument of Government which made Oliver Cromwell Lord Protector of a united \"Commonwealth of England, Scotland and Ireland\", inaugurating the period now usually known as the Protectorate. After Cromwell's death, and following a brief period of rule under his son, Richard Cromwell, the Protectorate Parliament was dissolved in 1659 and the Rump Parliament recalled, starting a process that led to the restoration of the monarchy in 1660. The term Commonwealth is sometimes used for the whole of 1649 to 1660 \u2013 called by some the Interregnum \u2013 although for other historians, the use of the term is limited to the years prior to Cromwell's formal assumption of power in 1653.\nIn retrospect, the period of republican rule for England was a failure in the short term. During the 11-year period, no stable government was established to rule the English state for longer than a few months at a time. Several administrative structures were tried, and several Parliaments called and seated, but little in the way of meaningful, lasting legislation was passed. The only force keeping it together was the personality of Oliver Cromwell, who exerted control through the military by way of the \"Grandees\", being the Major-Generals and other senior military leaders of the New Model Army. Not only did Cromwell's regime crumble into near anarchy upon his death and the brief administration of his son, but the monarchy he overthrew was restored in 1660, and its first act was officially to erase all traces of any constitutional reforms of the Republican period. Still, the memory of the Parliamentarian cause, dubbed Good Old Cause by the soldiers of the New Model Army, lingered on. It would carry through English politics and eventually result in a constitutional monarchy.\nThe Commonwealth period is better remembered for the military success of Thomas Fairfax, Oliver Cromwell, and the New Model Army. Besides resounding victories in the English Civil War, the reformed Navy under the command of Robert Blake defeated the Dutch in the First Anglo-Dutch War which marked the first step towards England's naval supremacy. In Ireland, the Commonwealth period is remembered for Cromwell's brutal subjugation of the Irish, which continued the policies of the Tudor and Stuart periods.\n1649\u20131653.\nRump Parliament.\nThe Rump was created by Pride's Purge of those members of the Long Parliament who did not support the political position of the Grandees in the New Model Army. Just before and after the execution of King Charles I on 30 January 1649, the Rump passed a number of acts of Parliament creating the legal basis for the republic. With the abolition of the monarchy, Privy Council and the House of Lords, it had unchecked executive and legislative power. The English Council of State, which replaced the Privy Council, took over many of the executive functions of the monarchy. It was selected by the Rump, and most of its members were MPs. However, the Rump depended on the support of the Army with which it had a very uneasy relationship. After the execution of Charles I, the House of Commons abolished the monarchy and the House of Lords. It declared the people of England \"and of all the Dominions and Territories thereunto belonging\" to be henceforth under the governance of a \"Commonwealth\", effectively a republic.\nStructure.\nIn Pride's Purge, all members of parliament (including most of the political Presbyterians) who would not accept the need to bring the King to trial had been removed. Thus the Rump never had more than two hundred members (less than half the number of the Commons in the original Long Parliament). They included: supporters of religious independents who did not want an established church and some of whom had sympathies with the Levellers; Presbyterians who were willing to countenance the trial and execution of the King; and later admissions, such as formerly excluded MPs who were prepared to denounce the Newport Treaty negotiations with the King.\nMost Rumpers were gentry, though there was a higher proportion of lesser gentry and lawyers than in previous parliaments. Less than one-quarter of them were regicides. This left the Rump as basically a conservative body whose vested interests in the existing land ownership and legal systems made it unlikely to want to reform them.\nIssues and achievements.\nFor the first two years of the Commonwealth, the Rump faced economic depression and the risk of invasion from Scotland and Ireland. By 1653 Cromwell and the Army had largely eliminated these threats.\nThere were many disagreements amongst factions of the Rump. Some wanted a republic, but others favoured retaining some type of monarchical government. Most of England's traditional ruling classes regarded the Rump as an illegal government made up of regicides and upstarts. However, they were also aware that the Rump might be all that stood in the way of an outright military dictatorship. High taxes, mainly to pay the Army, were resented by the gentry. Limited reforms were enough to antagonise the ruling class but not enough to satisfy the radicals.\nDespite its unpopularity, the Rump was a link with the old constitution and helped to settle England down and make it secure after the biggest upheaval in its history. By 1653, France and Spain had recognised England's new government.\nReforms.\nThough the Church of England was retained, episcopacy was suppressed and the Act of Uniformity 1558 was repealed in September 1650. Mainly on the insistence of the Army, many independent churches were tolerated, although everyone still had to pay tithes to the established church.\nSome small improvements were made to law and court procedure; for example, all court proceedings were now conducted in English rather than in Law French or Latin. However, there were no widespread reforms of the common law. This would have upset the gentry, who regarded the common law as reinforcing their status and property rights.\nThe Rump passed many restrictive laws to regulate people's moral behaviour, such as closing down theatres and requiring strict observance of Sunday. This antagonised most of the gentry.\nDismissal.\nCromwell, aided by Thomas Harrison, forcibly dismissed the Rump on 20 April 1653, for reasons that are unclear. Theories are that he feared the Rump was trying to perpetuate itself as the government, or that the Rump was preparing for an election which could return an anti-Commonwealth majority. Many former members of the Rump continued to regard themselves as England's only legitimate constitutional authority. The Rump had not agreed to its own dissolution; their legal, constitutional view it was unlawful was based on Charles' concessionary Act prohibiting the dissolution of Parliament without its own consent (on 11 May 1641, leading to the entire Commonwealth being the latter years of the Long Parliament in their majority view).\nBarebone's Parliament, July\u2013December 1653.\nThe dissolution of the Rump was followed by a short period in which Cromwell and the Army ruled alone. Nobody had the constitutional authority to call an election, but Cromwell did not want to impose a military dictatorship. Instead, he ruled through a 'nominated assembly' which he believed would be easy for the Army to control since Army officers did the nominating.\nBarebone's Parliament was opposed by former Rumpers and ridiculed by many gentries as being an assembly of 'inferior' people. However, over 110 of its 140 members were lesser gentry or of higher social status. (An exception was Praise-God Barebone, a Baptist merchant after whom the Assembly got its derogatory nickname.) Many were well educated.\nThe assembly reflected the range of views of the officers who nominated it. The Radicals (approximately 40) included a hard core of Fifth Monarchists who wanted to be rid of Common Law and any state control of religion. The Moderates (approximately 60) wanted some improvements within the existing system and might move to either the radical or conservative side depending on the issue. The Conservatives (approximately 40) wanted to keep the status quo (since Common Law protected the interests of the gentry, and tithes and advowsons were valuable property).\nCromwell saw Barebone's Parliament as a temporary legislative body which he hoped would produce reforms and develop a constitution for the Commonwealth. However, members were divided over key issues, only 25 had previous parliamentary experience, and although many had some legal training, there were no qualified lawyers.\nCromwell seems to have expected this group of 'amateurs' to produce reform without management or direction. When the radicals mustered enough support to defeat a bill which would have preserved the status quo in religion, the conservatives, together with many moderates, surrendered their authority back to Cromwell who sent soldiers to clear the rest of the Assembly. Barebone's Parliament was over.\nThe Protectorate, 1653\u20131659.\nThroughout 1653, Cromwell and the Army slowly dismantled the machinery of the Commonwealth state. The English Council of State, which had assumed the executive function formerly held by the King and his Privy Council, was forcibly dissolved by Cromwell on 20 April, and in its place a new council, filled with Cromwell's own chosen men, was installed. Three days after Barebone's Parliament dissolved itself, the Instrument of Government was adopted by Cromwell's council and a new state structure, now known historically as The Protectorate, was given its shape. This new constitution granted Cromwell sweeping powers as Lord Protector, an office which ironically had much the same role and powers as the King had under the monarchy, a fact not lost on Cromwell's critics.\nOn 12 April 1654, under the terms of the Tender of Union, the \"Ordinance for uniting Scotland into one Commonwealth with England\" was issued by the Lord Protector and proclaimed in Scotland by the military governor of Scotland, General George Monck, 1st Duke of Albemarle. The ordinance declared that \"the people of Scotland should be united with the people of England into one Commonwealth and under one Government\" and decreed that a new \"Arms of the Commonwealth\", incorporating the Saltire, should be placed on \"all the public seals, seals of office, and seals of bodies civil or corporate, in Scotland\" as \"a badge of this Union\".\nFirst Protectorate Parliament.\nCromwell and his Council of State spent the first several months of 1654 preparing for the First Protectorate Parliament by drawing up a set of 84 bills for consideration. The Parliament was freely elected (as free as such elections could be in the 17th century) and as such, the Parliament was filled with a wide range of political interests, and as such did not accomplish any of its goals; it was dissolved as soon as law would allow by Cromwell having passed none of Cromwell's proposed bills.\nRule of the Major-Generals and Second Protectorate Parliament.\nHaving decided that Parliament was not an efficient means of getting his policies enacted, Cromwell instituted a system of direct military rule of England during a period known as the Rule of the Major-Generals; all of England was divided into ten regions, each was governed directly by one of Cromwell's Major-Generals, who were given sweeping powers to collect taxes and enforce the peace. The Major-Generals were highly unpopular, a fact that they themselves noticed and many urged Cromwell to call another Parliament to give his rule legitimacy.\nUnlike the prior Parliament, which had been open to all eligible males in the Commonwealth, the new elections specifically excluded Catholics and Royalists from running or voting; as a result, it was stocked with members who were more in line with Cromwell's own politics. The first major bill to be brought up for debate was the Militia Bill, which was ultimately voted down by the House. As a result, the authority of the Major-Generals to collect taxes to support their own regimes ended, and the Rule of the Major Generals came to an end. The second piece of major legislation was the passage of the Humble Petition and Advice, a sweeping constitutional reform which had two purposes. The first was to reserve for Parliament certain rights, such as a three-year fixed term (which the Lord Protector was required to abide by) and to reserve for the Parliament the sole right of taxation. The second, as a concession to Cromwell, was to make the Lord Protector a hereditary position and to convert the title to a formal constitutional Kingship. Cromwell refused the title as King, but accepted the rest of the legislation, which was passed in final form on 25 May 1657.\nA second session of the Parliament met in 1658; it allowed previously excluded MPs (who had been not allowed to take their seats because of Catholic and/or Royalist leanings) to take their seats, however this made the Parliament far less compliant to the wishes of Cromwell and the Major-Generals; it accomplished little in the way of a legislative agenda and was dissolved after a few months.\nRichard Cromwell and the Third Protectorate Parliament.\nOn the death of Oliver Cromwell in 1658, his son, Richard Cromwell, inherited the title, Lord Protector. Richard had never served in the Army, which meant he lost control over the Major-Generals that had been the source of his own father's power. The Third Protectorate Parliament was summoned in late 1658 and was seated on 27 January 1659. Its first act was to confirm Richard's role as Lord Protector, which it did by a sizeable, but not overwhelming, majority. Quickly, however, it became apparent that Richard had no control over the Army and divisions quickly developed in the Parliament. One faction called for a recall of the Rump Parliament and a return to the constitution of the Commonwealth, while another preferred the existing constitution. As the parties grew increasingly quarrelsome, Richard dissolved it. He was quickly removed from power, and the remaining Army leadership recalled the Rump Parliament, setting the stage for the return of the Monarchy a year later.\n1659\u20131660.\nAfter the Grandees in the New Model Army removed Richard, they reinstalled the Rump Parliament on May 1659. Charles Fleetwood was appointed a member of the Committee of Safety and of the Council of State, and one of the seven commissioners for the army. On 9 June he was nominated lord-general (commander-in-chief) of the army. However, his power was undermined in parliament, which chose to disregard the army's authority in a similar fashion to the pre\u2013Civil War parliament. On 12 October 1659 the Commons cashiered General John Lambert and other officers, and installed Fleetwood as chief of a military council under the authority of the Speaker. The next day Lambert ordered that the doors of the House be shut and the members kept out. On 26 October a \"Committee of Safety\" was appointed, of which Fleetwood and Lambert were members. Lambert was appointed major-general of all the forces in England and Scotland, Fleetwood being general. Lambert was now sent, by the Committee of Safety, with a large force to meet George Monck, who was in command of the English forces in Scotland, and either negotiate with him or force him to come to terms.\nIt was into this atmosphere that General George Monck marched south with his army from Scotland. Lambert's army began to desert him, and he returned to London almost alone. On 21 February 1660, Monck reinstated the Presbyterian members of the Long Parliament 'secluded' by Pride, so that they could prepare legislation for a new parliament. Fleetwood was deprived of his command and ordered to appear before parliament to answer for his conduct. On 3 March Lambert was sent to the Tower, from which he escaped a month later. Lambert tried to rekindle the civil war in favour of the Commonwealth by issuing a proclamation calling on all supporters of the \"Good Old Cause\" to rally on the battlefield of Edgehill. However, he was recaptured by Colonel Richard Ingoldsby, a regicide who hoped to win a pardon by handing Lambert over to the new regime. The Long Parliament dissolved itself on 16 March.\nOn 4 April 1660, in response to a secret message sent by Monck, Charles II issued the Declaration of Breda, which made known the conditions of his acceptance of the crown of England. Monck organised the Convention Parliament, which met for the first time on 25 April. On 8 May it proclaimed that King Charles II had been the lawful monarch since the execution of Charles I in January 1649. Charles returned from exile on 23 May. He entered London on 29 May, his birthday. To celebrate \"his Majesty's Return to his Parliament\" 29 May was made a public holiday, popularly known as Oak Apple Day. He was crowned at Westminster Abbey on 23 April 1661."}
{"id": "7131", "revid": "40053569", "url": "https://en.wikipedia.org/wiki?curid=7131", "title": "Charles Evers", "text": "James Charles Evers (September 11, 1922July 22, 2020) was an American civil rights activist, businessman, disc jockey, and politician. Evers was known for his role in the civil rights movement along with his younger brother Medgar Evers. After serving in World War II, Evers began his career as a disc jockey at WHOC in Philadelphia, Mississippi. In 1954, he was made the National Association for the Advancement of Colored People (NAACP) State Voter Registration chairman. After his brother's assassination in 1963, Evers took over his position as field director of the NAACP in Mississippi. In this role, he organized and led many demonstrations for the rights of African Americans.\nIn 1969, Evers was named \"Man of the Year\" by the NAACP. On June 3, 1969, Evers was elected in Fayette, Mississippi, as the first African-American mayor in Mississippi in the post-Reconstruction era, following passage of the Voting Rights Act of 1965 which enforced constitutional rights for citizens.\nAt the time of Evers's election as mayor, the town of Fayette had a population of 1,600 of which 75% was African-American and almost 25% white; the white officers on the Fayette city police \"resigned rather than work under a black administration,\" according to the Associated Press. Evers told reporters \"I guess we will just have to operate with an all-black police department for the present. But I am still looking for some whites to join us in helping Fayette grow.\" Evers then outlawed the carrying of firearms within city limits.\nHe unsuccessfully ran for governor in 1971 and the United States Senate in 1978, both times as an independent candidate, and in 1989, Evers was defeated for re-election after serving sixteen years as mayor. In his later life, he became a Republican, endorsing Ronald Reagan in 1980, and more recently Donald Trump in 2016. This diversity in party affiliations throughout his life was reflected in his fostering of friendships with people from a variety of backgrounds, as well as his advising of politicians from across the political spectrum. After his political career ended, he returned to radio and hosted his own show, \"Let's Talk\". In 2017, Evers was inducted into the National Rhythm &amp; Blues Hall of Fame for his contributions to the music industry.\nEarly life and education.\nCharles Evers was born in Decatur, Mississippi, on September 11, 1922, to James Evers, a laborer, and Jesse Wright Evers, a maid. He was the eldest of four children; Medgar Evers was his younger brother. He attended segregated public schools, which were typically underfunded in Mississippi following the exclusion of African Americans from the political system by disenfranchisement after 1890. Evers graduated from Alcorn State University in Lorman, Mississippi.\nCareer.\nDuring World War II, Charles and Medgar Evers both served in the United States Army. Charles fell in love with a Philippine woman while stationed overseas. He could not marry her and bring her home to his native Mississippi because the state's constitution prohibited interracial marriages.\nBefore and after the war, Evers participated in bootlegging operations, prostitution, and numbers in Mississippi and Chicago. He revealed this part of his past in 1971 prior to his campaign for governor. He said he was not proud of it, but was proud that he had changed his life and left such crime activities far behind.\nIn 1949, Evers began a career in radio as a disc jockey at WHOC in Philadelphia, Mississippi. After serving a year of reserve duty following the Korean War, he settled in Philadelphia, Mississippi, where he operated \"a hotel, restaurant, cab service and gas station, became a disc jockey and promoted prostitution and bootlegging\".\nCivil rights activism.\nIn Mississippi about 1951, brothers Charles and Medgar Evers grew interested in African freedom movements. They were interested in Jomo Kenyatta and the rise of the Kikuyu tribal resistance to colonialism in Kenya, known as the Mau Mau uprising as it moved to open violence. Along with his brother, Charles became active in the Regional Council of Negro Leadership (RCNL), a civil rights organization that promoted self-help and business ownership. Between 1952 and 1955, Evers often spoke at the RCNL's annual conferences in Mound Bayou, a town founded by freedmen, on such issues as voting rights.\nAround 1956, Evers' entrepreneurial gifts and his civil rights activism landed him in trouble in Philadelphia. He left town and moved to Chicago, Illinois. There, he fell into a life of hustling, running numbers for organized crime, and pimping. He documented these activities in his 1971 autobiography, \"Evers\". His brother Medgar continued to be involved in civil rights, becoming field secretary and head of the NAACP in Mississippi.\nOn June 12, 1963, Byron De La Beckwith, a member of a Ku Klux Klan chapter, fatally shot Evers' brother, Medgar, in Mississippi as he arrived home from work. Evers died at the hospital in Jackson. Evers was working in Chicago at the time of his brother's death. He was shocked and deeply upset by his brother's assassination. Over the opposition of more establishment figures in the National Association for the Advancement of Colored People (NAACP) such as Roy Wilkins, Evers took over his brother's post as head of the NAACP in Mississippi. A decade after his death, Evers and blues musician B.B. King created the Medgar Evers Homecoming Festival, an annual three-day event held the first week of June in Mississippi.\nMayor of Fayette.\nIn 1969, following passage of the federal Voting Rights Act of 1965, which authorized federal enforcement of the right to vote, Evers was elected mayor of Fayette, Mississippi. He was the first African-American mayor elected in his state since Reconstruction. In a rural area dominated by cotton plantations, Fayette had a majority of black residents. Its minority white community was known to be hostile toward blacks.\nEvers' election as mayor had great symbolic significance statewide and attracted national attention. The NAACP named Evers the 1969 Man of the Year. Author John Updike mentioned Evers in his popular novel \"Rabbit Redux\" (1971). Evers popularized the slogan, \"Hands that picked cotton can now pick the mayor.\"\nEvers served many terms as mayor of Fayette. Admired by some, he alienated others with his inflexible stands on various issues. Evers did not like to share or delegate power. Evers lost the Democratic primary for mayor in 1981 to Kennie Middleton. Four years later, Evers defeated Middleton in the primaries and won back the office of mayor. In 1989, Evers lost the nomination once again to political rival Kennie Middleton. In his response to the defeat, Evers accepted his defeating citing that he was tired and that: \"Twenty years is enough. I'm tired of being out front. Let someone else be out front.\"\nPolitical influence.\nEvers endorsed Ronald Reagan for President of the United States during the 1980 United States presidential election. Evers later attracted controversy for his support of judicial nominee Charles W. Pickering, a Republican, who was nominated by President George H. W. Bush for a seat on the U.S. Court of Appeals. Evers criticized the NAACP and other organizations for opposing Pickering, as he said the candidate had a record of supporting the civil rights movement in Mississippi.\nEvers befriended a range of people from sharecroppers to presidents. He was an informal adviser to politicians as diverse as Lyndon B. Johnson, George C. Wallace, Ronald Reagan and Robert F. Kennedy. On the other hand, Evers severely criticized such national leaders as Roy Wilkins, Stokely Carmichael, H. Rap Brown and Louis Farrakhan over various issues.\nEvers was a member of the Republican Party for 30 years when he spoke warmly of the 2008 election of Barack Obama as the first black President of the United States. During the 2016 presidential election Evers supported Donald Trump's presidential campaign.\nElectoral campaigns.\nIn 1968, Evers used volunteer armed guards to protect his Jackson residence during the campaign when he competed with six white candidates for the vacant congressional seat which became open when John Bell Williams was elected governor.\nIn 1971, Evers ran in the gubernatorial general election, but was defeated by Democrat William \"Bill\" Waller, 601,222 (77 percent) to 172,762 (22.1 percent). Waller had prosecuted the murder case of suspect Byron De La Beckwith. When Waller gave a victory speech on election night, Evers drove across town to a local TV station to congratulate him. A reporter later wrote that\nWaller's aides learned Evers was in the building and tried to hustle the governor-elect out of the studio as soon as the interview ended. They were not quite quick enough. Surrounded by photographers, reporters, and television crews, Evers approached Waller's car just as it was about to pull out. Waller and his wife were in the back seat. \"I just wanted to congratulate you,\" said Evers. \"Whaddya say, Charlie?\" boomed Waller. His wife leaned across with a stiff smile and shook the loser's hand. During the campaign Evers told reporters that his main purpose in running was to encourage registration of black voters.\nIn 1978, Evers ran as an independent for the U.S. Senate seat vacated by Democrat James O. Eastland. He finished in third place behind his opponents, Democrat Maurice Dantin and Republican Thad Cochran. He received 24 percent of the vote, likely siphoning off African-American votes that would have otherwise gone to Dantin. Cochran won the election with a plurality of 45 percent of the vote. With the shift in white voters moving into the Republican Party in the state (and the rest of the South), Cochran was continuously re-elected to his Senate seat. After his failed Senate race, Evers briefly switched political parties and became a Republican.\nIn 1983, Evers ran as an independent for governor of Mississippi but lost to the Democrat Bill Allain. Republican Leon Bramlett of Clarksdale, also known as a college All-American football player, finished second with 39 percent of the vote.\nBooks.\nEvers wrote two autobiographies or memoirs: \"Evers\" (1971), written with Grace Halsell and self-published; and \"Have No Fear,\" written with Andrew Szanton and published by John Wiley &amp; Sons (1997).\nPersonal life.\nEvers was briefly married to Christine Evers until their marriage ended in annulment. In 1951, Evers married Nannie L. Magee, with whom he had four daughters. The couple divorced in June 1974. Evers lived in Brandon, Mississippi, and served as station manager of WMPR 90.1 FM in Jackson.\nOn July 22, 2020, Evers died in Brandon at age 97.\nMedia portrayal.\nEvers was portrayed by Bill Cobbs in the 1996 film \"Ghosts of Mississippi\" (1996)."}
{"id": "7133", "revid": "1201040", "url": "https://en.wikipedia.org/wiki?curid=7133", "title": "Collective nouns/All sorted by collective term", "text": ""}
{"id": "7142", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=7142", "title": "CDMA", "text": ""}
{"id": "7143", "revid": "41416156", "url": "https://en.wikipedia.org/wiki?curid=7143", "title": "Code-division multiple access", "text": "Code-division multiple access (CDMA) is a channel access method used by various radio communication technologies. CDMA is an example of multiple access, where several transmitters can send information simultaneously over a single communication channel. This allows several users to share a band of frequencies (see bandwidth). To permit this without undue interference between the users, CDMA employs spread spectrum technology and a special coding scheme (where each transmitter is assigned a code).\nCDMA optimizes the use of available bandwidth as it transmits over the entire frequency range and does not limit the user's frequency range.\nCDMA allows several users to share a band of frequencies without undue interference between the users. It is used as the access method in many mobile phone standards. IS-95, also called \"cdmaOne\", and its 3G evolution CDMA2000, are often simply referred to as \"CDMA\", but UMTS, the 3G standard used by GSM carriers, also uses \"wideband CDMA\", or W-CDMA, as well as TD-CDMA and TD-SCDMA, as its radio technologies.\nHistory.\nThe technology of code-division multiple access channels has long been known. In the Soviet Union (USSR), the first work devoted to this subject was published in 1935 by Dmitry Ageev. It was shown that through the use of linear methods, there are three types of signal separation: frequency, time and compensatory. The technology of CDMA was used in 1957, when the young military radio engineer Leonid Kupriyanovich in Moscow made an experimental model of a wearable automatic mobile phone, called LK-1 by him, with a base station. LK-1 has a weight of 3\u00a0kg, 20\u201330\u00a0km operating distance, and 20\u201330 hours of battery life. The base station, as described by the author, could serve several customers. In 1958, Kupriyanovich made the new experimental \"pocket\" model of mobile phone. This phone weighed 0.5\u00a0kg. To serve more customers, Kupriyanovich proposed the device, which he called \"correlator.\" In 1958, the USSR also started the development of the \"Altai\" national civil mobile phone service for cars, based on the Soviet MRT-1327 standard. The phone system weighed . It was placed in the trunk of the vehicles of high-ranking officials and used a standard handset in the passenger compartment. The main developers of the Altai system were VNIIS (Voronezh Science Research Institute of Communications) and GSPI (State Specialized Project Institute). In 1963 this service started in Moscow, and in 1970 Altai service was used in 30 USSR cities.\nSteps in CDMA modulation.\nCDMA is a spread-spectrum multiple-access technique. A spread-spectrum technique spreads the bandwidth of the data uniformly for the same transmitted power. A spreading code is a pseudo-random code that has a narrow ambiguity function, unlike other narrow pulse codes. In CDMA a locally generated code runs at a much higher rate than the data to be transmitted. Data for transmission is combined by bitwise XOR (exclusive OR) with the faster code. The figure shows how a spread-spectrum signal is generated. The data signal with pulse duration of formula_1 (symbol period) is XORed with the code signal with pulse duration of formula_2 (chip period). (Note: bandwidth is proportional to formula_3, where formula_4 = bit time.) Therefore, the bandwidth of the data signal is formula_5 and the bandwidth of the spread spectrum signal is formula_6. Since formula_2 is much smaller than formula_1, the bandwidth of the spread-spectrum signal is much larger than the bandwidth of the original signal. The ratio formula_9 is called the spreading factor or processing gain and determines to a certain extent the upper limit of the total number of users supported simultaneously by a base station.\nEach user in a CDMA system uses a different code to modulate their signal. Choosing the codes used to modulate the signal is very important in the performance of CDMA systems. The best performance occurs when there is good separation between the signal of a desired user and the signals of other users. The separation of the signals is made by correlating the received signal with the locally generated code of the desired user. If the signal matches the desired user's code, then the correlation function will be high and the system can extract that signal. If the desired user's code has nothing in common with the signal, the correlation should be as close to zero as possible (thus eliminating the signal); this is referred to as cross-correlation. If the code is correlated with the signal at any time offset other than zero, the correlation should be as close to zero as possible. This is referred to as auto-correlation and is used to reject multi-path interference.\nAn analogy to the problem of multiple access is a room (channel) in which people wish to talk to each other simultaneously. To avoid confusion, people could take turns speaking (time division), speak at different pitches (frequency division), or speak in different languages (code division). CDMA is analogous to the last example where people speaking the same language can understand each other, but other languages are perceived as noise and rejected. Similarly, in radio CDMA, each group of users is given a shared code. Many codes occupy the same channel, but only users associated with a particular code can communicate.\nIn general, CDMA belongs to two basic categories: synchronous (orthogonal codes) and asynchronous (pseudorandom codes).\nCode-division multiplexing (synchronous CDMA).\nThe digital modulation method is analogous to those used in simple radio transceivers. In the analog case, a low-frequency data signal is time-multiplied with a high-frequency pure sine-wave carrier and transmitted. This is effectively a frequency convolution (Wiener\u2013Khinchin theorem) of the two signals, resulting in a carrier with narrow sidebands. In the digital case, the sinusoidal carrier is replaced by Walsh functions. These are binary square waves that form a complete orthonormal set. The data signal is also binary and the time multiplication is achieved with a simple XOR function. This is usually a Gilbert cell mixer in the circuitry.\nSynchronous CDMA exploits mathematical properties of orthogonality between vectors representing the data strings. For example, binary string \"1011\" is represented by the vector (1, 0, 1, 1). Vectors can be multiplied by taking their dot product, by summing the products of their respective components (for example, if u = (\"a\", \"b\") and v = (\"c\", \"d\"), then their dot product u\u00b7v = \"ac\" + \"bd\"). If the dot product is zero, the two vectors are said to be \"orthogonal\" to each other. Some properties of the dot product aid understanding of how W-CDMA works. If vectors a and b are orthogonal, then formula_10 and:\nEach user in synchronous CDMA uses a code orthogonal to the others' codes to modulate their signal. An example of 4 mutually orthogonal digital signals is shown in the figure below. Orthogonal codes have a cross-correlation equal to zero; in other words, they do not interfere with each other. In the case of IS-95, 64-bit Walsh codes are used to encode the signal to separate different users. Since each of the 64 Walsh codes is orthogonal to all other, the signals are channelized into 64 orthogonal signals. The following example demonstrates how each user's signal can be encoded and decoded.\nExample.\nStart with a set of vectors that are mutually orthogonal. (Although mutual orthogonality is the only condition, these vectors are usually constructed for ease of decoding, for example columns or rows from Walsh matrices.) An example of orthogonal functions is shown in the adjacent picture. These vectors will be assigned to individual users and are called the \"code\", \"chip code\", or \"chipping code\". In the interest of brevity, the rest of this example uses codes v with only two bits.\nEach user is associated with a different code, say v. A 1 bit is represented by transmitting a positive code v, and a 0 bit is represented by a negative code \u2212v. For example, if v = (\"v\"0, \"v\"1) = (1, \u22121) and the data that the user wishes to transmit is (1, 0, 1, 1), then the transmitted symbols would be\nFor the purposes of this article, we call this constructed vector the \"transmitted vector\".\nEach sender has a different, unique vector v chosen from that set, but the construction method of the transmitted vector is identical.\nNow, due to physical properties of interference, if two signals at a point are in phase, they add to give twice the amplitude of each signal, but if they are out of phase, they subtract and give a signal that is the difference of the amplitudes. Digitally, this behaviour can be modelled by the addition of the transmission vectors, component by component.\nIf sender0 has code (1, \u22121) and data (1, 0, 1, 1), and sender1 has code (1, 1) and data (0, 0, 1, 1), and both senders transmit simultaneously, then this table describes the coding steps:\nBecause signal0 and signal1 are transmitted at the same time into the air, they add to produce the raw signal\nThis raw signal is called an interference pattern. The receiver then extracts an intelligible signal for any known sender by combining the sender's code with the interference pattern. The following table explains how this works and shows that the signals do not interfere with one another:\nFurther, after decoding, all values greater than 0 are interpreted as 1, while all values less than zero are interpreted as 0. For example, after decoding, data0 is (2, \u22122, 2, 2), but the receiver interprets this as (1, 0, 1, 1). Values of exactly 0 means that the sender did not transmit any data, as in the following example:\nAssume signal0 = (1, \u22121, \u22121, 1, 1, \u22121, 1, \u22121) is transmitted alone. The following table shows the decode at the receiver:\nWhen the receiver attempts to decode the signal using sender1's code, the data is all zeros, therefore the cross-correlation is equal to zero and it is clear that sender1 did not transmit any data.\nAsynchronous CDMA.\nWhen mobile-to-base links cannot be precisely coordinated, particularly due to the mobility of the handsets, a different approach is required. Since it is not mathematically possible to create signature sequences that are both orthogonal for arbitrarily random starting points and which make full use of the code space, unique \"pseudo-random\" or \"pseudo-noise\" sequences called spreading sequences are used in \"asynchronous\" CDMA systems. A spreading sequence is a binary sequence that appears random but can be reproduced in a deterministic manner by intended receivers. These spreading sequences are used to encode and decode a user's signal in asynchronous CDMA in the same manner as the orthogonal codes in synchronous CDMA (shown in the example above). These spreading sequences are statistically uncorrelated, and the sum of a large number of spreading sequences results in \"multiple access interference\" (MAI) that is approximated by a Gaussian noise process (following the central limit theorem in statistics). Gold codes are an example of a spreading sequence suitable for this purpose, as there is low correlation between the codes. If all of the users are received with the same power level, then the variance (e.g., the noise power) of the MAI increases in direct proportion to the number of users. In other words, unlike synchronous CDMA, the signals of other users will appear as noise to the signal of interest and interfere slightly with the desired signal in proportion to number of users.\nAll forms of CDMA use the spread-spectrum spreading factor to allow receivers to partially discriminate against unwanted signals. Signals encoded with the specified spreading sequences are received, while signals with different sequences (or the same sequences but different timing offsets) appear as wideband noise reduced by the spreading factor.\nSince each user generates MAI, controlling the signal strength is an important issue with CDMA transmitters. A CDM (synchronous CDMA), TDMA, or FDMA receiver can in theory completely reject arbitrarily strong signals using different codes, time slots or frequency channels due to the orthogonality of these systems. This is not true for asynchronous CDMA; rejection of unwanted signals is only partial. If any or all of the unwanted signals are much stronger than the desired signal, they will overwhelm it. This leads to a general requirement in any asynchronous CDMA system to approximately match the various signal power levels as seen at the receiver. In CDMA cellular, the base station uses a fast closed-loop power-control scheme to tightly control each mobile's transmit power.\nAdvantages of asynchronous CDMA over other techniques.\nEfficient practical utilization of the fixed frequency spectrum.\nIn theory CDMA, TDMA and FDMA have exactly the same spectral efficiency, but, in practice, each has its own challenges \u2013 power control in the case of CDMA, timing in the case of TDMA, and frequency generation/filtering in the case of FDMA.\nTDMA systems must carefully synchronize the transmission times of all the users to ensure that they are received in the correct time slot and do not cause interference. Since this cannot be perfectly controlled in a mobile environment, each time slot must have a guard time, which reduces the probability that users will interfere, but decreases the spectral efficiency.\nSimilarly, FDMA systems must use a guard band between adjacent channels, due to the unpredictable Doppler shift of the signal spectrum because of user mobility. The guard bands will reduce the probability that adjacent channels will interfere, but decrease the utilization of the spectrum.\nFlexible allocation of resources.\nAsynchronous CDMA offers a key advantage in the flexible allocation of resources i.e. allocation of spreading sequences to active users. In the case of CDM (synchronous CDMA), TDMA, and FDMA the number of simultaneous orthogonal codes, time slots, and frequency slots respectively are fixed, hence the capacity in terms of the number of simultaneous users is limited. There are a fixed number of orthogonal codes, time slots or frequency bands that can be allocated for CDM, TDMA, and FDMA systems, which remain underutilized due to the bursty nature of telephony and packetized data transmissions. There is no strict limit to the number of users that can be supported in an asynchronous CDMA system, only a practical limit governed by the desired bit error probability since the SIR (signal-to-interference ratio) varies inversely with the number of users. In a bursty traffic environment like mobile telephony, the advantage afforded by asynchronous CDMA is that the performance (bit error rate) is allowed to fluctuate randomly, with an average value determined by the number of users times the percentage of utilization. Suppose there are 2\"N\" users that only talk half of the time, then 2\"N\" users can be accommodated with the same \"average\" bit error probability as \"N\" users that talk all of the time. The key difference here is that the bit error probability for \"N\" users talking all of the time is constant, whereas it is a \"random\" quantity (with the same mean) for 2\"N\" users talking half of the time.\nIn other words, asynchronous CDMA is ideally suited to a mobile network where large numbers of transmitters each generate a relatively small amount of traffic at irregular intervals. CDM (synchronous CDMA), TDMA, and FDMA systems cannot recover the underutilized resources inherent to bursty traffic due to the fixed number of orthogonal codes, time slots or frequency channels that can be assigned to individual transmitters. For instance, if there are \"N\" time slots in a TDMA system and 2\"N\" users that talk half of the time, then half of the time there will be more than \"N\" users needing to use more than \"N\" time slots. Furthermore, it would require significant overhead to continually allocate and deallocate the orthogonal-code, time-slot or frequency-channel resources. By comparison, asynchronous CDMA transmitters simply send when they have something to say and go off the air when they do not, keeping the same signature sequence as long as they are connected to the system.\nSpread-spectrum characteristics of CDMA.\nMost modulation schemes try to minimize the bandwidth of this signal since bandwidth is a limited resource. However, spread-spectrum techniques use a transmission bandwidth that is several orders of magnitude greater than the minimum required signal bandwidth. One of the initial reasons for doing this was military applications including guidance and communication systems. These systems were designed using spread spectrum because of its security and resistance to jamming. Asynchronous CDMA has some level of privacy built in because the signal is spread using a pseudo-random code; this code makes the spread-spectrum signals appear random or have noise-like properties. A receiver cannot demodulate this transmission without knowledge of the pseudo-random sequence used to encode the data. CDMA is also resistant to jamming. A jamming signal only has a finite amount of power available to jam the signal. The jammer can either spread its energy over the entire bandwidth of the signal or jam only part of the entire signal.\nCDMA can also effectively reject narrow-band interference. Since narrow-band interference affects only a small portion of the spread-spectrum signal, it can easily be removed through notch filtering without much loss of information. Convolution encoding and interleaving can be used to assist in recovering this lost data. CDMA signals are also resistant to multipath fading. Since the spread-spectrum signal occupies a large bandwidth, only a small portion of this will undergo fading due to multipath at any given time. Like the narrow-band interference, this will result in only a small loss of data and can be overcome.\nAnother reason CDMA is resistant to multipath interference is because the delayed versions of the transmitted pseudo-random codes will have poor correlation with the original pseudo-random code, and will thus appear as another user, which is ignored at the receiver. In other words, as long as the multipath channel induces at least one chip of delay, the multipath signals will arrive at the receiver such that they are shifted in time by at least one chip from the intended signal. The correlation properties of the pseudo-random codes are such that this slight delay causes the multipath to appear uncorrelated with the intended signal, and it is thus ignored.\nSome CDMA devices use a rake receiver, which exploits multipath delay components to improve the performance of the system. A rake receiver combines the information from several correlators, each one tuned to a different path delay, producing a stronger version of the signal than a simple receiver with a single correlation tuned to the path delay of the strongest signal.\nFrequency reuse is the ability to reuse the same radio channel frequency at other cell sites within a cellular system. In the FDMA and TDMA systems, frequency planning is an important consideration. The frequencies used in different cells must be planned carefully to ensure signals from different cells do not interfere with each other. In a CDMA system, the same frequency can be used in every cell, because channelization is done using the pseudo-random codes. Reusing the same frequency in every cell eliminates the need for frequency planning in a CDMA system; however, planning of the different pseudo-random sequences must be done to ensure that the received signal from one cell does not correlate with the signal from a nearby cell.\nSince adjacent cells use the same frequencies, CDMA systems have the ability to perform soft hand-offs. Soft hand-offs allow the mobile telephone to communicate simultaneously with two or more cells. The best signal quality is selected until the hand-off is complete. This is different from hard hand-offs utilized in other cellular systems. In a hard-hand-off situation, as the mobile telephone approaches a hand-off, signal strength may vary abruptly. In contrast, CDMA systems use the soft hand-off, which is undetectable and provides a more reliable and higher-quality signal.\nCollaborative CDMA.\nA novel collaborative multi-user transmission and detection scheme called collaborative CDMA has been investigated for the uplink that exploits the differences between users' fading channel signatures to increase the user capacity well beyond the spreading length in the MAI-limited environment. The authors show that it is possible to achieve this increase at a low complexity and high bit error rate performance in flat fading channels, which is a major research challenge for overloaded CDMA systems. In this approach, instead of using one sequence per user as in conventional CDMA, the authors group a small number of users to share the same spreading sequence and enable group spreading and despreading operations. The new collaborative multi-user receiver consists of two stages: group multi-user detection (MUD) stage to suppress the MAI between the groups and a low-complexity maximum-likelihood detection stage to recover jointly the co-spread users' data using minimal Euclidean-distance measure and users' channel-gain coefficients. An enhanced CDMA version known as interleave-division multiple access (IDMA) uses the orthogonal interleaving as the only means of user separation in place of signature sequence used in CDMA system."}
{"id": "7144", "revid": "40883815", "url": "https://en.wikipedia.org/wiki?curid=7144", "title": "Content-control software", "text": "Content-control software, commonly referred to as an Internet filter, is software that restricts or controls the content an Internet user is capable to access, especially when utilised to restrict material delivered over the Internet via the Web, e-mail, or other means. Content-control software determines what content will be available or be blocked.\nSuch restrictions can be applied at various levels: a government can attempt to apply them nationwide (see Internet censorship), or they can, for example, be applied by an ISP to its clients, by an employer to its personnel, by a school to its students, by a library to its visitors, by a parent to a child's computer, or by an individual user to their own computer.\nThe motive is often to prevent access to content which the computer's owner(s) or other authorities may consider objectionable. When imposed without the consent of the user, content control can be characterised as a form of internet censorship. Some content-control software includes time control functions that empowers parents to set the amount of time that child may spend accessing the Internet or playing games or other computer activities.\nIn some countries, such software is ubiquitous. In Cuba, if a computer user at a government-controlled Internet cafe types certain words, the word processor or web browser is automatically closed, and a \"state security\" warning is given.\nTerminology.\nThe term \"content control\" is used on occasion by CNN, \"Playboy\" magazine, the \"San Francisco Chronicle\", and \"The New York Times\". However, several other terms, including \"content filtering software\", \"filtering proxy servers\", \"secure web gateways\", \"censorware\", \"content security and control\", \"web filtering software\", \"content-censoring software\", and \"content-blocking software\", are often used. \"Nannyware\" has also been used in both product marketing and by the media. Industry research company Gartner uses \"secure web gateway\" (SWG) to describe the market segment.\nCompanies that make products that selectively block Web sites do not refer to these products as censorware, and prefer terms such as \"Internet filter\" or \"URL Filter\"; in the specialized case of software specifically designed to allow parents to monitor and restrict the access of their children, \"parental control software\" is also used. Some products log all sites that a user accesses and rates them based on content type for reporting to an \"accountability partner\" of the person's choosing, and the term accountability software is used. Internet filters, parental control software, and/or accountability software may also be combined into one product.\nThose critical of such software, however, use the term \"censorware\" freely: consider the Censorware Project, for example. The use of the term \"censorware\" in editorials criticizing makers of such software is widespread and covers many different varieties and applications: Xeni Jardin used the term in a 9 March 2006 editorial in \"The New York Times\" when discussing the use of American-made filtering software to suppress content in China; in the same month a high school student used the term to discuss the deployment of such software in his school district.\nIn general, outside of editorial pages as described above, traditional newspapers do not use the term \"censorware\" in their reporting, preferring instead to use less overtly controversial terms such as \"content filter\", \"content control\", or \"web filtering\"; \"The New York Times\" and \"The Wall Street Journal\" both appear to follow this practice. On the other hand, Web-based newspapers such as CNET use the term in both editorial and journalistic contexts, for example \"Windows Live to Get Censorware.\"\nTypes of filtering.\nFilters can be implemented in many different ways: by software on a personal computer, via network infrastructure such as proxy servers, DNS servers, or firewalls that provide Internet access. No solution provides complete coverage, so most companies deploy a mix of technologies to achieve the proper content control in line with their policies.\nReasons for filtering.\nThe Internet does not intrinsically provide content blocking, and therefore there is much content on the Internet that is considered unsuitable for children, given that much content is given certifications as suitable for adults only, e.g. 18-rated games and movies.\nInternet service providers (ISPs) that block material containing pornography, or controversial religious, political, or news-related content en route are often utilized by parents who do not permit their children to access content not conforming to their personal beliefs. Content filtering software can, however, also be used to block malware and other content that is or contains hostile, intrusive, or annoying material including adware, spam, computer viruses, worms, trojan horses, and spyware.\nMost content control software is marketed to organizations or parents. It is, however, also marketed on occasion to facilitate self-censorship, for example by people struggling with addictions to online pornography, gambling, chat rooms, etc. Self-censorship software may also be utilised by some in order to avoid viewing content they consider immoral, inappropriate, or simply distracting. A number of accountability software products are marketed as \"self-censorship\" or \"accountability software\". These are often promoted by religious media and at religious gatherings.\nCriticism.\nFiltering errors.\nOverblocking.\nUtilizing a filter that is overly zealous at filtering content, or mislabels content not intended to be censored can result in over blocking, or over-censoring. Over blocking can filter out material that should be acceptable under the filtering policy in effect, for example health related information may unintentionally be filtered along with porn-related material because of the Scunthorpe problem. Filter administrators may prefer to err on the side of caution by accepting over blocking to prevent any risk of access to sites that they determine to be undesirable. Content-control software was mentioned as blocking access to Beaver College before its name change to Arcadia University. Another example was the filtering of Horniman Museum. As well, over-blocking may encourage users to bypass the filter entirely.\nUnderblocking.\nWhenever new information is uploaded to the Internet, filters can under block, or under-censor, content if the parties responsible for maintaining the filters do not update them quickly and accurately, and a blacklisting rather than a whitelisting filtering policy is in place.\nMorality and opinion.\nMany would not be satisfied with government filtering viewpoints on moral or political issues, agreeing that this could become support for propaganda. Many would also find it unacceptable that an ISP, whether by law or by the ISP's own choice, should deploy such software without allowing the users to disable the filtering for their own connections. In the United States, the First Amendment to the United States Constitution has been cited in calls to criminalise forced internet censorship. (See section below)\nWithout adequate governmental supervision, content-filtering software could enable private companies to censor as they please. (See Religious or political censorship, below). Government utilisation or encouragement of content-control software is a component of Internet Censorship (not to be confused with Internet Surveillance, in which content is monitored and not necessarily restricted). The governments of countries such as the People's Republic of China, and Cuba are current examples of countries in which this ethically controversial activity is alleged to have taken place.\nLegal actions.\nIn 1998, a United States federal district court in Virginia ruled (Loudoun v. Board of Trustees of the Loudoun County Library) that the imposition of mandatory filtering in a public library violates the First Amendment.\nIn 1996 the US Congress passed the Communications Decency Act, banning indecency on the Internet. Civil liberties groups challenged the law under the First Amendment, and in 1997 the Supreme Court ruled in their favor. Part of the civil liberties argument, especially from groups like the Electronic Frontier Foundation, was that parents who wanted to block sites could use their own content-filtering software, making government involvement unnecessary.\nIn the late 1990s, groups such as the Censorware Project began reverse-engineering the content-control software and decrypting the blacklists to determine what kind of sites the software blocked. This led to legal action alleging violation of the \"Cyber Patrol\" license agreement. They discovered that such tools routinely blocked unobjectionable sites while also failing to block intended targets.\nSome content-control software companies responded by claiming that their filtering criteria were backed by intensive manual checking. The companies' opponents argued, on the other hand, that performing the necessary checking would require resources greater than the companies possessed and that therefore their claims were not valid.\nThe Motion Picture Association successfully obtained a UK ruling enforcing ISPs to use content-control software to prevent copyright infringement by their subscribers.\nReligious, anti-religious, and political censorship.\nMany types of content-control software have been shown to block sites based on the religious and political leanings of the company owners. Examples include blocking several religious sites (including the Web site of the Vatican), many political sites, and homosexuality-related sites. \"X-Stop\" was shown to block sites such as the Quaker web site, the National Journal of Sexual Orientation Law, The Heritage Foundation, and parts of The Ethical Spectacle. CYBERsitter blocks out sites like National Organization for Women. Nancy Willard, an academic researcher and attorney, pointed out that many U.S. public schools and libraries use the same filtering software that many Christian organizations use. Cyber Patrol, a product developed by The Anti-Defamation League and Mattel's The Learning Company, has been found to block not only political sites it deems to be engaging in 'hate speech' but also human rights web sites, such as Amnesty International's web page about Israel and gay-rights web sites, such as glaad.org.\nContent labeling.\nContent labeling may be considered another form of content-control software. In 1994, the Internet Content Rating Association (ICRA) \u2014 now part of the Family Online Safety Institute \u2014 developed a content rating system for online content providers. Using an online questionnaire a webmaster describes the nature of their web content. A small file is generated that contains a condensed, computer readable digest of this description that can then be used by content filtering software to block or allow that site.\nICRA labels come in a variety of formats. These include the World Wide Web Consortium's Resource Description Framework (RDF) as well as Platform for Internet Content Selection (PICS) labels used by Microsoft's Internet Explorer Content Advisor.\nICRA labels are an example of self-labeling. Similarly, in 2006 the Association of Sites Advocating Child Protection (ASACP) initiated the Restricted to Adults self-labeling initiative. ASACP members were concerned that various forms of legislation being proposed in the United States were going to have the effect of forcing adult companies to label their content. The RTA label, unlike ICRA labels, does not require a webmaster to fill out a questionnaire or sign up to use. Like ICRA the RTA label is free. Both labels are recognized by a wide variety of content-control software.\nThe Voluntary Content Rating (VCR) system was devised by Solid Oak Software for their CYBERsitter filtering software, as an alternative to the PICS system, which some critics deemed too complex. It employs HTML metadata tags embedded within web page documents to specify the type of content contained in the document. Only two levels are specified, \"mature\" and \"adult\", making the specification extremely simple.\nUse in public libraries.\nUnited States.\nThe use of Internet filters or content-control software varies widely in public libraries in the United States, since Internet use policies are established by the local library board. Many libraries adopted Internet filters after Congress conditioned the receipt of universal service discounts on the use of Internet filters through the Children's Internet Protection Act (CIPA). Other libraries do not install content control software, believing that acceptable use policies and educational efforts address the issue of children accessing age-inappropriate content while preserving adult users' right to freely access information. Some libraries use Internet filters on computers used by children only. Some libraries that employ content-control software allow the software to be deactivated on a case-by-case basis on application to a librarian; libraries that are subject to CIPA are required to have a policy that allows adults to request that the filter be disabled without having to explain the reason for their request.\nMany legal scholars believe that a number of legal cases, in particular \"Reno v. American Civil Liberties Union\", established that the use of content-control software in libraries is a violation of the First Amendment. The Children's Internet Protection Act [CIPA] and the June 2003 case \"United States v. American Library Association\" found CIPA constitutional as a condition placed on the receipt of federal funding, stating that First Amendment concerns were dispelled by the law's provision that allowed adult library users to have the filtering software disabled, without having to explain the reasons for their request. The plurality decision left open a future \"as-applied\" Constitutional challenge, however.\nIn November 2006, a lawsuit was filed against the North Central Regional Library District (NCRL) in Washington State for its policy of refusing to disable restrictions upon requests of adult patrons, but CIPA was not challenged in that matter. In May 2010, the Washington State Supreme Court provided an opinion after it was asked to certify a question referred by the United States District Court for the Eastern District of Washington: \"Whether a public library, consistent with Article I, \u00a7 5 of the Washington Constitution, may filter Internet access for all patrons without disabling Web sites containing constitutionally-protected speech upon the request of an adult library patron.\" The Washington State Supreme Court ruled that NCRL's internet filtering policy did not violate Article I, Section 5 of the Washington State Constitution. The Court said: \"It appears to us that NCRL's filtering policy is reasonable and accords with its mission and these policies and is viewpoint neutral. It appears that no article I, section 5 content-based violation exists in this case. NCRL's essential mission is to promote reading and lifelong learning. As NCRL maintains, it is reasonable to impose restrictions on Internet access in order to maintain an environment that is conducive to study and contemplative thought.\" The case returned to federal court.\nIn March 2007, Virginia passed a law similar to CIPA that requires public libraries receiving state funds to use content-control software. Like CIPA, the law requires libraries to disable filters for an adult library user when requested to do so by the user.\nAustralia.\nThe Australian Internet Safety Advisory Body has information about \"practical advice on Internet safety, parental control and filters for the protection of children, students and families\" that also includes public libraries.\nNetAlert, the software made available free of charge by the Australian government, was allegedly cracked by a 16-year-old student, Tom Wood, less than a week after its release in August 2007. Wood supposedly bypassed the $84 million filter in about half an hour to highlight problems with the government's approach to Internet content filtering.\nThe Australian Government has introduced legislation that requires ISP's to \"restrict access to age restricted content (commercial MA15+ content and R18+ content) either hosted in Australia or provided from Australia\" that was due to commence from 20 January 2008, known as Cleanfeed.\nCleanfeed is a proposed mandatory ISP level content filtration system. It was proposed by the Beazley led Australian Labor Party opposition in a 2006 press release, with the intention of protecting children who were vulnerable due to claimed parental computer illiteracy. It was announced on 31 December 2007 as a policy to be implemented by the Rudd ALP government, and initial tests in Tasmania have produced a 2008 report. Cleanfeed is funded in the current budget, and is moving towards an Expression of Interest for live testing with ISPs in 2008. Public opposition and criticism have emerged, led by the EFA and gaining irregular mainstream media attention, with a majority of Australians reportedly \"strongly against\" its implementation. Criticisms include its expense, inaccuracy (it will be impossible to ensure only illegal sites are blocked) and the fact that it will be compulsory, which can be seen as an intrusion on free speech rights. Another major criticism point has been that although the filter is claimed to stop certain materials, the underground rings dealing in such materials will not be affected. The filter might also provide a false sense of security for parents, who might supervise children less while using the Internet, achieving the exact opposite effect. Cleanfeed is a responsibility of Senator Conroy's portfolio.\nDenmark.\nIn Denmark it is stated policy that it will \"prevent inappropriate Internet sites from being accessed from children's libraries across Denmark.\" \"'It is important that every library in the country has the opportunity to protect children against pornographic material when they are using library computers. It is a main priority for me as Culture Minister to make sure children can surf the net safely at libraries,' states Brian Mikkelsen in a press-release of the Danish Ministry of Culture.\"\nBypassing filters.\nContent filtering in general can \"be bypassed entirely by tech-savvy individuals.\" Blocking content on a device \"[will not]...guarantee that users won't eventually be able to find a way around the filter.\"\nSome software may be bypassed successfully by using alternative protocols such as FTP or telnet or HTTPS, conducting searches in a different language, using a proxy server or a circumventor such as Psiphon. Also cached web pages returned by Google or other searches could bypass some controls as well. Web syndication services may provide alternate paths for content. Some of the more poorly designed programs can be shut down by killing their processes: for example, in Microsoft Windows through the Windows Task Manager, or in Mac OS X using Force Quit or Activity Monitor. Numerous workarounds and counters to workarounds from content-control software creators exist.\nGoogle services are often blocked by filters, but these may most often be bypassed by using \"https://\" in place of \"http://\" since content filtering software is not able to interpret content under secure connections (in this case SSL).\nAn encrypted VPN can be used as means of bypassing content control software, especially if the content control software is installed on an Internet gateway or firewall.\nThere many other ways in which to bypass a content control filter that include translation sites, establishing a remote connection with another computer that has no content control filter on it and altering the proxy settings of the browser.\nProducts and services.\nSome ISPs offer parental control options. Some offer security software which includes parental controls. Mac OS X v10.4 offers parental controls for several applications (Mail, Finder, iChat, Safari &amp; Dictionary). Microsoft's Windows Vista operating system also includes content-control software.\nContent filtering technology exists in two major forms: application gateway or packet inspection. For HTTP access the application gateway is called a web-proxy or just a proxy. Such web-proxies can inspect both the initial request and the returned web page using arbitrarily complex rules and will not return any part of the page to the requester until a decision is made. In addition they can make substitutions in whole or for any part of the returned result. Packet inspection filters do not initially interfere with the connection to the server but inspect the data in the connection as it goes past, at some point the filter may decide that the connection is to be filtered and it will then disconnect it by injecting a TCP-Reset or similar faked packet. The two techniques can be used together with the packet filter monitoring a link until it sees an HTTP connection starting to an IP address that has content that needs filtering. The packet filter then redirects the connection to the web-proxy which can perform detailed filtering on the website without having to pass through all unfiltered connections. This combination is quite popular because it can significantly reduce the cost of the system.\nGateway-based content control software may be more difficult to bypass than desktop software as the user does not have physical access to the filtering device. However, many of the techniques in the Bypassing filters section still work."}
{"id": "7145", "revid": "11308236", "url": "https://en.wikipedia.org/wiki?curid=7145", "title": "Chambered cairn", "text": "A chambered cairn is a burial monument, usually constructed during the Neolithic, consisting of a sizeable (usually stone) chamber around and over which a cairn of stones was constructed. Some chambered cairns are also passage-graves. They are found throughout Britain and Ireland, with the largest number in Scotland.\nTypically, the chamber is larger than a cist, and will contain a larger number of interments, which are either excarnated bones or inhumations (cremations). Most were situated near a settlement, and served as that community's \"graveyard\".\nScotland.\nBackground.\nDuring the early Neolithic (4000\u20133300 BC) architectural forms are highly regionalised with timber and earth monuments predominating in the east and stone-chambered cairns in the west. During the later Neolithic (3300\u20132500 BC) massive circular enclosures and the use of grooved ware and Unstan ware pottery emerge. Scotland has a particularly large number of chambered cairns; they are found in various different types described below. Along with the excavations of settlements such as Skara Brae, Links of Noltland, Barnhouse, Rinyo and Balfarg and the complex site at Ness of Brodgar these cairns provide important clues to the character of civilization in Scotland in the Neolithic. However the increasing use of cropmarks to identify Neolithic sites in lowland areas has tended to diminish the relative prominence of these cairns.\nIn the early phases bones of numerous bodies are often found together and it has been argued that this suggests that in death at least, the status of individuals was played down. During the late Neolithic henge sites were constructed and single burials began to become more commonplace; by the Bronze Age it is possible that even where chambered cairns were still being built they had become the burial places of prominent individuals rather than of communities as a whole.\nClyde-Carlingford court cairns.\nThe Clyde or Clyde-Carlingford type are principally found in northern and western Ireland and southwestern Scotland. They first were identified as a separate group in the Firth of Clyde region, hence the name. Over 100 have been identified in Scotland alone. Lacking a significant passage, they are a form of gallery grave. The burial chamber is normally located at one end of a rectangular or trapezoidal cairn, while a roofless, semi-circular forecourt at the entrance provided access from the outside (although the entrance itself was often blocked), and gives this type of chambered cairn its alternate name of court tomb or court cairn. These forecourts are typically fronted by large stones and it is thought the area in front of the cairn was used for public rituals of some kind. The chambers were created from large stones set on end, roofed with large flat stones and often sub-divided by slabs into small compartments. They are generally considered to be the earliest in Scotland.\nExamples include Cairn Holy I and Cairn Holy II near Newton Stewart, a cairn at Port Charlotte, Islay, which dates to 3900\u20134000 BC, and Monamore, or Meallach's Grave, Arran, which may date from the early fifth millennium BC. Excavations at the Mid Gleniron cairns near Cairnholy revealed a multi-period construction which shed light on the development of this class of chambered cairn.\nOrkney-Cromarty.\nThe Orkney-Cromarty group is by far the largest and most diverse. It has been subdivided into Yarrows, Camster and Cromarty subtypes but the differences are extremely subtle. The design is of dividing slabs at either side of a rectangular chamber, separating it into compartments or stalls. The number of these compartments ranges from 4 in the earliest examples to over 24 in an extreme example on Orkney. The actual shape of the cairn varies from simple circular designs to elaborate 'forecourts' protruding from each end, creating what look like small amphitheatres. It is likely that these are the result of cultural influences from mainland Europe, as they are similar to designs found in France and Spain.\nExamples include Midhowe on Rousay and Unstan Chambered Cairn from the Orkney Mainland, both of which date from the mid 4th millennium BC and were probably in use over long periods of time. When the latter was excavated in 1884, grave goods were found that gave their name to Unstan ware pottery. Blackhammer cairn on Rousay is another example dating from the 3rd millennium BC.\nThe Grey Cairns of Camster in Caithness are examples of this type from mainland Scotland. The Tomb of the Eagles on South Ronaldsay is a stalled cairn that shows some similarities with the later Maeshowe type. It was in use for 800 years or more and numerous bird bones were found here, predominantly white-tailed sea eagle.\nMaeshowe.\nThe Maeshowe group, named after the famous Orkney monument, is among the most elaborate. They appear relatively late and only in Orkney and it is not clear why the use of cairns continued in the north when their construction had largely ceased elsewhere in Scotland. They consist of a central chamber from which lead small compartments, into which burials would be placed. The central chambers are tall and steep-sided and have corbelled roofing faced with high quality stone.\nIn addition to Maeshowe itself, which was constructed c. 2700 BC, there are various other examples from the Orkney Mainland. These include Quanterness chambered cairn (3250 BC) in which the remains of 157 individuals were found when excavated in the 1970s, Cuween Hill near Finstown which was found to contain the bones of men, dogs and oxen and Wideford Hill cairn, which dates from 2000 BC.\nExamples from elsewhere in Orkney are the Vinquoy cairn, found at an elevated location on the north end of the island of Eday and Quoyness on Sanday constructed about 2900 BC and which is surrounded by an arc of Bronze Age mounds. The central chamber of Holm of Papa Westray South cairn is over 20 metres long.\nBookan.\nThe Bookan type is named after a cairn found to the north-west of the Ring of Brodgar in Orkney, which is now a dilapidated oval mound, about 16 metres in diameter. Excavations in 1861 indicated a rectangular central chamber surrounded by five smaller chambers. Because of the structure's unusual design, it was originally presumed to be an early form. However, later interpretations and further excavation work in 2002 suggested that they have more in common with the later Maeshowe type rather than the stalled Orkney-Cromarty cairns.\nHuntersquoy cairn on Eday is a Bookan type cairn with an upper and lower storey.\nShetland.\nThe Shetland or Zetland group are relatively small passage graves, that are round or heel-shaped in outline. The whole chamber is cross or trefoil-shaped and there are no smaller individual compartments. An example is to be found on the uninhabited island of Vementry on the north side of the West Mainland, where it appears that the cairn may have originally been circular and its distinctive heel shape added as a secondary development, a process repeated elsewhere in Shetland. This probably served to make the cairn more distinctive and the forecourt area more defined.\nHebridean.\nLike the Shetland cairn the Hebridean group appear relatively late in the Neolithic. They are largely found in the Outer Hebrides, although a mixture of cairn types are found here. These passage graves are usually larger than the Shetland type and are round or have funnel-shaped forecourts, although a few are long cairns \u2013 perhaps originally circular but with later tails added. They often have a polygonal chamber and a short passage to one end of the cairn.\nThe Rubha an D\u00f9nain peninsula on the island of Skye provides an example from the 2nd or 3rd millennium BC. Barpa Langass on North Uist is the best preserved chambered cairn in the Hebrides.\nBargrennan.\nBargrennan chambered cairns are a class of passage graves found only in south-west Scotland, in western Dumfries and Galloway and southern Ayrshire. As well as being structurally different to the nearby Clyde cairns, Bargrennan cairns are distinguished by their siting and distribution; they are found in upland, inland areas of Galloway and Ayrshire.\nBronze Age.\nIn addition to the increasing prominence of individual burials, during the Bronze Age regional differences in architecture in Scotland became more pronounced. The Clava cairns date from this period, with about 50 cairns of this type in the Inverness area. Corrimony chambered cairn near Drumnadrochit is an example dated to 2000 BC or older. The only surviving evidence of burial was a stain indicating the presence of a single body. The cairn is surrounded by a circle of 11 standing stones. The cairns at Balnuaran of Clava are of a similar date. The largest of three is the north-east cairn, which was partially reconstructed in the 19th century and the central cairn may have been used as a funeral pyre.\nGlebe cairn in Kilmartin Glen in Argyll dates from 1700 BC and has two stone cists inside one of which a jet necklace was found during 19th century excavations. There are numerous prehistoric sites in the vicinity including Nether Largie North cairn, which was entirely removed and rebuilt during excavations in 1930.\nWales.\nChambered long cairns.\nThere are 18 Scheduled Ancient Monuments listed:"}
{"id": "7146", "revid": "19382112", "url": "https://en.wikipedia.org/wiki?curid=7146", "title": "Currency code", "text": ""}
{"id": "7147", "revid": "305540", "url": "https://en.wikipedia.org/wiki?curid=7147", "title": "Canadian whisky", "text": "Canadian whisky is a type of whisky produced in Canada. Most Canadian whiskies are blended multi-grain liquors containing a large percentage of corn spirits, and are typically lighter and smoother than other whisky styles. When Canadian distillers began adding small amounts of highly-flavourful rye grain to their mashes, people began demanding this new rye-flavoured whisky, referring to it simply as \"rye\". Today, as for the past two centuries, the terms \"rye whisky\" and \"Canadian whisky\" are used interchangeably in Canada and (as defined in Canadian law) refer to exactly the same product, which generally is made with only a small amount of rye grain.\nCharacteristics.\nHistorically, in Canada, corn-based whisky that had some rye grain added to the mash bill to give it more flavour came to be called \"rye\".\nThe regulations under Canada's \"Food and Drugs Act\" stipulate the minimum conditions that must be met in order to label a product as \"Canadian whisky\" or \"Canadian Rye Whisky\" (or \"Rye Whisky\")\u2014these are also upheld internationally through geographical indication agreements. These regulations state that whisky must \"be mashed, distilled and aged in Canada\", \"be aged in small wood vessels for not less than three years\", \"contain not less than 40 per cent alcohol by volume\" and \"may contain caramel and flavouring\". Within these parameters Canadian whiskies can vary considerably, especially with the allowance of \"flavouring\"\u2014though the additional requirement that they \"possess the aroma, taste and character generally attributed to Canadian whisky\" can act as a limiting factor.\nCanadian whiskies are most typically blends of whiskies made from a single grain, principally corn and rye, but also sometimes wheat or barley. Mash bills of multiple grains may also be used for some flavouring whiskies. The availability of inexpensive American corn, with its higher proportion of usable starches relative to other cereal grains, has led it to be most typically used to create base whiskies to which flavouring whiskies are blended in. Exceptions to this include the Highwood Distillery which specializes in using wheat and the Alberta Distillers which developed its own proprietary yeast strain that specializes in distilling rye. The flavouring whiskies are most typically rye whiskies, blended into the product to add most of its flavour and aroma. While Canadian whisky may be labelled as a \"rye whisky\" this blending technique only necessitates a small percentage (such as 10%) of rye to create the flavour, whereas much more rye is required if it were added to a mash bill alongside the more readily distilled corn.\nThe base whiskies are distilled to between 180 and 190 proof which results in few congener by-products (such as fusel alcohol, aldehydes, esters, etc.) and creates a lighter taste. By comparison, an American whisky distilled any higher than 160 proof is labelled as \"light whiskey\". The flavouring whiskies are distilled to a lower proof so that they retain more of the grain's flavour. The relative lightness created by the use of base whiskies makes Canadian whisky useful for mixing into cocktails and highballs. The minimum three year aging in small wood barrels applies to all whiskies used in the blend. As the regulations do not limit the specific type of wood that must be used, a variety of flavours can be achieved by blending whiskies aged in different types of barrels. In addition to new wood barrels, charred or uncharred, flavour can be added by aging whiskies in previously used bourbon or fortified wine barrels for different lengths of time.\nHistory.\nIn the 18th and early 19th centuries, gristmills distilled surplus grains to avoid spoilage. Most of these early whiskies would have been rough, mostly unaged wheat whiskey. Distilling methods and technologies were brought to Canada by American and European immigrants with experience in distilling wheat and rye. This early whisky from improvised stills, often with the grains closest to spoilage, was produced with various, uncontrolled proofs and was consumed, unaged, by the local market. While most distilling capacity was taken up producing rum, a result of Atlantic Canada's position in the British sugar trade, the first commercial scale production of whisky in Canada began in 1801 when John Molson purchased a copper pot still, previously used to produce rum, in Montreal. With his son Thomas Molson, and eventually partner James Morton, the Molsons operated a distillery in Montreal and Kingston and were the first in Canada to export whisky, benefiting from Napoleonic Wars' disruption in supplying French wine and brandies to England. \nGooderham and Worts began producing whisky in 1837 in Toronto as a side business to their wheat milling but surpassed Molson's production by the 1850s as it expanded their operations with a new distillery in what would become the Distillery District. Henry Corby started distilling whisky as a side business from his gristmill in 1859 in what became known as Corbyville and Joseph Seagram began working in his father-in-law's Waterloo flour mill and distillery in 1864, which he would eventually purchase in 1883. Meanwhile, Americans Hiram Walker and J.P. Wiser moved to Canada: Walker to Windsor in 1858 to open a flour mill and distillery and Wiser to Prescott in 1857 to work at his uncle's distillery where he introduced a rye whisky and was successful enough to buy the distillery five years later. The disruption of American Civil War created an export opportunity for Canadian-made whiskies and their quality, particularly those from Walker and Wiser who had already begun the practice of aging their whiskies, sustained that market even after post-war tariffs were introduced. In the 1880s, Canada's National Policy placed high tariffs on foreign alcoholic products as whisky began to be sold in bottles and the federal government instituted a bottled in bond program that provided certification of the time a whisky spent aging and allowed deferral of taxes for that period, which encouraged aging. In 1890 Canada became the first country to enact an aging law for whiskies, requiring them to be aged at least two years. The growing temperance movement culminated in prohibition in 1916 and distilleries had to either specialize in the export market or switch to alternative products, like industrial alcohols which were in demand in support of the war effort.\nWith the deferred revenue and storage costs of the Aging Law acting as a barrier to new entrants and the reduced market due to prohibition, consolidation of Canadian whisky had begun. Henry Corby Jr. modernized and expanded upon his father's distillery and sold it, in 1905, to businessman Mortimer Davis who also purchased the Wiser distillery, in 1918, from the heirs of J.P. Wiser. Davis's salesman Harry Hatch spent time promoting the Corby and Wiser brands and developing a distribution network in the United States which held together as Canadian prohibition ended and American prohibition began. After Hatch's falling out with Davis, Hatch purchased the struggling Gooderham and Worts in 1923 and switched out Davis's whisky for his. Hatch was successful enough to be able to also purchase the Walker distillery, and the popular Canadian Club brand, from Hiram's grandsons in 1926. While American prohibition created risk and instability in the Canadian whisky industry, some benefited from purchasing unused American distillation equipment and from sales to exporters (nominally to foreign countries like Saint Pierre and Miquelon, though actually to bootleggers to the United States). Along with Hatch, the Bronfman family was able to profit from making whisky destined for United States during prohibition, though mostly in Western Canada and were able to open a distillery in LaSalle, Quebec and merge their company, in 1928, with Seagram's which had struggled with transitioning to the prohibition marketplace. Samuel Bronfman became president of the company and, with his dominant personality, began a strategy of increasing their capacity and aging whiskies in anticipation of the end of prohibition. When that did occur, in 1933, Seagram's was in a position to quickly expand; they purchased The British Columbia Distilling Company from the Riefel family in 1935, as well as several American distilleries and introduced new brands, one of them being Crown Royal, in 1939, which would eventually become one of the best-selling Canadian whiskies.\nWhile some capacity was switched to producing industrial alcohols in support of the country's World War II efforts, the industry expanded again after the war until the 1980s. In 1945, Schenley Industries purchased one of those industrial alcohol distilleries in Valleyfield, Quebec, and repurposed several defunct American whiskey brands, like Golden Wedding, Old Fine Copper, and starting in 1972, Gibson's Finest. Seeking to secure their supply of Canadian whisky, Barton Brands also built a new distillery in Collingwood, Ontario, in 1967, where they would produce Canadian Mist, though they sold the distillery and brand only four years later to Brown\u2013Forman. As proximity to the shipping routes (by rail and boat) to the US became less important, large distilleries were established in Alberta and Manitoba. Five years after starting to experiment with whiskies in their Toronto gin distillery, W. &amp; A. Gilbey Ltd. created the Black Velvet blend in 1951 which was so successful a new distillery in Lethbridge, Alberta was constructed in 1973 to produce it. \nAlso in the west, a Calgary-based business group recruited the Riefels from British Columbia to oversee their Alberta Distillers operations in 1948. The company became an innovator in the practice of bulk shipping whiskies to the United States for bottling and the success of their Windsor Canadian brand (produced in Alberta but bottled in the United States) led National Distillers Limited to purchase Alberta Distillers, in 1964, to secure their supply chain. More Alberta investors founded the Highwood Distillery in 1974 in High River, Alberta, which specialized in wheat-based whiskies. Seagram's opened a large, new plant in Gimli, Manitoba, in 1969, which would eventually replace their Waterloo and LaSalle distilleries. In British Columbia, Ernie Potter who had been producing fruit liqueurs from alcohols distilled at Alberta Distillers built his own whisky distillery in Langley in 1958 and produced the Potter's and Century brands of whisky. Hiram Walker's built the Okanagan Distillery in Winfield, British Columbia, in 1970 with the intention of producing Canadian Club but was redirected to fulfill contracts to produce whiskies for Suntory before being closed in 1995.\nAfter decades of expansion, a shift in consumer preferences towards white spirits (such as vodka) in the American market resulted in an excess supply of Canadian whiskies. While this allowed the whiskies to be aged longer, the unexpected storage costs and deferred revenue strained individual companies. With the distillers seeking investors and multinational corporations seeking value brands, a series of acquisitions and mergers occurred. Alberta Distillers was bought in 1987 by Fortune Brands which would go on to become part of Beam Suntory. Hiram Walker was sold in 1987 to Allied Lyons which Pernod Ricard took over in 2006, with Fortune Brands acquiring the Canadian Club brand. Grand Metropolitan had purchased Black Velvet in 1972 but sold the brand in 1999 to Constellation Brands who in turn sold it to Heaven Hill in 2019. Schenley was acquired in 1990 by United Distillers which would go on to become part of Diageo, though Gibson's Finest was sold to William Grant &amp; Sons in 2001. Seagram's was sold in 2000 to Vivendi, which in turn sold its various brands and distilleries to Pernod Ricard and Diageo. Highwood would purchase Potter's in 2006. Despite the consolidation, the Kittling Ridge Distillery in Grimsby, Ontario, began to produce the Forty Creek brand, though it was sold to the Campari Group in 2014. Later, the Sazerac Company would purchase the brands Seagram's VO, Canadian 83 and Five Star from Diageo in 2018.\nIllicit export to the United States.\nCanadian whisky featured prominently in rum-running into the U.S. during Prohibition. Hiram Walker's distillery in Windsor, Ontario, directly across the Detroit River and the international boundary between Canada and the United States, easily served bootleggers using small, fast smuggling boats.\nDistilleries and brands.\nThe following is a listing of distilleries presently producing Canadian whiskys:\nAlberta.\nThere are several distilleries based in Alberta, including the Alberta Distillers, established in 1946 in Calgary, Alberta. The distillery was purchased in 1987 by Fortune Brands which became Beam Suntory in 2011. The distillery uses a specific strain of yeast which they developed that specializes in fermenting rye. While the distillery exports much of its whisky for bottling in other countries, they also produce the brands Alberta Premium, Alberta Springs, Windsor Canadian, Tangle Ridge, and Canadian Club Chairman's Select.\nBlack Velvet Distillery (formerly the Palliser Distillery) was established in 1973 in Lethbridge, Alberta it has been owned by Heaven Hill since 2019. They produce the Black Velvet brand which is mostly shipped in bulk for bottling in the American market, with some bottled onsite for the Canadian market. The distillery also produces Danfield's and the Schenley's Golden Wedding and OFC labels.\nHighwood Distillery (formerly the Sunnyvale Distillery) was established in 1974 in High River, Alberta, the Highwood Distillery specializes in using wheat in their base whiskies. This distillery also produces vodka, rum, gin and liqueurs. Brands of Canadian whisky produced at the Highwood Distillery include Centennial, Century, Ninety, and Potter's. They also produce White Owl whisky which is charcoal-filtered to remove the colouring introduced by aging in wood barrels.\nManitoba.\nGimli Distillery was established in 1968 in Gimli, Manitoba, to produce Seagram brands, the distillery was acquired by Diageo in 2001. The Gimli Distillery is responsible for producing Crown Royal, the best-selling Canadian whisky in the world with 7 million cases shipped in 2017. They also supply some of the whisky used in Seagram's VO and other blends.\nOntario.\nDistilleries were established in Ontario during the mid-19th century, with Gooderham and Worts's beginning operations in Toronto's Distillery District in the 1830s. Distilleries continued to operate from the Distillery District until 1990, when the area was reoriented towards commercial and residential development. Other former distilleries in the province includes one in Corbyville, which hosted a distillery operated by Corby Spirit and Wine. A distillery in Waterloo was operated by Seagram to produce Crown Royal until 1992; although the company still maintains a blending and bottling plant in Amherstburg.\nPresently, there are several major distilleries based in Ontario. The oldest functioning distillery in Ontario is the Hiram Walker Distillery, established in 1858 in Windsor, Ontario, but modernized and expanded upon several times since. The distillery is owned by Pernod Ricard and operated by Corby Spirit and Wine, of which Pernod has a controlling share. Brands produced at the Walker Distillery include Lot 40, Pike Creek, Gooderham and Worts, Hiram Walker's Special Old, Corby's Royal Reserve, and J.P. Wiser's brands. Most of its capacity is used for contract production of the Beam Suntory brand (and former Hiram Walker brand) Canadian Club, in addition to generic Canadian whisky that is exported in bulk and bottled under various labels in other countries.\nCanadian Mist Distillery was established in 1967 in Collingwood, Ontario, the distillery is owned by Brown\u2013Forman and primarily produces the Canadian Mist brand for export, in bulk at barrel strength, to the company's bottling plant in Louisville, Kentucky. The distillery also produces whiskies used in the Collingwood brand, introduced 2011, and the Bearface brand, introduced 2018.\nKittling Ridge Distillery was established in 1992 with an associated winery in Grimsby, Ontario, its first whiskies came to market in 2002. The distillery was purchased in 2014 by Campari Group. The distillery produces the Forty Creek brand.\nQuebec.\nOld Montreal Distillery was established in 1929 as a Corby Spirit and Wine distillery, it was acquired by Sazerac Company in 2011 and modernized in 2018. It produces Sazerac brands and has taken over bottling of Caribou Crossing.\nValleyfield Distillery (formerly the Schenley Distillery) was established in 1945 in a former brewery in Salaberry-de-Valleyfield, Quebec, near Montreal, the distillery has been owned by Diageo in 2008. Seagram's VO is bottled here with flavouring whisky from the Gimli Distillery. Otherwise, the Valleyfield Distillery specializes in producing base whiskies distilled from corn for other Diageo products."}
{"id": "7148", "revid": "41345048", "url": "https://en.wikipedia.org/wiki?curid=7148", "title": "Collective noun", "text": "In linguistics, a collective noun is a collection of things taken as a whole. Most collective nouns in everyday speech are not specific to one kind of thing, such as the word \"group\", which can be applied to people (\"a group of people\") or dogs (\"a group of dogs\") or other things. Some collective nouns are specific to one kind of thing, especially terms of venery, which identify groups of specific animals. For example, \"pride\" as a term of venery always refers to lions, never to dogs or cows. Other examples come from popular culture such as a group of owls, which is called a \"parliament\".\nDifferent forms of English handle verb agreement with collective count nouns differently. For example, users of British English generally accept that collective nouns take either singular or plural verb forms depending on context and the metonymic shift that it implies.\nDerivation.\nMorphological derivation accounts for many collective words and various languages have common affixes for denoting collective nouns. Because derivation is a slower and less productive word formation process than the more overtly syntactical morphological methods, there are fewer collectives formed this way. As with all derived words, derivational collectives often differ semantically from the original words, acquiring new connotations and even new denotations.\nAffixes.\nProto-Indo-European.\nEarly Proto-Indo-European used the suffix *eh\u2082 to form collective nouns, which evolved into the Latin neuter plural ending -a. Late Proto-Indo-European used the ending *t, which evolved into the English ending -th, as in \"youth\".\nEnglish.\nThe English endings \"-age\" and \"-ade\" often signify a collective. Sometimes, the relationship is easily recognizable: \"baggage, drainage, blockade\". Though the etymology is plain to see, the derived words take on a distinct meaning. This is a productive ending, as evidenced in the recent coin, \"signage\".\nGerman.\nGerman uses the prefix \"ge-\" to create collectives. The root word often undergoes umlaut and suffixation as well as receiving the \"ge-\" prefix. Nearly all nouns created in that way are of neuter gender:\nThere are also several endings that can be used to create collectives, such as \"welt\" and \"masse\".\nDutch.\nDutch has a similar pattern but sometimes uses the (unproductive) circumfix \":\nSwedish.\nThe following Swedish example has different words in the collective form and in the individual form:\nEsperanto.\nEsperanto uses the collective infix -\"ar\" to produce a large number of derived words:\nMetonymic merging of grammatical number.\nTwo good examples of collective nouns are \"team\" and \"government\", which are both words referring to groups of (usually) people. Both \"team\" and \"government\" are count nouns (consider: \"one team\", \"two teams\", \"most teams\"; \"one government\", \"two governments\", \"many governments\"). \nAgreement in different forms of English.\nConfusion often stems from the way that different forms of English handle agreement with collective nouns\u2014specifically, whether or not to use the collective singular: the singular verb form with a collective noun. The plural verb forms are often used in British English with the singular forms of these count nouns (e.g., \"The team \"have\" finished the project.\"). Conversely, in the English language as a whole, singular verb forms can often be used with nouns ending in \"-s\" that were once considered plural (e.g., \"Physics \"is\" my favorite academic subject\"). This apparent \"number mismatch\" is a natural and logical feature of human language, and its mechanism is a subtle metonymic shift in the concepts underlying the words.\nIn British English, it is generally accepted that collective nouns can take either singular or plural verb forms depending on the context and the metonymic shift that it implies. For example, \"the team \"is\" in the dressing room\" (\"formal agreement\") refers to \"the team\" as an ensemble, while \"the team \"are\" fighting among themselves\" (\"notional agreement\") refers to \"the team\" as individuals. That is also the British English practice with names of countries and cities in sports contexts (e.g., \"Newcastle \"have\" won the competition.\").\nIn American English, collective nouns almost always take singular verb forms (formal agreement). In cases that a metonymic shift would be revealed nearby, the whole sentence should be recast to avoid the metonymy. (For example, \"The team are fighting among themselves\" may become \"the team \"members\" are fighting among themselves\" or simply \"The team is infighting.\") Collective proper nouns are usually taken as singular (\"Apple is expected to release a new phone this year\"), unless the plural is explicit in the proper noun itself, in which case it is taken as plural (\"The Green Bay Packers are scheduled to play the Minnesota Vikings this weekend\"). More explicit examples of collective proper nouns include \"General Motors is once again the world's largest producer of vehicles,\" and \"Texas Instruments is a large producer of electronics here,\" and \"British Airways is an airline company in Europe.\" Furthermore, \"American Telephone &amp; Telegraph is a telecommunications company in North America.\" Such phrases might look plural, but they are not.\nExamples of metonymic shift.\nA good example of such a metonymic shift in the singular-to-plural direction (which, generally, occurs only in British English) is the following sentence: \"The team have finished the project.\" In that sentence, the underlying thought is of the individual members of the team working together to finish the project. Their accomplishment is collective, and the emphasis is not on their individual identities, but they are still discrete individuals; the word choice \"team have\" manages to convey both their collective and discrete identities simultaneously. Collective nouns that have a singular form but take a plural verb form are called collective plurals. A good example of such a metonymic shift in the plural-to-singular direction is the following sentence: \"Mathematics is my favorite academic subject.\" The word \"mathematics\" may have originally been plural in concept, referring to mathematic endeavors, but metonymic shift (the shift in concept from \"the endeavors\" to \"the whole set of endeavors\") produced the usage of \"mathematics\" as a singular entity taking singular verb forms. (A true mass-noun sense of \"mathematics\" followed naturally.)\nNominally singular pronouns can be collective nouns taking plural verb forms, according to the same rules that apply to other collective nouns. For example, it is correct usage in both British English and American English usage to say: \"None are so fallible as those who are sure they're right.\" In that case, the plural verb is used because the context for \"none\" suggests more than one thing or person. This also applies to the use of an adjective as a collective noun: \"The British are coming!\"; \"The poor will always be with you.\"\nOther examples include:\nThis does not, however, affect the tense later in the sentence:\nExceptions to the rule include singular group names preceded by an article:\nThis exception does not apply to artists whose name is preceded by an article, but ends in a singular:\nAbbreviations provide other \"exceptions\" in American usage concerning plurals:\nWhen only the name is plural but not the object, place, or person:\nTerms of venery.\nThe tradition of using \"terms of venery\" or \"nouns of assembly\", collective nouns that are specific to certain kinds of animals, stems from an English hunting tradition of the Late Middle Ages. The fashion of a consciously developed hunting language came to England from France. It was marked by an extensive proliferation of specialist vocabulary, applying different names to the same feature in different animals. The elements can be shown to have already been part of French and English hunting terminology by the beginning of the 14th century. In the course of the 14th century, it became a courtly fashion to extend the vocabulary, and by the 15th century, the tendency had reached exaggerated and even satirical proportions.\n\"The Treatise\", written by Walter of Bibbesworth in the mid-1200s, is the earliest source for collective nouns of animals in any European vernacular (and also the earliest source for animal noises). The \"Venerie\" of Twiti (early 14th century) distinguished three types of droppings of animals, and three different terms for herds of animals. Gaston Phoebus (14th century) had five terms for droppings of animals, which were extended to seven in the \"Master of the Game\" (early 15th century). The focus on collective terms for groups of animals emerged in the later 15th century. Thus, a list of collective nouns in Egerton MS 1995, dated to c. 1452 under the heading of \"termis of venery &amp;c.\", extends to 70 items, and the list in the \"Book of Saint Albans\" (1486) runs to 164 items, many of which, even though introduced by \"the compaynys of beestys and fowlys\", relate not to venery but to human groups and professions and are clearly humorous, such as \"a Doctryne of doctoris\", \"a Sentence of Juges\", \"a Fightyng of beggers\", \"an uncredibilite of Cocoldis\", \"a Melody of harpers\", \"a Gagle of women\", \"a Disworship of Scottis\", etc.\nThe \"Book of Saint Albans\" became very popular during the 16th century and was reprinted frequently. Gervase Markham edited and commented on the list in his \"The Gentleman's Academic\", in 1595. The book's popularity had the effect of perpetuating many of these terms as part of the Standard English lexicon even if they were originally meant to be humorous and have long ceased to have any practical application.\nEven in their original context of medieval venery, the terms were of the nature of kennings, intended as a mark of erudition of the gentlemen able to use them correctly rather than for practical communication. The popularity of the terms in the modern period has resulted in the addition of numerous lighthearted, humorous or facetious collective nouns."}
{"id": "7158", "revid": "122969", "url": "https://en.wikipedia.org/wiki?curid=7158", "title": "Carat (mass)", "text": "The carat (ct) is a unit of mass equal to or 0.00643 troy oz, and is used for measuring gemstones and pearls.\nThe current definition, sometimes known as the metric carat, was adopted in 1907 at the Fourth General Conference on Weights and Measures, and soon afterwards in many countries around the world. The carat is divisible into 100 \"points\" of 2\u00a0mg. Other subdivisions, and slightly different mass values, have been used in the past in different locations.\nIn terms of diamonds, a paragon is a flawless stone of at least 100 carats (20\u00a0g).\nThe ANSI X.12 EDI standard abbreviation for the carat is CD.\nEtymology.\nFirst attested in English in the mid-15th century, the word \"carat\" comes from Italian \"carato\", which comes from Arabic \"q\u012br\u0101\u1e6d\" \u0642\u064a\u0631\u0627\u0637, in turn borrowed from Greek \"ker\u00e1tion\" \u03ba\u03b5\u03c1\u03ac\u03c4\u03b9\u03bf\u03bd 'carob seed', a diminutive of \"keras\" 'horn'. It was a unit of weight, equal to 1/1728 (1/12) of a pound (see Mina (unit)).\nHistory.\nCarob seeds have been used throughout history to measure jewelry, because it was believed that there was little variance in their mass distribution. However, this was a factual inaccuracy, as their mass varies about as much as seeds of other species.\nIn the past, each country had its own carat. It was often used for weighing gold. Beginning in the 1570s, it was used to measure weights of diamonds.\nStandardization.\nAn 'international carat' of 205 milligrams was proposed in 1871 by the Syndical Chamber of Jewellers, etc., in Paris, and accepted in 1877 by the Syndical Chamber of Diamond Merchants in Paris. A metric carat of 200 milligrams \u2013 exactly one-fifth of a gram \u2013 had often been suggested in various countries, and was finally proposed by the International Committee of Weights and Measures, and unanimously accepted at the fourth sexennial General Conference of the Metric Convention held in Paris in October 1907. It was soon made compulsory by law in France, but uptake of the new carat was slower in England, where its use was allowed by the Weights and Measures (Metric System) Act of 1897.\nHistorical definitions.\nUK Board of Trade.\nIn the United Kingdom the original Board of Trade carat was exactly grains (~3.170\u00a0grains = ~205\u00a0mg); in 1888, the Board of Trade carat was changed to exactly grains (~3.168\u00a0grains = ~205\u00a0mg). Despite its being a non-metric unit, a number of metric countries have used this unit for its limited range of application.\nThe Board of Trade carat was divisible into four \"diamond grains\", but measurements were typically made in multiples of carat.\nRefiners' carats.\nThere were also two varieties of \"refiners' carats\" once used in the United Kingdom \u2014 the pound carat and the ounce carat. The pound troy was divisible into 24 \"pound carats\" of 240 grains troy each; the pound carat was divisible into four \"pound grains\" of 60 grains troy each; and the pound grain was divisible into four \"pound quarters\" of 15 grains troy each. Likewise, the ounce troy was divisible into 24 \"ounce carats\" of 20 grains troy each; the ounce carat was divisible into four \"ounce grains\" of 5 grains troy each; and the ounce grain was divisible into four \"ounce quarters\" of grains troy each.\nGreco-Roman.\nThe \"solidus\" was also a Roman weight unit. There is literary evidence that the weight of 72\u00a0coins of the type called \"solidus\" was exactly 1 Roman pound, and that the weight of 1\u00a0\"solidus\" was 24\u00a0\"siliquae\". The weight of a Roman pound is generally believed to have been 327.45\u00a0g or possibly up to 5\u00a0g less. Therefore, the metric equivalent of 1 \"siliqua\" was approximately 189\u00a0mg. The Greeks had a similar unit of the same value.\nGold fineness in carats comes from carats and grains of gold in a solidus of coin. The conversion rates 1\u00a0solidus = 24\u00a0carats, 1\u00a0carat = 4\u00a0grains still stand. Woolhouse's \"Measures, Weights and Moneys of all Nations\" gives gold fineness in carats of 4 grains, and silver in pounds of 12\u00a0troy ounces of 20\u00a0pennyweight each."}
{"id": "7160", "revid": "1008730806", "url": "https://en.wikipedia.org/wiki?curid=7160", "title": "European Conference of Postal and Telecommunications Administrations", "text": "The European Conference of Postal and Telecommunications Administrations (CEPT) was established on June 26, 1959 by nineteen European states in Montreux, Switzerland, as a coordinating body for European state telecommunications and postal organizations. The acronym comes from the French version of its name \"Conf\u00e9rence europ\u00e9enne des administrations des postes et des t\u00e9l\u00e9communications\".\nCEPT was responsible for the creation of the European Telecommunications Standards Institute (ETSI) in 1988.\nCEPT is organised into three main components:\nMember countries.\n\"As of March 2019 : 48 countries.\" \nAlbania, Andorra, Austria, Azerbaijan, Belarus, Belgium, Bosnia and Herzegovina, Bulgaria, Croatia, Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Georgia, Germany, Greece, Hungary, Iceland, Ireland, Italy, Latvia, Liechtenstein, Lithuania, Luxembourg, Malta, Moldova, Monaco, Montenegro, Netherlands, North Macedonia, Norway, Poland, Portugal, Romania, Russian Federation, San Marino, Serbia, Slovak Republic, Slovenia, Spain, Sweden, Switzerland, Turkey, Ukraine, United Kingdom, Vatican City,"}
{"id": "7161", "revid": "1234701", "url": "https://en.wikipedia.org/wiki?curid=7161", "title": "Chain termination method", "text": ""}
{"id": "7162", "revid": "9238235", "url": "https://en.wikipedia.org/wiki?curid=7162", "title": "Tramlink", "text": "Tramlink, previously Croydon Tramlink, is a light rail tram system serving Croydon and surrounding areas in South London, England. It began operation in 2000, the first tram system in London since 1952. It is owned by London Trams, part of Transport for London (TfL), and has been operated by FirstGroup since 2017.\nThe network consists of 39 stops along of track, on a mixture of street track shared with other traffic, dedicated track in public roads, and off-street track consisting of new rights-of-way, former railway lines, and one right-of-way where the Tramlink track runs parallel to a third rail-electrified Network Rail line.\nThe network's lines coincide in central Croydon, with eastbound termini at Beckenham Junction, Elmers End and New Addington, and a westbound terminus at Wimbledon, where there is an interchange for London Underground. Tramlink is the fourth-busiest light rail network in the UK behind the Docklands Light Railway, Manchester Metrolink and Tyne &amp; Wear Metro.\nHistory.\nInception.\nIn the first half of the 20th century, Croydon had many tramlines. The first to close was the Addiscombe \u2013 East Croydon Station route through George Street to Cherry Orchard Road in 1927 and the last was the Purley - Embankment and Croydon (Coombe Road) - Thornton Heath routes closed April 1951. However, in the Spring of 1950, the Highways Committee were presented by the Mayor with the concept of running trams between East Croydon station and the new estate being constructed at New Addington. This was based on the fact that the Feltham cars used in Croydon were going to Leeds to serve their new estates on reserved tracks. In 1962, a private study with assistance from BR engineers, showed how easy it was to convert the West Croydon - Wimbledon train service to tram operation and successfully prevent conflict between trams and trains.\nThese two concepts became joined in joint LRTL/TLRS concept of New Addington to Wimbledon every 15 minutes via East and West Croydon and Mitcham plus New Addington to Tattenham Corner every 15 minutes via East and West Croydon, Sutton and Epsom Downs. A branch into Forestdale to give an overlap service from Sutton was also included. During the 1970s, several BR directors and up-and-coming managers were aware of the advantages. Chris Green, upon becoming Managing Director, Network South East, published his plans in 1987 expanding the concept to take in the Tattenham Corner and Caterham branches and provide a service from Croydon to Lewisham via Addiscombe and Hayes. Working with Tony Ridley, then Managing Director, London Transport, the scheme was brought out into the open with Scott Mackintosh being appointed Light Rail Manager in 1989.\nThe scheme was accepted in principle in February 1990 by Croydon Council who worked with what was then London Regional Transport (LRT) to propose Tramlink to Parliament. The Croydon Tramlink Act 1994 resulted, which gave LRT the power to build and run Tramlink.\nConstruction.\nIn 1995 four consortia were shortlisted to build, operate and maintain Tramlink:\nIn 1996 Tramtrack Croydon (TC) won a 99-year Private Finance Initiative (PFI) contract to design, build, operate and maintain Tramlink. The equity partners in TC were Amey (50%), Royal Bank of Scotland (20%), 3i (20%) and Sir Robert McAlpine with Bombardier Transportation contracted to build and maintain the trams and FirstGroup operate the service. TC retained the revenue generated by Tramlink and LRT had to pay compensation to TC for any changes to the fares and ticketing policy introduced later.\nConstruction work started in January 1997, with an expected opening in November 1999. The first tram was delivered in October 1998 to the new Depot at Therapia Lane and testing on the sections of the Wimbledon line began shortly afterwards.\nOpening.\nThe official opening of Tramlink took place on 10 May 2000 when route 3 from Croydon to New Addington opened to the public. Route 2 from Croydon to Beckenham Junction followed on 23 May 2000, and route 1 from Elmers End to Wimbledon opened a week later on 30 May 2000.\nBuyout by Transport for London.\nIn March 2008, TfL announced that it had reached agreement to buy TC for \u00a398 million. The purchase was finalised on 28 June 2008. The background to this purchase relates to the requirement that TfL (who took over from London Regional Transport in 2000) compensates TC for the consequences of any changes to the fares and ticketing policy introduced since 1996. In 2007 that payment was \u00a34m, with an annual increase in rate. FirstGroup continues to operate the service.\nIn October 2008 TfL introduced a new livery, using the blue, white and green of the routes on TfL maps, to distinguish the trams from buses operating in the area. The colour of the cars was changed to green, and the brand name was changed from Croydon Tramlink to simply Tramlink. These refurbishments were completed in early 2009.\nAdditional stop and trams.\nCentrale tram stop, in Tamworth Road on the one-way central loop, opened on 10 December 2005, increasing journey times slightly. As turnround times were already quite tight, this raised the issue of buying an extra tram to maintain punctuality. Partly for this reason but also to take into account the planned restructuring of services (subsequently introduced in July 2006), TfL issued tenders for a new tram. However, nothing resulted from this.\nIn January 2011, Tramtrack Croydon opened a tender for the supply of ten new or second-hand trams from the end of summer 2011, for use between Therapia Lane and Elmers End. On 18 August 2011, TfL announced that Stadler Rail had won a $19.75 million contract to supply six Variobahn trams similar to those used by Bybanen in Bergen, Norway. They entered service in 2012. In August 2013, TfL ordered an additional four Variobahns for delivery in 2015, for use on the Wimbledon to Croydon link, an order later increased to six. This brought the total Variobahn fleet up to ten in 2015, and twelve in 2016 when the final two trams were delivered.\nCurrent network.\nStops.\nThere are 39 stops, with 38 opened in the initial phase, and Centrale tram stop added on 10 December 2005. Most stops are long. They are virtually level with the doors and are all wider than . This allows wheelchairs, prams, pushchairs and the elderly to board the tram easily with no steps. In street sections, the stop is integrated with the pavement.\nThe tram stops have low platforms, above rail level. They are unstaffed and had automated ticket machines that are no longer in use due to TfL making trams cashless. In general, access between the platforms involves crossing the tracks by pedestrian level crossing.\nTramlink uses some former main-line stations on the Wimbledon\u2013West Croydon and Elmers End\u2013Coombe Lane stretches of line. The railway platforms have been demolished and rebuilt to Tramlink specifications, except at Elmers End and Wimbledon where the track level was raised to meet the higher main-line platforms to enable cross-platform interchange.\nAll stops have disabled access, raised paving, CCTV, a Passenger Help Point, a Passenger Information Display (PID), litter bins, a ticket machine, a noticeboard and lamp-posts, and most also have seats and a shelter.\nThe PIDs display the destinations and expected arrival times of the next two trams. They can also display any message the controllers want to display, such as information on delays or even safety instructions for vandals to stop putting rubbish or other objects onto the track.\nRoutes.\nTramlink has been shown on the principal tube map since 1 June 2016, having previously appeared only on the \"London Connections\" map.\nWhen Tramlink first opened it had three routes: Line 1 (yellow) from Wimbledon to Elmers End, Line 2 (red) from Croydon to Beckenham Junction, and Line 3 (green) from Croydon to New Addington. On 23 July 2006 the network was restructured, with Route 1 from Elmers End to Croydon, Route 2 from Beckenham Junction to Croydon and Route 3 from New Addington to Wimbledon. On 25 June 2012 Route 4 from Therapia Lane to Elmers End was introduced. On Monday 4 April 2016, Route 4 was extended from Therapia Lane to Wimbledon.\nOn 25 February 2018, the network and timetables were restructured again for more even and reliable services. As part of this change, trams would no longer display route numbers on their dot matrix destination screens. This resulted in three routes:\nAdditionally, the first two trams from New Addington will run to Wimbledon. Overall, this would result in a decrease in 2tph leaving Elmers End, resulting in a 25% decrease in capacity here, and 14% in the Addiscombe area. However, this would also regulate waiting times in this area and on the Wimbledon branch to every 5 minutes, from every 2\u20137 minutes.\nFormer lines reused.\nTramlink makes use of a number of National Rail lines, running parallel to franchised services, or in some cases, runs on previously abandoned railway corridors. Between Birkbeck and Beckenham Junction, Tramlink uses the Crystal Palace line, running on a single track alongside the track carrying Southern rail services. The National Rail track had been singled some years earlier.\nFrom Elmers End to Woodside, Tramlink follows the former Addiscombe Line. At Woodside, the old station buildings stand disused, and the original platforms have been replaced by accessible low platforms. Tramlink then follows the former Woodside and South Croydon Railway (W&amp;SCR) to reach the current Addiscombe tram stop, adjacent to the site of the demolished Bingham Road railway station. It continues along the former railway route to near Sandilands, where Tramlink curves sharply towards Sandilands tram stop. Another route from Sandilands tram stop curves sharply on to the W&amp;SCR before passing through Park Hill (or Sandilands) tunnels and to the site of Coombe Road station after which it curves away across Lloyd Park.\nBetween Wimbledon station and Wandle Park, Tramlink follows the former West Croydon to Wimbledon Line, which was first opened in 1855 and closed on 31 May 1997 to allow for conversion into Tramlink. Within this section, from near Phipps Bridge to near Reeves Corner, Tramlink follows the Surrey Iron Railway, giving Tramlink a claim to one of the world's oldest railway alignments. Beyond Wandle Park, a Victorian footbridge beside Waddon New Road was dismantled to make way for the flyover over the West Croydon to Sutton railway line. The footbridge has been re-erected at Corfe Castle station on the Swanage Railway (although some evidence suggests that this was a similar footbridge removed from the site of Merton Park railway station).\nFeeder buses.\nBus routes T31, T32 and T33 used to connect with Tramlink at the New Addington, Fieldway and Addington Village stops. T31 and T32 no longer run, and T33 has been renumbered as 433.\nRolling stock.\nCurrent fleet.\nTramlink currently uses 35 trams. In summary:\nBombardier CR4000.\nThe original fleet comprised 24 articulated low floor Bombardier Flexity Swift CR4000 trams built in Vienna numbered beginning at 2530, continuing from the highest-numbered tram 2529 on London's former tram network, which closed in 1952. The original livery was red and white. One (2550) was painted in FirstGroup white, blue and pink livery. In 2006, the CR4000 fleet was refreshed, with the bus-style destination roller blinds being replaced with a digital dot-matrix display. In 2008/09 the fleet was repainted externally in the new green livery and the interiors were refurbished with new flooring, seat covers retrimmed in a new moquette and stanchions repainted from yellow to green. One (2551) has not returned to service after the fatal accident on 9 November 2016.\nCroydon Variobahn.\nIn January 2011, Tramtrack Croydon invited tenders for the supply of ten new or second-hand trams, and on 18 August 2011, TfL announced that Stadler Rail had won a $19.75 million contract to supply six Variobahn trams similar to those used by Bybanen in Bergen, Norway. They entered service in 2012. In August 2013, TfL ordered an additional four Variobahn trams for delivery in 2015, an order which was later increased to six. This brought the total Variobahn fleet up to ten in 2015, and twelve in 2016 when the final two trams were delivered.\nAncillary vehicles.\nEngineers' vehicles used in Tramlink construction were hired for that purpose.\nIn November 2006 Tramlink purchased five second-hand engineering vehicles from Deutsche Bahn. These were two engineers' trams (numbered 058 and 059 in Tramlink service), and three 4-wheel wagons (numbered 060, 061, and 062). Service tram 058 and trailer 061 were both sold to the National Tramway Museum in 2010.\nFares and ticketing.\nTfL Bus &amp; Tram Passes are valid on Tramlink, as are Travelcards that include any of zones 3, 4, 5 and 6.\nPay-as-you-go Oyster Card fares are the same as on London Buses, although special fares may apply when using Tramlink feeder buses.\nWhen using Oyster cards, passengers must touch in on the platform before boarding the tram. Special arrangements apply at Wimbledon station, where the Tramlink stop is within the National Rail and London Underground station. Tramlink passengers must therefore touch in at the station entry barriers then again at the Tramlink platform to inform the system that no mainline/LUL rail journey has been made.\nEMV contactless payment cards can also be used to pay for fares in the same manner as Oyster cards.\nTicket machines were withdrawn on 16 July 2018.\nServices.\nOnboard announcements.\nThe onboard announcements are by BBC News reader (and tram enthusiast) Nicholas Owen. The announcement pattern is as follows: e.g. \"This tram is for Wimbledon; the next stop will be Merton Park.\"\nCorporate affairs.\nOwnership and structure.\nThe service was created as a result of the Croydon Tramlink Act 1994 that received Royal Assent on 21 July 1994, a Private Bill jointly promoted by London Regional Transport (the predecessor of Transport for London (TfL)) and Croydon London Borough Council. Following a competitive tender, a consortium company Tramtrack Croydon Limited (incorporated in 1995) was awarded a 99-year concession to build and run the system. Since 28 June 2008, the company has been a subsidiary of TfL.\nTramlink is currently operated by Tram Operations Ltd (TOL), a subsidiary of FirstGroup, who have a contract to operate the service until 2030. TOL provides the drivers and management to operate the service; the infrastructure and trams are owned and maintained by a TfL subsidiary.\nBusiness trends.\nThe key trends in recent years for Tramlink are (years ending 31 March):\nPassenger numbers.\nDetailed passenger journeys since Tramlink commenced operations in May 2000 were:\nFuture developments.\nSutton Link.\nAs of 2020, the only extension actively being pursued by the Mayor of London and TfL is a new line to Sutton from Wimbledon or Colliers Wood, known as the Sutton Link.\nIn July 2013, then-Mayor Boris Johnson had affirmed that there was a reasonable business case for Tramlink to cover the Wimbledon - Sutton corridor, which might also include a loop via St Helier Hospital and an extension to The Royal Marsden Hospital. In 2014, a proposed \u00a3320m scheme for a new line to connect Wimbledon to Sutton via Morden was made and brought to consultation jointly by the London Boroughs of Merton and Sutton. Although \u00a3100m from TfL was initially secured in the draft 2016/17 budget, this was subsequently reallocated.\nIn 2018, TfL opened a consultation on proposals for a connection to Sutton, with three route options: from South Wimbledon, from Colliers Wood (both having an option of a bus rapid transit route or a tram line) or from Wimbledon (only as a tram line). In February 2020, following the consultation, TfL announced their preference for a north\u2013south tramway between Colliers Wood and Sutton town centre, with a projected cost of \u00a3425m, on the condition of securing additional funding.\nPrevious proposals.\nNumerous extensions to the network have been discussed or proposed over the years, involving varying degrees of support and investigative effort.\nIn 2002, as part of The Mayor's Transport Strategy for London, a number of proposed extensions were identified, including to Sutton from Wimbledon or Mitcham; to Crystal Palace; to Colliers Wood/Tooting; and along the A23. The Strategy said that \"extensions to the network could, in principle, be developed at relatively modest cost where there is potential demand...\" and sought initial views on the viability of a number of extensions by summer 2002.\nIn 2006, in a TfL consultation on an extension to Crystal Palace, three options were presented: on-street, off-street and a mixture of the two. After the consultation, the off-street option was favoured, to include Crystal Palace Station and Crystal Palace Parade. TfL stated in 2008 that due to lack of funding the plans for this extension would not be taken forward. They were revived shortly after Boris Johnson's re-election as Mayor in May 2012, but six months later they were cancelled again.\nIn November 2014, a 15-year plan, Trams 2030, called for upgrades to increase capacity on the network in line with an expected increase in ridership to 60 million passengers by 2031 (although the passenger numbers at the time (2013/14: 31.2 million) have not been exceeded since (as at 2019)).\nThe upgrades were to improve reliability, support regeneration in the Croydon metropolitan centre, and future-proof the network for Crossrail 2, a potential Bakerloo line extension, and extensions to the tram network itself to a wide variety of destinations. The plans involve dual-tracking across the network and introducing diverting loops on either side of Croydon, allowing for a higher frequency of trams on all four branches without increasing congestion in central Croydon. The \u00a3737m investment was to be funded by the Croydon Growth Zone, TfL Business Plan, housing levies, and the respective boroughs, and by the affected developers.\nAll the various developments, if implemented, could theoretically require an increase in the fleet from 30 to up to 80 trams (depending on whether longer trams or coupled trams are used). As such, an increase in depot and stabling capacity would also be required; enlargement of the current Therapia Lane site, as well as sites near the Elmers End and Harrington Road tram stops, were shortlisted."}
{"id": "7163", "revid": "1011045313", "url": "https://en.wikipedia.org/wiki?curid=7163", "title": "Catenary", "text": "In physics and geometry, a catenary (, ) is the curve that an idealized hanging chain or cable assumes under its own weight when supported only at its ends.\nThe catenary curve has a U-like shape, superficially similar in appearance to a parabolic arch, but it is not a parabola.\nThe curve appears in the design of certain types of arches and as a cross section of the catenoid\u2014the shape assumed by a soap film bounded by two parallel circular rings.\nThe catenary is also called the alysoid, chainette, or, particularly in the materials sciences, funicular. Rope statics describes catenaries in a classic statics problem involving a hanging rope.\nMathematically, the catenary curve is the graph of the hyperbolic cosine function. The surface of revolution of the catenary curve, the catenoid, is a minimal surface, specifically a minimal surface of revolution. A hanging chain will assume a shape of least potential energy which is a catenary. The mathematical properties of the catenary curve were first studied by Robert Hooke in the 1670s, and its equation was derived by Leibniz, Huygens and Johann Bernoulli in 1691.\nCatenaries and related curves are used in architecture and engineering (e.g., in the design of bridges and arches so that forces do not result in bending moments). In the offshore oil and gas industry, \"catenary\" refers to a steel catenary riser, a pipeline suspended between a production platform and the seabed that adopts an approximate catenary shape. In the rail industry it refers to the overhead wiring that transfers power to trains. (This often supports a lighter contact wire, in which case it does not follow a true catenary curve.)\nIn optics and electromagnetics, the hyperbolic cosine and sine functions are basic solutions to Maxwell's equations. The symmetric modes consisting of two evanescent waves would form a catenary shape.\nHistory.\nThe word \"catenary\" is derived from the Latin word \"cat\u0113na\", which means \"chain\". The English word \"catenary\" is usually attributed to Thomas Jefferson,\nwho wrote in a letter to Thomas Paine on the construction of an arch for a bridge:\nIt is often said that Galileo thought the curve of a hanging chain was parabolic. In his \"Two New Sciences\" (1638), Galileo says that a hanging cord is an approximate parabola, and he correctly observes that this approximation improves as the curvature gets smaller and is almost exact when the elevation is less than 45\u00b0. That the curve followed by a chain is not a parabola was proven by Joachim Jungius (1587\u20131657); this result was published posthumously in 1669.\nThe application of the catenary to the construction of arches is attributed to Robert Hooke, whose \"true mathematical and mechanical form\" in the context of the rebuilding of St Paul's Cathedral alluded to a catenary. Some much older arches approximate catenaries, an example of which is the Arch of Taq-i Kisra in Ctesiphon.\nIn 1671, Hooke announced to the Royal Society that he had solved the problem of the optimal shape of an arch, and in 1675 published an encrypted solution as a Latin anagram in an appendix to his \"Description of Helioscopes,\" where he wrote that he had found \"a true mathematical and mechanical form of all manner of Arches for Building.\" He did not publish the solution to this anagram in his lifetime, but in 1705 his executor provided it as \"ut pendet continuum flexile, sic stabit contiguum rigidum inversum\", meaning \"As hangs a flexible cable so, inverted, stand the touching pieces of an arch.\"\nIn 1691, Gottfried Leibniz, Christiaan Huygens, and Johann Bernoulli derived the equation in response to a challenge by Jakob Bernoulli; their solutions were published in the \"Acta Eruditorum\" for June 1691. David Gregory wrote a treatise on the catenary in 1697 in which he provided an incorrect derivation of the correct differential equation.\nEuler proved in 1744 that the catenary is the curve which, when rotated about the -axis, gives the surface of minimum surface area (the catenoid) for the given bounding circles. Nicolas Fuss gave equations describing the equilibrium of a chain under any force in 1796.\nInverted catenary arch.\nCatenary arches are often used in the construction of kilns. To create the desired curve, the shape of a hanging chain of the desired dimensions is transferred to a form which is then used as a guide for the placement of bricks or other building material.\nThe Gateway Arch in St. Louis, Missouri, United States is sometimes said to be an (inverted) catenary, but this is incorrect. It is close to a more general curve called a flattened catenary, with equation , which is a catenary if . While a catenary is the ideal shape for a freestanding arch of constant thickness, the Gateway Arch is narrower near the top. According to the U.S. National Historic Landmark nomination for the arch, it is a \"weighted catenary\" instead. Its shape corresponds to the shape that a weighted chain, having lighter links in the middle, would form.\nCatenary bridges.\nIn free-hanging chains, the force exerted is uniform with respect to length of the chain, and so the chain follows the catenary curve. The same is true of a simple suspension bridge or \"catenary bridge,\" where the roadway follows the cable.\nA stressed ribbon bridge is a more sophisticated structure with the same catenary shape.\nHowever, in a suspension bridge with a suspended roadway, the chains or cables support the weight of the bridge, and so do not hang freely. In most cases the roadway is flat, so when the weight of the cable is negligible compared with the weight being supported, the force exerted is uniform with respect to horizontal distance, and the result is a parabola, as discussed below (although the term \"catenary\" is often still used, in an informal sense). If the cable is heavy then the resulting curve is between a catenary and a parabola.\nAnchoring of marine objects.\nThe catenary produced by gravity provides an advantage to heavy anchor rodes. An anchor rode (or anchor line) usually consists of chain or cable or both. Anchor rodes are used by ships, oil rigs, docks, floating wind turbines, and other marine equipment which must be anchored to the seabed.\nWhen the rope is slack, the catenary curve presents a lower angle of pull on the anchor or mooring device than would be the case if it were nearly straight. This enhances the performance of the anchor and raises the level of force it will resist before dragging. To maintain the catenary shape in the presence of wind, a heavy chain is needed, so that only larger ships in deeper water can rely on this effect. Smaller boats also rely on catenary to maintain maximum holding power.\nMathematical description.\nEquation.\nThe equation of a catenary in Cartesian coordinates has the form\nwhere is the hyperbolic cosine function, and where is measured from the lowest point. All catenary curves are similar to each other; changing the parameter is equivalent to a uniform scaling of the curve.\nThe Whewell equation for the catenary is\nDifferentiating gives\nand eliminating gives the Ces\u00e0ro equation\nThe radius of curvature is then\nwhich is the length of the line normal to the curve between it and the -axis.\nRelation to other curves.\nWhen a parabola is rolled along a straight line, the roulette curve traced by its focus is a catenary. The envelope of the directrix of the parabola is also a catenary. The involute from the vertex, that is the roulette formed traced by a point starting at the vertex when a line is rolled on a catenary, is the tractrix.\nAnother roulette, formed by rolling a line on a catenary, is another line. This implies that square wheels can roll perfectly smoothly on a road made of a series of bumps in the shape of an inverted catenary curve. The wheels can be any regular polygon except a triangle, but the catenary must have parameters corresponding to the shape and dimensions of the wheels.\nGeometrical properties.\nOver any horizontal interval, the ratio of the area under the catenary to its length equals , independent of the interval selected. The catenary is the only plane curve other than a horizontal line with this property. Also, the geometric centroid of the area under a stretch of catenary is the midpoint of the perpendicular segment connecting the centroid of the curve itself and the -axis.\nScience.\nA moving charge in a uniform electric field travels along a catenary (which tends to a parabola if the charge velocity is much less than the speed of light ).\nThe surface of revolution with fixed radii at either end that has minimum surface area is a catenary revolved about the -axis.\nAnalysis.\nModel of chains and arches.\nIn the mathematical model the chain (or cord, cable, rope, string, etc.) is idealized by assuming that it is so thin that it can be regarded as a curve and that it is so flexible any force of tension exerted by the chain is parallel to the chain. The analysis of the curve for an optimal arch is similar except that the forces of tension become forces of compression and everything is inverted.\nAn underlying principle is that the chain may be considered a rigid body once it has attained equilibrium. Equations which define the shape of the curve and the tension of the chain at each point may be derived by a careful inspection of the various forces acting on a segment using the fact that these forces must be in balance if the chain is in static equilibrium.\nLet the path followed by the chain be given parametrically by where represents arc length and is the position vector. This is the natural parameterization and has the property that\nwhere is a unit tangent vector.\nA differential equation for the curve may be derived as follows. Let be the lowest point on the chain, called the vertex of the catenary. The slope of the curve is zero at C since it is a minimum point. Assume is to the right of since the other case is implied by symmetry. The forces acting on the section of the chain from to are the tension of the chain at , the tension of the chain at , and the weight of the chain. The tension at is tangent to the curve at and is therefore horizontal without any vertical component and it pulls the section to the left so it may be written where is the magnitude of the force. The tension at is parallel to the curve at and pulls the section to the right. The tension at can be split into two components so it may be written , where is the magnitude of the force and is the angle between the curve at and the -axis (see tangential angle). Finally, the weight of the chain is represented by where is the mass per unit length, is the acceleration of gravity and is the length of the segment of chain between and .\nThe chain is in equilibrium so the sum of three forces is , therefore\nand\nand dividing these gives\nIt is convenient to write\nwhich is the length of chain whose weight on Earth is equal in magnitude to the tension at . Then\nis an equation defining the curve.\nThe horizontal component of the tension, is constant and the vertical component of the tension, is proportional to the length of chain between and the vertex.\nDerivation of equations for the curve.\nThe differential equation given above can be solved to produce equations for the curve.\nFrom\nthe formula for arc length gives\nThen\nand\nThe second of these equations can be integrated to give\nand by shifting the position of the -axis, can be taken to be 0. Then\nThe -axis thus chosen is called the \"directrix\" of the catenary.\nIt follows that the magnitude of the tension at a point is , which is proportional to the distance between the point and the directrix.\nThe integral of the expression for can be found using standard techniques, giving\nand, again, by shifting the position of the -axis, can be taken to be 0. Then\nThe -axis thus chosen passes through the vertex and is called the axis of the catenary.\nThese results can be used to eliminate giving\nAlternative derivation.\nThe differential equation can be solved using a different approach. From\nit follows that\nand\nIntegrating gives,\nand\nAs before, the and -axes can be shifted so and can be taken to be 0. Then\nand taking the reciprocal of both sides\nAdding and subtracting the last two equations then gives the solution\nand\nDetermining parameters.\nIn general the parameter is the position of the axis. The equation can be determined in this case as follows:\nRelabel if necessary so that is to the left of and let be the horizontal and be the vertical distance from to . Translate the axes so that the vertex of the catenary lies on the -axis and its height is adjusted so the catenary satisfies the standard equation of the curve\nand let the coordinates of and be and respectively. The curve passes through these points, so the difference of height is\nand the length of the curve from to is\nWhen is expanded using these expressions the result is\nso\nThis is a transcendental equation in and must be solved numerically. It can be shown with the methods of calculus that there is at most one solution with and so there is at most one position of equilibrium.\nHowever, if both ends of the curve ( and ) are at the same level (), it can be shown that\nwhere L is the total length of the curve between and and is the sag (vertical distance between , and the vertex of the curve).\nIt can also be shown that\nand\nwhere H is the horizontal distance between and which are located at the same level ().\nThe horizontal traction force at and is , where is the mass per unit length of the chain or cable.\nGeneralizations with vertical force.\nNonuniform chains.\nIf the density of the chain is variable then the analysis above can be adapted to produce equations for the curve given the density, or given the curve to find the density.\nLet denote the weight per unit length of the chain, then the weight of the chain has magnitude\nwhere the limits of integration are and . Balancing forces as in the uniform chain produces\nand\nand therefore\nDifferentiation then gives\nIn terms of and the radius of curvature this becomes\nSuspension bridge curve.\nA similar analysis can be done to find the curve followed by the cable supporting a suspension bridge with a horizontal roadway. If the weight of the roadway per unit length is and the weight of the cable and the wire supporting the bridge is negligible in comparison, then the weight on the cable (see the figure in Catenary#Model of chains and arches) from to is where is the horizontal distance between and . Proceeding as before gives the differential equation\nThis is solved by simple integration to get\nand so the cable follows a parabola. If the weight of the cable and supporting wires is not negligible then the analysis is more complex.\nCatenary of equal strength.\nIn a catenary of equal strength, the cable is strengthened according to the magnitude of the tension at each point, so its resistance to breaking is constant along its length. Assuming that the strength of the cable is proportional to its density per unit length, the weight, , per unit length of the chain can be written , where is constant, and the analysis for nonuniform chains can be applied.\nIn this case the equations for tension are\nCombining gives\nand by differentiation\nwhere is the radius of curvature.\nThe solution to this is\nIn this case, the curve has vertical asymptotes and this limits the span to . Other relations are\nThe curve was studied 1826 by Davies Gilbert and, apparently independently, by Gaspard-Gustave Coriolis in 1836.\nRecently, it was shown that this type of catenary could act as a building block of electromagnetic metasurface and was known as \"catenary of equal phase gradient\".\nElastic catenary.\nIn an elastic catenary, the chain is replaced by a spring which can stretch in response to tension. The spring is assumed to stretch in accordance with Hooke's Law. Specifically, if is the natural length of a section of spring, then the length of the spring with tension applied has length\nwhere is a constant equal to , where is the stiffness of the spring. In the catenary the value of is variable, but ratio remains valid at a local level, so\nThe curve followed by an elastic spring can now be derived following a similar method as for the inelastic spring.\nThe equations for tension of the spring are\nand\nfrom which\nwhere is the natural length of the segment from to and is the mass per unit length of the spring with no tension and is the acceleration of gravity. Write\nso\nThen \nfrom which\nIntegrating gives the parametric equations\nAgain, the and -axes can be shifted so and can be taken to be 0. So\nare parametric equations for the curve. At the rigid limit where is large, the shape of the curve reduces to that of a non-elastic chain.\nOther generalizations.\nChain under a general force.\nWith no assumptions being made regarding the force acting on the chain, the following analysis can be made.\nFirst, let be the force of tension as a function of . The chain is flexible so it can only exert a force parallel to itself. Since tension is defined as the force that the chain exerts on itself, must be parallel to the chain. In other words,\nwhere is the magnitude of and is the unit tangent vector.\nSecond, let be the external force per unit length acting on a small segment of a chain as a function of . The forces acting on the segment of the chain between and are the force of tension at one end of the segment, the nearly opposite force at the other end, and the external force acting on the segment which is approximately . These forces must balance so\nDivide by and take the limit as to obtain\nThese equations can be used as the starting point in the analysis of a flexible chain acting under any external force. In the case of the standard catenary, where the chain has mass per unit length and is the acceleration of gravity."}
{"id": "7164", "revid": "6603956", "url": "https://en.wikipedia.org/wiki?curid=7164", "title": "Color temperature", "text": "The color temperature of a light source is the temperature of an ideal black-body radiator that radiates light of a color comparable to that of the light source. Color temperature is a characteristic of visible light that has important applications in lighting, photography, videography, publishing, manufacturing, astrophysics, horticulture, and other fields. In practice, color temperature is meaningful only for light sources that do in fact correspond somewhat closely to the radiation of some black body, i.e., light in a range going from red to orange to yellow to white to blueish white; it does not make sense to speak of the color temperature of, e.g., a green or a purple light. Color temperature is conventionally expressed in kelvins, using the symbol K, a unit of measure for absolute temperature.\nColor temperatures over 5000\u00a0K are called \"cool colors\" (bluish), while lower color temperatures (2700\u20133000\u00a0K) are called \"warm colors\" (yellowish). \"Warm\" in this context is an analogy to radiated heat flux of traditional incandescent lighting rather than temperature. The spectral peak of warm-coloured light is closer to infrared, and most natural warm-coloured light sources emit significant infrared radiation. The fact that \"warm\" lighting in this sense actually has a \"cooler\" color temperature often leads to confusion.\nCategorizing different lighting.\nThe color temperature of the electromagnetic radiation emitted from an ideal black body is defined as its surface temperature in kelvins, or alternatively in micro reciprocal degrees (mired). This permits the definition of a standard by which light sources are compared.\nTo the extent that a hot surface emits thermal radiation but is not an ideal black-body radiator, the color temperature of the light is not the actual temperature of the surface. An incandescent lamp's light is thermal radiation, and the bulb approximates an ideal black-body radiator, so its color temperature is essentially the temperature of the filament. Thus a relatively low temperature emits a dull red and a high temperature emits the almost white of the traditional incandescent light bulb. Metal workers are able to judge the temperature of hot metals by their color, from dark red to orange-white and then white (see red heat).\nMany other light sources, such as fluorescent lamps, or light emitting diodes (LEDs) emit light primarily by processes other than thermal radiation. This means that the emitted radiation does not follow the form of a black-body spectrum. These sources are assigned what is known as a correlated color temperature (CCT). CCT is the color temperature of a black-body radiator which to human color perception most closely matches the light from the lamp. Because such an approximation is not required for incandescent light, the CCT for an incandescent light is simply its unadjusted temperature, derived from comparison to a black-body radiator.\nThe Sun.\nThe Sun closely approximates a black-body radiator. The effective temperature, defined by the total radiative power per square unit, is about 5780\u00a0K. The color temperature of sunlight above the atmosphere is about 5900\u00a0K.\nThe Sun may appear red, orange, yellow, or white from Earth, depending on its position in the sky. The changing color of the Sun over the course of the day is mainly a result of the scattering of sunlight and is not due to changes in black-body radiation. Rayleigh scattering of sunlight by Earth's atmosphere causes the blue color of the sky, which tends to scatter blue light more than red light.\nSome daylight in the early morning and late afternoon (the golden hours) has a lower (\"warmer\") color temperature due to increased scattering of shorter-wavelength sunlight by atmospheric particles \u2013 an optical phenomenon called the Tyndall effect.\nDaylight has a spectrum similar to that of a black body with a correlated color temperature of 6500\u00a0K (D65 viewing standard) or 5500\u00a0K (daylight-balanced photographic film standard).\nFor colors based on black-body theory, blue occurs at higher temperatures, whereas red occurs at lower temperatures. This is the opposite of the cultural associations attributed to colors, in which \"red\" is \"hot\", and \"blue\" is \"cold\".\nApplications.\nLighting.\nFor lighting building interiors, it is often important to take into account the color temperature of illumination. A warmer (i.e., a lower color temperature) light is often used in public areas to promote relaxation, while a cooler (higher color temperature) light is used to enhance concentration, for example in schools and offices.\nCCT dimming for LED technology is regarded as a difficult task, since binning, age and temperature drift effects of LEDs change the actual color value output. Here feedback loop systems are used, for example with color sensors, to actively monitor and control the color output of multiple color mixing LEDs.\nAquaculture.\nIn fishkeeping, color temperature has different functions and foci in the various branches.\nDigital photography.\nIn digital photography, the term color temperature sometimes refers to remapping of color values to simulate variations in ambient color temperature. Most digital cameras and raw image software provide presets simulating specific ambient values (e.g., sunny, cloudy, tungsten, etc.) while others allow explicit entry of white balance values in kelvins. These settings vary color values along the blue\u2013yellow axis, while some software includes additional controls (sometimes labeled \"tint\") adding the magenta\u2013green axis, and are to some extent arbitrary and a matter of artistic interpretation.\nPhotographic film.\nPhotographic emulsion film does not respond to lighting color identically to the human retina or visual perception. An object that appears to the observer to be white may turn out to be very blue or orange in a photograph. The color balance may need to be corrected during printing to achieve a neutral color print. The extent of this correction is limited since color film normally has three layers sensitive to different colors and when used under the \"wrong\" light source, every layer may not respond proportionally, giving odd color casts in the shadows, although the mid-tones may have been correctly white-balanced under the enlarger. Light sources with discontinuous spectra, such as fluorescent tubes, cannot be fully corrected in printing either, since one of the layers may barely have recorded an image at all.\nPhotographic film is made for specific light sources (most commonly daylight film and tungsten film), and, used properly, will create a neutral color print. Matching the sensitivity of the film to the color temperature of the light source is one way to balance color. If tungsten film is used indoors with incandescent lamps, the yellowish-orange light of the tungsten incandescent lamps will appear as white (3200\u00a0K) in the photograph. Color negative film is almost always daylight-balanced, since it is assumed that color can be adjusted in printing (with limitations, see above). Color transparency film, being the final artefact in the process, has to be matched to the light source or filters must be used to correct color.\nFilters on a camera lens, or color gels over the light source(s) may be used to correct color balance. When shooting with a bluish light (high color temperature) source such as on an overcast day, in the shade, in window light, or if using tungsten film with white or blue light, a yellowish-orange filter will correct this. For shooting with daylight film (calibrated to 5600\u00a0K) under warmer (low color temperature) light sources such as sunsets, candlelight or tungsten lighting, a bluish (e.g. #80A) filter may be used. More-subtle filters are needed to correct for the difference between, say 3200\u00a0K and 3400\u00a0K tungsten lamps or to correct for the slightly blue cast of some flash tubes, which may be 6000\u00a0K.\nIf there is more than one light source with varied color temperatures, one way to balance the color is to use daylight film and place color-correcting gel filters over each light source.\nPhotographers sometimes use color temperature meters. These are usually designed to read only two regions along the visible spectrum (red and blue); more expensive ones read three regions (red, green, and blue). However, they are ineffective with sources such as fluorescent or discharge lamps, whose light varies in color and may be harder to correct for. Because this light is often greenish, a magenta filter may correct it. More sophisticated colorimetry tools can be used if such meters are lacking.\nDesktop publishing.\nIn the desktop publishing industry, it is important to know a monitor's color temperature. Color matching software, such as Apple's ColorSync for Mac OS, measures a monitor's color temperature and then adjusts its settings accordingly. This enables on-screen color to more closely match printed color. Common monitor color temperatures, along with matching standard illuminants in parentheses, are as follows:\nD50 is scientific shorthand for a standard illuminant: the daylight spectrum at a correlated color temperature of 5000\u00a0K. Similar definitions exist for D55, D65 and D75. Designations such as \"D50\" are used to help classify color temperatures of light tables and viewing booths. When viewing a color slide at a light table, it is important that the light be balanced properly so that the colors are not shifted towards the red or blue.\nDigital cameras, web graphics, DVDs, etc., are normally designed for a 6500\u00a0K color temperature. The sRGB standard commonly used for images on the Internet stipulates (among other things) a 6500\u00a0K display white point.\nTV, video, and digital still cameras.\nThe NTSC and PAL TV norms call for a compliant TV screen to display an electrically black and white signal (minimal color saturation) at a color temperature of 6500\u00a0K. On many consumer-grade televisions, there is a very noticeable deviation from this requirement. However, higher-end consumer-grade televisions can have their color temperatures adjusted to 6500\u00a0K by using a preprogrammed setting or a custom calibration. Current versions of ATSC explicitly call for the color temperature data to be included in the data stream, but old versions of ATSC allowed this data to be omitted. In this case, current versions of ATSC cite default colorimetry standards depending on the format. Both of the cited standards specify a 6500\u00a0K color temperature.\nMost video and digital still cameras can adjust for color temperature by zooming into a white or neutral colored object and setting the manual \"white balance\" (telling the camera that \"this object is white\"); the camera then shows true white as white and adjusts all the other colors accordingly. White-balancing is necessary especially when indoors under fluorescent lighting and when moving the camera from one lighting situation to another. Most cameras also have an automatic white balance function that attempts to determine the color of the light and correct accordingly. While these settings were once unreliable, they are much improved in today's digital cameras and produce an accurate white balance in a wide variety of lighting situations.\nArtistic application via control of color temperature.\nVideo camera operators can white-balance objects that are not white, downplaying the color of the object used for white-balancing. For instance, they can bring more warmth into a picture by white-balancing off something that is light blue, such as faded blue denim; in this way white-balancing can replace a filter or lighting gel when those are not available.\nCinematographers do not \"white balance\" in the same way as video camera operators; they use techniques such as filters, choice of film stock, pre-flashing, and, after shooting, color grading, both by exposure at the labs and also digitally. Cinematographers also work closely with set designers and lighting crews to achieve the desired color effects.\nFor artists, most pigments and papers have a cool or warm cast, as the human eye can detect even a minute amount of saturation. Gray mixed with yellow, orange, or red is a \"warm gray\". Green, blue, or purple create \"cool grays\". Note that this sense of temperature is the reverse of that of real temperature; bluer is described as \"cooler\" even though it corresponds to a higher-temperature black body.\nLighting designers sometimes select filters by color temperature, commonly to match light that is theoretically white. Since fixtures using discharge type lamps produce a light of a considerably higher color temperature than do tungsten lamps, using the two in conjunction could potentially produce a stark contrast, so sometimes fixtures with HID lamps, commonly producing light of 6000\u20137000\u00a0K, are fitted with 3200\u00a0K filters to emulate tungsten light. Fixtures with color mixing features or with multiple colors, (if including 3200\u00a0K) are also capable of producing tungsten-like light. Color temperature may also be a factor when selecting lamps, since each is likely to have a different color temperature.\nCorrelated color temperature.\nMotivation.\nBlack-body radiators are the reference by which the whiteness of light sources is judged. A black body can be described by its temperature and produces light of a particular hue, as depicted above. This set of colors is called \"color temperature\". By analogy, nearly Planckian light sources such as certain fluorescent or high-intensity discharge lamps can be judged by their correlated color temperature (CCT), the temperature of the Planckian radiator whose color best approximates them. For light source spectra that are not Planckian, matching them to that of a black body is not well defined; the concept of correlated color temperature was extended to map such sources as well as possible onto the one-dimensional scale of color temperature, where \"as well as possible\" is defined in the context of an objective color space.\nBackground.\nThe notion of using Planckian radiators as a yardstick against which to judge other light sources is not new. In 1923, writing about \"grading of illuminants with reference to quality of color ... the temperature of the source as an index of the quality of color\", Priest essentially described CCT as we understand it today, going so far as to use the term \"apparent color temperature\", and astutely recognized three cases:\nSeveral important developments occurred in 1931. In chronological order:\nThese developments paved the way for the development of new chromaticity spaces that are more suited to estimating correlated color temperatures and chromaticity differences. Bridging the concepts of color difference and color temperature, Priest made the observation that the eye is sensitive to constant differences in \"reciprocal\" temperature:\nPriest proposed to use \"the scale of temperature as a scale for arranging the chromaticities of the several illuminants in a serial order\". Over the next few years, Judd published three more significant papers:\nThe first verified the findings of Priest, Davis, and Judd, with a paper on sensitivity to change in color temperature.\nThe second proposed a new chromaticity space, guided by a principle that has become the holy grail of color spaces: perceptual uniformity (chromaticity distance should be commensurate with perceptual difference). By means of a projective transformation, Judd found a more \"uniform chromaticity space\" (UCS) in which to find the CCT. Judd determined the \"nearest color temperature\" by simply finding the point on the Planckian locus nearest to the chromaticity of the stimulus on Maxwell's color triangle, depicted aside. The transformation matrix he used to convert X,Y,Z tristimulus values to R,G,B coordinates was:\nFrom this, one can find these chromaticities:\nThe third depicted the locus of the isothermal chromaticities on the CIE 1931 \"x,y\" chromaticity diagram. Since the isothermal points formed normals on his UCS diagram, transformation back into the xy plane revealed them still to be lines, but no longer perpendicular to the locus.\nCalculation.\nJudd's idea of determining the nearest point to the Planckian locus on a uniform chromaticity space is current. In 1937, MacAdam suggested a \"modified uniform chromaticity scale diagram\", based on certain simplifying geometrical considerations:\nThis (u,v) chromaticity space became the CIE 1960 color space, which is still used to calculate the CCT (even though MacAdam did not devise it with this purpose in mind). Using other chromaticity spaces, such as \"u'v\"', leads to non-standard results that may nevertheless be perceptually meaningful.\nThe distance from the locus (i.e., degree of departure from a black body) is traditionally indicated in units of formula_4; positive for points above the locus. This concept of distance has evolved to become Delta E, which continues to be used today.\nRobertson's method.\nBefore the advent of powerful personal computers, it was common to estimate the correlated color temperature by way of interpolation from look-up tables and charts. The most famous such method is Robertson's, who took advantage of the relatively even spacing of the mired scale (see above) to calculate the CCT Tc using linear interpolation of the isotherm's mired values:\nwhere formula_6 and formula_7 are the color temperatures of the look-up isotherms and \"i\" is chosen such that formula_8. (Furthermore, the test chromaticity lies between the only two adjacent lines for which formula_9.)\nIf the isotherms are tight enough, one can assume formula_10, leading to\nThe distance of the test point to the \"i\"-th isotherm is given by\nwhere formula_13 is the chromaticity coordinate of the \"i\"-th isotherm on the Planckian locus and \"mi\" is the isotherm's slope. Since it is perpendicular to the locus, it follows that formula_14 where \"li\" is the slope of the locus at formula_13.\nPrecautions.\nAlthough the CCT can be calculated for any chromaticity coordinate, the result is meaningful only if the light sources are nearly white. The CIE recommends that \"The concept of correlated color temperature should not be used if the chromaticity of the test source differs more than [formula_16] from the Planckian radiator.\"\nBeyond a certain value of formula_17, a chromaticity co-ordinate may be equidistant to two points on the locus, causing ambiguity in the CCT.\nApproximation.\nIf a narrow range of color temperatures is considered\u2014those encapsulating daylight being the most practical case\u2014one can approximate the Planckian locus in order to calculate the CCT in terms of chromaticity coordinates. Following Kelly's observation that the isotherms intersect in the purple region near (\"x\" = 0.325, \"y\" = 0.154), McCamy proposed this cubic approximation:\nwhere is the inverse slope line, and is the \"epicenter\"; quite close to the intersection point mentioned by Kelly. The maximum absolute error for color temperatures ranging from 2856\u00a0K (illuminant A) to 6504\u00a0K (D65) is under 2\u00a0K.\nA more recent proposal, using exponential terms, considerably extends the applicable range by adding a second epicenter for high color temperatures:\nwhere is as before and the other constants are defined below:\nThe author suggests that one use the low-temperature equation to determine whether the higher-temperature parameters are needed.\nThe inverse calculation, from color temperature to corresponding chromaticity coordinates, is discussed in .\nColor rendering index.\nThe CIE color rendering index (CRI) is a method to determine how well a light source's illumination of eight sample patches compares to the illumination provided by a reference source. Cited together, the CRI and CCT give a numerical estimate of what reference (ideal) light source best approximates a particular artificial light, and what the difference is. See Color Rendering Index for full article.\nSpectral power distribution.\nLight sources and illuminants may be characterized by their spectral power distribution (SPD). The relative SPD curves provided by many manufacturers may have been produced using 10\u00a0nm increments or more on their spectroradiometer. The result is what would seem to be a smoother (\"fuller spectrum\") power distribution than the lamp actually has. Owing to their spiky distribution, much finer increments are advisable for taking measurements of fluorescent lights, and this requires more expensive equipment.\nColor temperature in astronomy.\nIn astronomy, the color temperature is defined by the local slope of the SPD at a given wavelength, or, in practice, a wavelength range. Given, for example, the color magnitudes \"B\" and \"V\" which are calibrated to be equal for an A0V star (e.g. Vega), the stellar color temperature formula_20 is given by the temperature for which the color index formula_21 of a black-body radiator fits the stellar one. Besides the formula_21, other color indices can be used as well. The color temperature (as well as the correlated color temperature defined above) may differ largely from the effective temperature given by the radiative flux of the stellar surface. For example, the color temperature of an A0V star is about 15000 K compared to an effective temperature of about 9500 K."}
{"id": "7165", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=7165", "title": "Cartoon", "text": "A cartoon is a type of illustration, sometimes animated, typically in a non-realistic or semi-realistic style. The specific meaning has evolved over time, but the modern usage usually refers to either: an image or series of images intended for satire, caricature, or humor; or a motion picture that relies on a sequence of illustrations for its animation. Someone who creates cartoons in the first sense is called a \"cartoonist\", and in the second sense they are usually called an \"animator\".\nThe concept originated in the Middle Ages, and first described a preparatory drawing for a piece of art, such as a painting, fresco, tapestry, or stained glass window. In the 19th century, beginning in \"Punch\" magazine in 1843, cartoon came to refer \u2013 ironically at first \u2013 to humorous illustrations in magazines and newspapers. Then it also was used for political cartoons and comic strips. When the medium developed, in the early 20th century, it began to refer to animated films which resembled print cartoons.\nFine art.\nA cartoon (from and \u2014words describing strong, heavy paper or pasteboard) is a full-size drawing made on sturdy paper as a design or \"modello\" for a painting, stained glass, or tapestry. Cartoons were typically used in the production of frescoes, to accurately link the component parts of the composition when painted on damp plaster over a series of days (\"giornate\"). In media such as stained tapestry or stained glass, the cartoon was handed over by the artist to the skilled craftsmen who produced the final work. \nSuch cartoons often have pinpricks along the outlines of the design so that a bag of soot patted or \"pounced\" over a cartoon, held against the wall, would leave black dots on the plaster (\"pouncing\"). Cartoons by painters, such as the Raphael Cartoons in London, and examples by Leonardo da Vinci, are highly prized in their own right. Tapestry cartoons, usually colored, were followed with the eye by the weavers on the loom.\nMass media.\nIn print media, a cartoon is an illustration or series of illustrations, usually humorous in intent. This usage dates from 1843, when \"Punch\" magazine applied the term to satirical drawings in its pages, particularly sketches by John Leech. The first of these parodied the preparatory cartoons for grand historical frescoes in the then-new Palace of Westminster. The original title for these drawings was \"Mr Punch's face is the letter Q\" and the new title \"cartoon\" was intended to be ironic, a reference to the self-aggrandizing posturing of Westminster politicians.\nCartoons can be divided into gag cartoons, which include editorial cartoons, and comic strips.\nModern single-panel \"gag cartoons\", found in magazines, generally consist of a single drawing with a typeset caption positioned beneath, or\u2014less often\u2014a speech balloon. Newspaper syndicates have also distributed single-panel gag cartoons by Mel Calman, Bill Holman, Gary Larson, George Lichty, Fred Neher and others. Many consider \"New Yorker\" cartoonist Peter Arno the father of the modern gag cartoon (as did Arno himself). The roster of magazine gag cartoonists includes Charles Addams, Charles Barsotti, and Chon Day.\nBill Hoest, Jerry Marcus, and Virgil Partch began as magazine gag cartoonists and moved to syndicated comic strips. Richard Thompson illustrated numerous feature articles in \"The Washington Post\" before creating his \"Cul de Sac\" comic strip. The sports section of newspapers usually featured cartoons, sometimes including syndicated features such as Chester \"Chet\" Brown's \"All in Sport\".\n\"Editorial cartoons\" are found almost exclusively in news publications and news websites. Although they also employ humor, they are more serious in tone, commonly using irony or satire. The art usually acts as a visual metaphor to illustrate a point of view on current social or political topics. Editorial cartoons often include speech balloons and sometimes use multiple panels. Editorial cartoonists of note include Herblock, David Low, Jeff MacNelly, Mike Peters, and Gerald Scarfe.\n\"Comic strips\", also known as \"cartoon strips\" in the United Kingdom, are found daily in newspapers worldwide, and are usually a short series of cartoon illustrations in sequence. In the United States, they are not commonly called \"cartoons\" themselves, but rather \"comics\" or \"funnies\". Nonetheless, the creators of comic strips\u2014as well as comic books and graphic novels\u2014are usually referred to as \"cartoonists\". Although humor is the most prevalent subject matter, adventure and drama are also represented in this medium. Some noteworthy cartoonists of humorous comic strips are Scott Adams, Steve Bell, Charles Schulz, E. C. Segar, Mort Walker and Bill Watterson.\nPolitical.\nPolitical cartoons are like illustrated editorial that serve visual commentaries on political events. They offer subtle criticism which are cleverly quoted with humour and satire to the extent that the criticized does not get embittered.\nThe pictorial satire of William Hogarth is regarded as a precursor to the development of political cartoons in 18th century England. George Townshend produced some of the first overtly political cartoons and caricatures in the 1750s. The medium began to develop in the latter part of the 18th century under the direction of its great exponents, James Gillray and Thomas Rowlandson, both from London. Gillray explored the use of the medium for lampooning and caricature, and has been referred to as the father of the political cartoon. By calling the king, prime ministers and generals to account for their behaviour, many of Gillray's satires were directed against George III, depicting him as a pretentious buffoon, while the bulk of his work was dedicated to ridiculing the ambitions of revolutionary France and Napoleon. George Cruikshank became the leading cartoonist in the period following Gillray, from 1815 until the 1840s. His career was renowned for his social caricatures of English life for popular publications.\nBy the mid 19th century, major political newspapers in many other countries featured cartoons commenting on the politics of the day. Thomas Nast, in New York City, showed how realistic German drawing techniques could redefine American cartooning. His 160 cartoons relentlessly pursued the criminal characteristic of the Tweed machine in New York City, and helped bring it down. Indeed, Tweed was arrested in Spain when police identified him from Nast's cartoons. In Britain, Sir John Tenniel was the toast of London. In France under the July Monarchy, Honor\u00e9 Daumier took up the new genre of political and social caricature, most famously lampooning the rotund King Louis Philippe.\nPolitical cartoons can be humorous or satirical, sometimes with piercing effect. The target of the humor may complain, but can seldom fight back. Lawsuits have been very rare; the first successful lawsuit against a cartoonist in over a century in Britain came in 1921, when J. H. Thomas, the leader of the National Union of Railwaymen (NUR), initiated libel proceedings against the magazine of the British Communist Party. Thomas claimed defamation in the form of cartoons and words depicting the events of \"Black Friday\", when he allegedly betrayed the locked-out Miners' Federation. To Thomas, the framing of his image by the far left threatened to grievously degrade his character in the popular imagination. Soviet-inspired communism was a new element in European politics, and cartoonists unrestrained by tradition tested the boundaries of libel law. Thomas won the lawsuit and restored his reputation.\nScientific.\nCartoons such as \"xkcd\" have also found their place in the world of science, mathematics, and technology. For example, the cartoon \"Wonderlab\" looked at daily life in the chemistry lab. In the U.S., one well-known cartoonist for these fields is Sidney Harris. Many of Gary Larson's cartoons have a scientific flavor.\nComic books.\nBooks with cartoons are usually magazine-format \"comic books,\" or occasionally reprints of newspaper cartoons.\nIn Britain in the 1930s adventure magazines became quite popular, especially those published by DC Thomson; the publisher sent observers around the country to talk to boys and learn what they wanted to read about. The story line in magazines, comic books and cinema that most appealed to boys was the glamorous heroism of British soldiers fighting wars that were exciting and just. D.C. Thomson issued the first \"The Dandy Comic\" in December 1937. It had a revolutionary design that broke away from the usual children's comics that were published broadsheet in size and not very colourful. Thomson capitalized on its success with a similar product \"The Beano\" in 1938.\nOn some occasions, new gag cartoons have been created for book publication, as was the case with \"Think Small\", a 1967 promotional book distributed as a giveaway by Volkswagen dealers. Bill Hoest and other cartoonists of that decade drew cartoons showing Volkswagens, and these were published along with humorous automotive essays by such humorists as H. Allen Smith, Roger Price and Jean Shepherd. The book's design juxtaposed each cartoon alongside a photograph of the cartoon's creator.\nAnimation.\nBecause of the stylistic similarities between comic strips and early animated movies, \"cartoon\" came to refer to animation, and the word \"cartoon\" is currently used in reference to both animated cartoons and gag cartoons. While \"animation\" designates any style of illustrated images seen in rapid succession to give the impression of movement, the word \"cartoon\" is most often used as a descriptor for television programs and short films aimed at children, possibly featuring anthropomorphized animals, superheroes, the adventures of child protagonists or related themes.\nIn the 1980s, \"cartoon\" was shortened to \"toon\", referring to characters in animated productions. This term was popularized in 1988 by the combined live-action/animated film \"Who Framed Roger Rabbit\", followed in 1990 by the animated TV series \"Tiny Toon Adventures\"."}
{"id": "7167", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=7167", "title": "Chief Minister of the Northern Territory", "text": "The Chief Minister of the Northern Territory is the head of government of the Northern Territory. The office is the equivalent of a State Premier. \nWhen the Northern Territory Legislative Assembly was created in 1974, the head of government was officially known as Majority Leader. This title was used in the first parliament (1974\u20131977) and the first eighteen months of the second. When self-government was granted the Northern Territory in 1978, the title of the head of government became Chief Minister.\nThe Chief Minister is formally appointed by the Administrator, who in normal circumstances will appoint the head of whichever party holds the majority of seats in the unicameral Legislative Assembly. In times of constitutional crisis, the Administrator can appoint someone else as Chief Minister, however, this has never occurred.\nSince 31 August, following the 2016 election, the Chief Minister is Michael Gunner of the Labor Party. He is the first Chief Minister to have been born in the Northern Territory.\nHistory.\nThe Country Liberal Party won the first Northern Territory election on 19 October 1974 and elected Goff Letts Majority Leader. He headed an Executive that carried out most of the functions of a ministry at the state level. At the 1977 election Letts lost his seat and party leadership. He was succeeded on 13 August 1977 by Paul Everingham (CLP) as Majority Leader. When the Territory attained self-government on 1 July 1978, Everingham became Chief Minister with greatly expanded powers.\nIn 2001, Clare Martin became the first Labor and female Chief Minister of the Northern Territory. Until 2004 the conduct of elections and drawing of electoral boundaries was performed by the Northern Territory Electoral Office, a unit of the Department of the Chief Minister. In March 2004 the independent Northern Territory Electoral Commission was established.\nIn 2013, Mills was replaced as Chief Minister and CLP leader by Adam Giles at the 2013 CLP leadership ballot on 13 March to become the first indigenous Australian to lead a state or territory government in Australia.\nFollowing the 2016 election landslide outcome, Labor's Michael Gunner became Chief Minister.\nList of Chief Ministers of the Northern Territory.\nFrom the foundation of the Northern Territory Legislative Assembly in 1974 until the granting of self-government in 1978, the head of government was known as the Majority Leader:\nFrom 1978, the position was known as the Chief Minister:"}
{"id": "7170", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=7170", "title": "Chinese exclusion", "text": ""}
{"id": "7172", "revid": "41400767", "url": "https://en.wikipedia.org/wiki?curid=7172", "title": "Chemotherapy", "text": "Chemotherapy (often abbreviated to chemo and sometimes CTX or CTx) is a type of cancer treatment that uses one or more anti-cancer drugs (chemotherapeutic agents) as part of a standardized chemotherapy regimen. Chemotherapy may be given with a curative intent (which almost always involves combinations of drugs), or it may aim to prolong life or to reduce symptoms (palliative chemotherapy). Chemotherapy is one of the major categories of the medical discipline specifically devoted to pharmacotherapy for cancer, which is called \"medical oncology\".\nThe term \"chemotherapy\" has come to connote non-specific usage of intracellular poisons to inhibit mitosis (cell division) or induce DNA damage, which is why inhibition of DNA repair can augment chemotherapy. The connotation of the word chemotherapy excludes more selective agents that block extracellular signals (signal transduction). The development of therapies with specific molecular or genetic targets, which inhibit growth-promoting signals from classic endocrine hormones (primarily estrogens for breast cancer and androgens for prostate cancer) are now called hormonal therapies. By contrast, other inhibitions of growth-signals like those associated with receptor tyrosine kinases are referred to as targeted therapy.\nImportantly, the use of drugs (whether chemotherapy, hormonal therapy or targeted therapy) constitutes \"systemic therapy\" for cancer in that they are introduced into the blood stream and are therefore in principle able to address cancer at any anatomic location in the body. Systemic therapy is often used in conjunction with other modalities that constitute \"local therapy\" (i.e. treatments whose efficacy is confined to the anatomic area where they are applied) for cancer such as radiation therapy, surgery or hyperthermia therapy.\nTraditional chemotherapeutic agents are cytotoxic by means of interfering with cell division (mitosis) but cancer cells vary widely in their susceptibility to these agents. To a large extent, chemotherapy can be thought of as a way to damage or stress cells, which may then lead to cell death if apoptosis is initiated. Many of the side effects of chemotherapy can be traced to damage to normal cells that divide rapidly and are thus sensitive to anti-mitotic drugs: cells in the bone marrow, digestive tract and hair follicles. This results in the most common side-effects of chemotherapy: myelosuppression (decreased production of blood cells, hence also immunosuppression), mucositis (inflammation of the lining of the digestive tract), and alopecia (hair loss). Because of the effect on immune cells (especially lymphocytes), chemotherapy drugs often find use in a host of diseases that result from harmful overactivity of the immune system against self (so-called autoimmunity). These include rheumatoid arthritis, systemic lupus erythematosus, multiple sclerosis, vasculitis and many others.\nTreatment strategies.\nThere are a number of strategies in the administration of chemotherapeutic drugs used today. Chemotherapy may be given with a curative intent or it may aim to prolong life or to palliate symptoms.\nAll chemotherapy regimens require that the recipient be capable of undergoing the treatment. Performance status is often used as a measure to determine whether a person can receive chemotherapy, or whether dose reduction is required. Because only a fraction of the cells in a tumor die with each treatment (fractional kill), repeated doses must be administered to continue to reduce the size of the tumor. Current chemotherapy regimens apply drug treatment in cycles, with the frequency and duration of treatments limited by toxicity.\nEfficiency.\nThe efficiency of chemotherapy depends on the type of cancer and the stage. The overall effectiveness ranges from being curative for some cancers, such as some leukemias, to being ineffective, such as in some brain tumors, to being needless in others, like most non-melanoma skin cancers.\nDosage.\nDosage of chemotherapy can be difficult: If the dose is too low, it will be ineffective against the tumor, whereas, at excessive doses, the toxicity (side-effects) will be intolerable to the person receiving it. The standard method of determining chemotherapy dosage is based on calculated body surface area (BSA). The BSA is usually calculated with a mathematical formula or a nomogram, using the recipient's weight and height, rather than by direct measurement of body area. This formula was originally derived in a 1916 study and attempted to translate medicinal doses established with laboratory animals to equivalent doses for humans. The study only included nine human subjects. When chemotherapy was introduced in the 1950s, the BSA formula was adopted as the official standard for chemotherapy dosing for lack of a better option.\nThe validity of this method in calculating uniform doses has been questioned because the formula only takes into account the individual's weight and height. Drug absorption and clearance are influenced by multiple factors, including age, sex, metabolism, disease state, organ function, drug-to-drug interactions, genetics, and obesity, which have major impacts on the actual concentration of the drug in the person's bloodstream. As a result, there is high variability in the systemic chemotherapy drug concentration in people dosed by BSA, and this variability has been demonstrated to be more than ten-fold for many drugs. In other words, if two people receive the same dose of a given drug based on BSA, the concentration of that drug in the bloodstream of one person may be 10 times higher or lower compared to that of the other person. This variability is typical with many chemotherapy drugs dosed by BSA, and, as shown below, was demonstrated in a study of 14 common chemotherapy drugs.\nThe result of this pharmacokinetic variability among people is that many people do not receive the right dose to achieve optimal treatment effectiveness with minimized toxic side effects. Some people are overdosed while others are underdosed. For example, in a randomized clinical trial, investigators found 85% of metastatic colorectal cancer patients treated with 5-fluorouracil (5-FU) did not receive the optimal therapeutic dose when dosed by the BSA standard\u201468% were underdosed and 17% were overdosed.\nThere has been controversy over the use of BSA to calculate chemotherapy doses for people who are obese. Because of their higher BSA, clinicians often arbitrarily reduce the dose prescribed by the BSA formula for fear of overdosing. In many cases, this can result in sub-optimal treatment.\nSeveral clinical studies have demonstrated that when chemotherapy dosing is individualized to achieve optimal systemic drug exposure, treatment outcomes are improved and toxic side effects are reduced. In the 5-FU clinical study cited above, people whose dose was adjusted to achieve a pre-determined target exposure realized an 84% improvement in treatment response rate and a six-month improvement in overall survival (OS) compared with those dosed by BSA.\nIn the same study, investigators compared the incidence of common 5-FU-associated grade 3/4 toxicities between the dose-adjusted people and people dosed per BSA. The incidence of debilitating grades of diarrhea was reduced from 18% in the BSA-dosed group to 4% in the dose-adjusted group and serious hematologic side effects were eliminated. Because of the reduced toxicity, dose-adjusted patients were able to be treated for longer periods of time. BSA-dosed people were treated for a total of 680 months while people in the dose-adjusted group were treated for a total of 791 months. Completing the course of treatment is an important factor in achieving better treatment outcomes.\nSimilar results were found in a study involving people with colorectal cancer who were treated with the popular FOLFOX regimen. The incidence of serious diarrhea was reduced from 12% in the BSA-dosed group of patients to 1.7% in the dose-adjusted group, and the incidence of severe mucositis was reduced from 15% to 0.8%.\nThe FOLFOX study also demonstrated an improvement in treatment outcomes. Positive response increased from 46% in the BSA-dosed group to 70% in the dose-adjusted group. Median progression free survival (PFS) and overall survival (OS) both improved by six months in the dose adjusted group.\nOne approach that can help clinicians individualize chemotherapy dosing is to measure the drug levels in blood plasma over time and adjust dose according to a formula or algorithm to achieve optimal exposure. With an established target exposure for optimized treatment effectiveness with minimized toxicities, dosing can be personalized to achieve target exposure and optimal results for each person. Such an algorithm was used in the clinical trials cited above and resulted in significantly improved treatment outcomes.\nOncologists are already individualizing dosing of some cancer drugs based on exposure. Carboplatin and busulfan dosing rely upon results from blood tests to calculate the optimal dose for each person. Simple blood tests are also available for dose optimization of methotrexate, 5-FU, paclitaxel, and docetaxel.\nThe serum albumin level immediately prior to chemotherapy administration is an independent prognostic predictor of survival in various cancer types.\nTypes.\nAlkylating agents.\nAlkylating agents are the oldest group of chemotherapeutics in use today. Originally derived from mustard gas used in World War I, there are now many types of alkylating agents in use. They are so named because of their ability to alkylate many molecules, including proteins, RNA and DNA. This ability to bind covalently to DNA via their alkyl group is the primary cause for their anti-cancer effects. DNA is made of two strands and the molecules may either bind twice to one strand of DNA (intrastrand crosslink) or may bind once to both strands (interstrand crosslink). If the cell tries to replicate crosslinked DNA during cell division, or tries to repair it, the DNA strands can break. This leads to a form of programmed cell death called apoptosis. Alkylating agents will work at any point in the cell cycle and thus are known as cell cycle-independent drugs. For this reason the effect on the cell is dose dependent; the fraction of cells that die is directly proportional to the dose of drug.\nThe subtypes of alkylating agents are the nitrogen mustards, nitrosoureas, tetrazines, aziridines, cisplatins and derivatives, and non-classical alkylating agents. Nitrogen mustards include mechlorethamine, cyclophosphamide, melphalan, chlorambucil, ifosfamide and busulfan. Nitrosoureas include N-Nitroso-N-methylurea (MNU), carmustine (BCNU), lomustine (CCNU) and semustine (MeCCNU), fotemustine and streptozotocin. Tetrazines include dacarbazine, mitozolomide and temozolomide. Aziridines include thiotepa, mytomycin and diaziquone (AZQ). Cisplatin and derivatives include cisplatin, carboplatin and oxaliplatin. They impair cell function by forming covalent bonds with the amino, carboxyl, sulfhydryl, and phosphate groups in biologically important molecules. Non-classical alkylating agents include procarbazine and hexamethylmelamine.\nAntimetabolites.\nAnti-metabolites are a group of molecules that impede DNA and RNA synthesis. Many of them have a similar structure to the building blocks of DNA and RNA. The building blocks are nucleotides; a molecule comprising a nucleobase, a sugar and a phosphate group. The nucleobases are divided into purines (guanine and adenine) and pyrimidines (cytosine, thymine and uracil). Anti-metabolites resemble either nucleobases or nucleosides (a nucleotide without the phosphate group), but have altered chemical groups. These drugs exert their effect by either blocking the enzymes required for DNA synthesis or becoming incorporated into DNA or RNA. By inhibiting the enzymes involved in DNA synthesis, they prevent mitosis because the DNA cannot duplicate itself. Also, after misincorporation of the molecules into DNA, DNA damage can occur and programmed cell death (apoptosis) is induced. Unlike alkylating agents, anti-metabolites are cell cycle dependent. This means that they only work during a specific part of the cell cycle, in this case S-phase (the DNA synthesis phase). For this reason, at a certain dose, the effect plateaus and proportionally no more cell death occurs with increased doses. Subtypes of the anti-metabolites are the anti-folates, fluoropyrimidines, deoxynucleoside analogues and thiopurines.\nThe anti-folates include methotrexate and pemetrexed. Methotrexate inhibits dihydrofolate reductase (DHFR), an enzyme that regenerates tetrahydrofolate from dihydrofolate. When the enzyme is inhibited by methotrexate, the cellular levels of folate coenzymes diminish. These are required for thymidylate and purine production, which are both essential for DNA synthesis and cell division. Pemetrexed is another anti-metabolite that affects purine and pyrimidine production, and therefore also inhibits DNA synthesis. It primarily inhibits the enzyme thymidylate synthase, but also has effects on DHFR, aminoimidazole carboxamide ribonucleotide formyltransferase and glycinamide ribonucleotide formyltransferase. The fluoropyrimidines include fluorouracil and capecitabine. Fluorouracil is a nucleobase analogue that is metabolised in cells to form at least two active products; 5-fluourouridine monophosphate (FUMP) and 5-fluoro-2'-deoxyuridine 5'-phosphate (fdUMP). FUMP becomes incorporated into RNA and fdUMP inhibits the enzyme thymidylate synthase; both of which lead to cell death. Capecitabine is a prodrug of 5-fluorouracil that is broken down in cells to produce the active drug. The deoxynucleoside analogues include cytarabine, gemcitabine, decitabine, azacitidine, fludarabine, nelarabine, cladribine, clofarabine, and pentostatin. The thiopurines include thioguanine and mercaptopurine.\nAnti-microtubule agents.\nAnti-microtubule agents are plant-derived chemicals that block cell division by preventing microtubule function. Microtubules are an important cellular structure composed of two proteins, \u03b1-tubulin and \u03b2-tubulin. They are hollow, rod-shaped structures that are required for cell division, among other cellular functions. Microtubules are dynamic structures, which means that they are permanently in a state of assembly and disassembly. \"Vinca\" alkaloids and taxanes are the two main groups of anti-microtubule agents, and although both of these groups of drugs cause microtubule dysfunction, their mechanisms of action are completely opposite: \"Vinca\" alkaloids prevent the assembly of microtubules, whereas taxanes prevent their disassembly. By doing so, they prevent cancer cells from completing mitosis. Following this, cell cycle arrest occurs, which induces programmed cell death (apoptosis). These drugs can also affect blood vessel growth, an essential process that tumours utilise in order to grow and metastasise.\n\"Vinca\" alkaloids are derived from the Madagascar periwinkle, \"Catharanthus roseus\", formerly known as \"Vinca rosea\". They bind to specific sites on tubulin, inhibiting the assembly of tubulin into microtubules. The original \"vinca\" alkaloids are natural products that include vincristine and vinblastine. Following the success of these drugs, semi-synthetic \"vinca\" alkaloids were produced: vinorelbine (used in the treatment of non-small-cell lung cancer), vindesine, and vinflunine. These drugs are cell cycle-specific. They bind to the tubulin molecules in S-phase and prevent proper microtubule formation required for M-phase.\nTaxanes are natural and semi-synthetic drugs. The first drug of their class, paclitaxel, was originally extracted from \"Taxus brevifolia\", the Pacific yew. Now this drug and another in this class, docetaxel, are produced semi-synthetically from a chemical found in the bark of another yew tree, \"Taxus baccata\".\nPodophyllotoxin is an antineoplastic lignan obtained primarily from the American mayapple (\"Podophyllum peltatum\") and Himalayan mayapple (\"Sinopodophyllum hexandrum\"). It has anti-microtubule activity, and its mechanism is similar to that of \"vinca\" alkaloids in that they bind to tubulin, inhibiting microtubule formation. Podophyllotoxin is used to produce two other drugs with different mechanisms of action: etoposide and teniposide.\nTopoisomerase inhibitors.\nTopoisomerase inhibitors are drugs that affect the activity of two enzymes: topoisomerase I and topoisomerase II. When the DNA double-strand helix is unwound, during DNA replication or transcription, for example, the adjacent unopened DNA winds tighter (supercoils), like opening the middle of a twisted rope. The stress caused by this effect is in part aided by the topoisomerase enzymes. They produce single- or double-strand breaks into DNA, reducing the tension in the DNA strand. This allows the normal unwinding of DNA to occur during replication or transcription. Inhibition of topoisomerase I or II interferes with both of these processes.\nTwo topoisomerase I inhibitors, irinotecan and topotecan, are semi-synthetically derived from camptothecin, which is obtained from the Chinese ornamental tree \"Camptotheca acuminata\". Drugs that target topoisomerase II can be divided into two groups. The topoisomerase II poisons cause increased levels enzymes bound to DNA. This prevents DNA replication and transcription, causes DNA strand breaks, and leads to programmed cell death (apoptosis). These agents include etoposide, doxorubicin, mitoxantrone and teniposide. The second group, catalytic inhibitors, are drugs that block the activity of topoisomerase II, and therefore prevent DNA synthesis and translation because the DNA cannot unwind properly. This group includes novobiocin, merbarone, and aclarubicin, which also have other significant mechanisms of action.\nCytotoxic antibiotics.\nThe cytotoxic antibiotics are a varied group of drugs that have various mechanisms of action. The common theme that they share in their chemotherapy indication is that they interrupt cell division. The most important subgroup is the anthracyclines and the bleomycins; other prominent examples include mitomycin C and actinomycin.\nAmong the anthracyclines, doxorubicin and daunorubicin were the first, and were obtained from the bacterium \"Streptomyces peucetius\". Derivatives of these compounds include epirubicin and idarubicin. Other clinically used drugs in the anthracycline group are pirarubicin, aclarubicin, and mitoxantrone. The mechanisms of anthracyclines include DNA intercalation (molecules insert between the two strands of DNA), generation of highly reactive free radicals that damage intercellular molecules and topoisomerase inhibition.\nActinomycin is a complex molecule that intercalates DNA and prevents RNA synthesis.\nBleomycin, a glycopeptide isolated from \"Streptomyces verticillus\", also intercalates DNA, but produces free radicals that damage DNA. This occurs when bleomycin binds to a metal ion, becomes chemically reduced and reacts with oxygen.\nMitomycin is a cytotoxic antibiotic with the ability to alkylate DNA.\nDelivery.\nMost chemotherapy is delivered intravenously, although a number of agents can be administered orally (e.g., melphalan, busulfan, capecitabine). According to a recent (2016) systematic review, oral therapies present additional challenges for patients and care teams to maintain and support adherence to treatment plans.\nThere are many intravenous methods of drug delivery, known as vascular access devices. These include the winged infusion device, peripheral venous catheter, midline catheter, peripherally inserted central catheter (PICC), central venous catheter and implantable port. The devices have different applications regarding duration of chemotherapy treatment, method of delivery and types of chemotherapeutic agent.\nDepending on the person, the cancer, the stage of cancer, the type of chemotherapy, and the dosage, intravenous chemotherapy may be given on either an inpatient or an outpatient basis. For continuous, frequent or prolonged intravenous chemotherapy administration, various systems may be surgically inserted into the vasculature to maintain access. Commonly used systems are the Hickman line, the Port-a-Cath, and the PICC line. These have a lower infection risk, are much less prone to phlebitis or extravasation, and eliminate the need for repeated insertion of peripheral cannulae.\nIsolated limb perfusion (often used in melanoma), or isolated infusion of chemotherapy into the liver or the lung have been used to treat some tumors. The main purpose of these approaches is to deliver a very high dose of chemotherapy to tumor sites without causing overwhelming systemic damage. These approaches can help control solitary or limited metastases, but they are by definition not systemic, and, therefore, do not treat distributed metastases or micrometastases.\nTopical chemotherapies, such as 5-fluorouracil, are used to treat some cases of non-melanoma skin cancer.\nIf the cancer has central nervous system involvement, or with meningeal disease, intrathecal chemotherapy may be administered.\nAdverse effects.\nChemotherapeutic techniques have a range of side-effects that depend on the type of medications used. The most common medications affect mainly the fast-dividing cells of the body, such as blood cells and the cells lining the mouth, stomach, and intestines. Chemotherapy-related toxicities can occur acutely after administration, within hours or days, or chronically, from weeks to years.\nImmunosuppression and myelosuppression.\nVirtually all chemotherapeutic regimens can cause depression of the immune system, often by paralysing the bone marrow and leading to a decrease of white blood cells, red blood cells, and platelets.\nAnemia and thrombocytopenia may require blood transfusion. Neutropenia (a decrease of the neutrophil granulocyte count below 0.5 x 109/litre) can be improved with synthetic G-CSF (granulocyte-colony-stimulating factor, e.g., filgrastim, lenograstim).\nIn very severe myelosuppression, which occurs in some regimens, almost all the bone marrow stem cells (cells that produce white and red blood cells) are destroyed, meaning \"allogenic\" or \"autologous\" bone marrow cell transplants are necessary. (In autologous BMTs, cells are removed from the person before the treatment, multiplied and then re-injected afterward; in \"allogenic\" BMTs, the source is a donor.) However, some people still develop diseases because of this interference with bone marrow.\nAlthough people receiving chemotherapy are encouraged to wash their hands, avoid sick people, and take other infection-reducing steps, about 85% of infections are due to naturally occurring microorganisms in the person's own gastrointestinal tract (including oral cavity) and skin. This may manifest as systemic infections, such as sepsis, or as localized outbreaks, such as Herpes simplex, shingles, or other members of the Herpesviridea. The risk of illness and death can be reduced by taking common antibiotics such as quinolones or trimethoprim/sulfamethoxazole before any fever or sign of infection appears. Quinolones show effective prophylaxis mainly with hematological cancer. However, in general, for every five people who are immunosuppressed following chemotherapy who take an antibiotic, one fever can be prevented; for every 34 who take an antibiotic, one death can be prevented. Sometimes, chemotherapy treatments are postponed because the immune system is suppressed to a critically low level.\nIn Japan, the government has approved the use of some medicinal mushrooms like \"Trametes versicolor\", to counteract depression of the immune system in people undergoing chemotherapy.\nTrilaciclib is an inhibitor of cyclin-dependent kinase 4/6 approved for the prevention of myelosuppression caused by chemotherapy. The drug is given before chemotherapy to protect bone marrow function.\nNeutropenic enterocolitis.\nDue to immune system suppression, neutropenic enterocolitis (typhlitis) is a \"life-threatening gastrointestinal complication of chemotherapy.\" Typhlitis is an intestinal infection which may manifest itself through symptoms including nausea, vomiting, diarrhea, a distended abdomen, fever, chills, or abdominal pain and tenderness.\nTyphlitis is a medical emergency. It has a very poor prognosis and is often fatal unless promptly recognized and aggressively treated. Successful treatment hinges on early diagnosis provided by a high index of suspicion and the use of CT scanning, nonoperative treatment for uncomplicated cases, and sometimes elective right hemicolectomy to prevent recurrence.\nGastrointestinal distress.\nNausea, vomiting, anorexia, diarrhoea, abdominal cramps, and constipation are common side-effects of chemotherapeutic medications that kill fast-dividing cells. Malnutrition and dehydration can result when the recipient does not eat or drink enough, or when the person vomits frequently, because of gastrointestinal damage. This can result in rapid weight loss, or occasionally in weight gain, if the person eats too much in an effort to allay nausea or heartburn. Weight gain can also be caused by some steroid medications. These side-effects can frequently be reduced or eliminated with antiemetic drugs. Low-certainty evidence also suggests that probiotics may have a preventative and treatment effect of diarrhoea related to chemotherapy alone and with radiotherapy. However, a high index of suspicion is appropriate, since diarrhea and bloating are also symptoms of typhlitis, a very serious and potentially life-threatening medical emergency that requires immediate treatment.\nAnemia.\nAnemia can be a combined outcome caused by myelosuppressive chemotherapy, and possible cancer-related causes such as bleeding, blood cell destruction (hemolysis), hereditary disease, kidney dysfunction, nutritional\ndeficiencies or anemia of chronic disease. Treatments to mitigate anemia include hormones to boost blood production (erythropoietin), iron supplements, and blood transfusions. Myelosuppressive therapy can cause a tendency to bleed easily, leading to anemia. Medications that kill rapidly dividing cells or blood cells can reduce the number of platelets in the blood, which can result in bruises and bleeding. Extremely low platelet counts may be temporarily boosted through platelet transfusions and new drugs to increase platelet counts during chemotherapy are being developed. Sometimes, chemotherapy treatments are postponed to allow platelet counts to recover.\nFatigue may be a consequence of the cancer or its treatment, and can last for months to years after treatment. One physiological cause of fatigue is anemia, which can be caused by chemotherapy, surgery, radiotherapy, primary and metastatic disease or nutritional depletion. Aerobic exercise has been found to be beneficial in reducing fatigue in people with solid tumours.\nNausea and vomiting.\nNausea and vomiting are two of the most feared cancer treatment-related side-effects for people with cancer and their families. In 1983, Coates et al. found that people receiving chemotherapy ranked nausea and vomiting as the first and second most severe side-effects, respectively. Up to 20% of people receiving highly emetogenic agents in this era postponed, or even refused, potentially curative treatments. Chemotherapy-induced nausea and vomiting (CINV) are common with many treatments and some forms of cancer. Since the 1990s, several novel classes of antiemetics have been developed and commercialized, becoming a nearly universal standard in chemotherapy regimens, and helping to successfully manage these symptoms in many people. Effective mediation of these unpleasant and sometimes-crippling symptoms results in increased quality of life for the recipient and more efficient treatment cycles, due to less stoppage of treatment due to better tolerance and better overall health.\nHair loss.\nHair loss (alopecia) can be caused by chemotherapy that kills rapidly dividing cells; other medications may cause hair to thin. These are most often temporary effects: hair usually starts to regrow a few weeks after the last treatment, but sometimes with a change in color, texture, thickness or style. Sometimes hair has a tendency to curl after regrowth, resulting in \"chemo curls.\" Severe hair loss occurs most often with drugs such as doxorubicin, daunorubicin, paclitaxel, docetaxel, cyclophosphamide, ifosfamide and etoposide. Permanent thinning or hair loss can result from some standard chemotherapy regimens.\nChemotherapy induced hair loss occurs by a non-androgenic mechanism, and can manifest as alopecia totalis, telogen effluvium, or less often alopecia areata. It is usually associated with systemic treatment due to the high mitotic rate of hair follicles, and more reversible than androgenic hair loss, although permanent cases can occur. Chemotherapy induces hair loss in women more often than men.\nScalp cooling offers a means of preventing both permanent and temporary hair loss; however, concerns about this method have been raised.\nSecondary neoplasm.\nDevelopment of secondary neoplasia after successful chemotherapy or radiotherapy treatment can occur. The most common secondary neoplasm is secondary acute myeloid leukemia, which develops primarily after treatment with alkylating agents or topoisomerase inhibitors. Survivors of childhood cancer are more than 13 times as likely to get a secondary neoplasm during the 30 years after treatment than the general population. Not all of this increase can be attributed to chemotherapy.\nInfertility.\nSome types of chemotherapy are gonadotoxic and may cause infertility. Chemotherapies with high risk include procarbazine and other alkylating drugs such as cyclophosphamide, ifosfamide, busulfan, melphalan, chlorambucil, and chlormethine. Drugs with medium risk include doxorubicin and platinum analogs such as cisplatin and carboplatin. On the other hand, therapies with low risk of gonadotoxicity include plant derivatives such as vincristine and vinblastine, antibiotics such as bleomycin and dactinomycin, and antimetabolites such as methotrexate, mercaptopurine, and 5-fluorouracil.\nFemale infertility by chemotherapy appears to be secondary to premature ovarian failure by loss of primordial follicles. This loss is not necessarily a direct effect of the chemotherapeutic agents, but could be due to an increased rate of growth initiation to replace damaged developing follicles.\nPeople may choose between several methods of fertility preservation prior to chemotherapy, including cryopreservation of semen, ovarian tissue, oocytes, or embryos. As more than half of cancer patients are elderly, this adverse effect is only relevant for a minority of patients. A study in France between 1999 and 2011 came to the result that embryo freezing before administration of gonadotoxic agents to females caused a delay of treatment in 34% of cases, and a live birth in 27% of surviving cases who wanted to become pregnant, with the follow-up time varying between 1 and 13 years.\nPotential protective or attenuating agents include GnRH analogs, where several studies have shown a protective effect \"in vivo\" in humans, but some studies show no such effect. Sphingosine-1-phosphate (S1P) has shown similar effect, but its mechanism of inhibiting the sphingomyelin apoptotic pathway may also interfere with the apoptosis action of chemotherapy drugs.\nIn chemotherapy as a conditioning regimen in hematopoietic stem cell transplantation, a study of people conditioned with cyclophosphamide alone for severe aplastic anemia came to the result that ovarian recovery occurred in all women younger than 26 years at time of transplantation, but only in five of 16 women older than 26 years.\nTeratogenicity.\nChemotherapy is teratogenic during pregnancy, especially during the first trimester, to the extent that abortion usually is recommended if pregnancy in this period is found during chemotherapy. Second- and third-trimester exposure does not usually increase the teratogenic risk and adverse effects on cognitive development, but it may increase the risk of various complications of pregnancy and fetal myelosuppression.\nIn males previously having undergone chemotherapy or radiotherapy, there appears to be no increase in genetic defects or congenital malformations in their children conceived after therapy. The use of assisted reproductive technologies and micromanipulation techniques might increase this risk. In females previously having undergone chemotherapy, miscarriage and congenital malformations are not increased in subsequent conceptions. However, when in vitro fertilization and embryo cryopreservation is practised between or shortly after treatment, possible genetic risks to the growing oocytes exist, and hence it has been recommended that the babies be screened.\nPeripheral neuropathy.\nBetween 30 and 40 percent of people undergoing chemotherapy experience chemotherapy-induced peripheral neuropathy (CIPN), a progressive, enduring, and often irreversible condition, causing pain, tingling, numbness and sensitivity to cold, beginning in the hands and feet and sometimes progressing to the arms and legs. Chemotherapy drugs associated with CIPN include thalidomide, epothilones, \"vinca\" alkaloids, taxanes, proteasome inhibitors, and the platinum-based drugs. Whether CIPN arises, and to what degree, is determined by the choice of drug, duration of use, the total amount consumed and whether the person already has peripheral neuropathy. Though the symptoms are mainly sensory, in some cases motor nerves and the autonomic nervous system are affected. CIPN often follows the first chemotherapy dose and increases in severity as treatment continues, but this progression usually levels off at completion of treatment. The platinum-based drugs are the exception; with these drugs, sensation may continue to deteriorate for several months after the end of treatment. Some CIPN appears to be irreversible. Pain can often be managed with drug or other treatment but the numbness is usually resistant to treatment.\nCognitive impairment.\nSome people receiving chemotherapy report fatigue or non-specific neurocognitive problems, such as an inability to concentrate; this is sometimes called post-chemotherapy cognitive impairment, referred to as \"chemo brain\" in popular and social media.\nTumor lysis syndrome.\nIn particularly large tumors and cancers with high white cell counts, such as lymphomas, teratomas, and some leukemias, some people develop tumor lysis syndrome. The rapid breakdown of cancer cells causes the release of chemicals from the inside of the cells. Following this, high levels of uric acid, potassium and phosphate are found in the blood. High levels of phosphate induce secondary hypoparathyroidism, resulting in low levels of calcium in the blood. This causes kidney damage and the high levels of potassium can cause cardiac arrhythmia. Although prophylaxis is available and is often initiated in people with large tumors, this is a dangerous side-effect that can lead to death if left untreated.\nOrgan damage.\nCardiotoxicity (heart damage) is especially prominent with the use of anthracycline drugs (doxorubicin, epirubicin, idarubicin, and liposomal doxorubicin). The cause of this is most likely due to the production of free radicals in the cell and subsequent DNA damage. Other chemotherapeutic agents that cause cardiotoxicity, but at a lower incidence, are cyclophosphamide, docetaxel and clofarabine.\nHepatotoxicity (liver damage) can be caused by many cytotoxic drugs. The susceptibility of an individual to liver damage can be altered by other factors such as the cancer itself, viral hepatitis, immunosuppression and nutritional deficiency. The liver damage can consist of damage to liver cells, hepatic sinusoidal syndrome (obstruction of the veins in the liver), cholestasis (where bile does not flow from the liver to the intestine) and liver fibrosis.\nNephrotoxicity (kidney damage) can be caused by tumor lysis syndrome and also due direct effects of drug clearance by the kidneys. Different drugs will affect different parts of the kidney and the toxicity may be asymptomatic (only seen on blood or urine tests) or may cause acute kidney injury.\nOtotoxicity (damage to the inner ear) is a common side effect of platinum based drugs that can produce symptoms such as dizziness and vertigo. Children treated with platinum analogues have been found to be at risk for developing hearing loss.\nOther side-effects.\nLess common side-effects include red skin (erythema), dry skin, damaged fingernails, a dry mouth (xerostomia), water retention, and sexual impotence. Some medications can trigger allergic or pseudoallergic reactions.\nSpecific chemotherapeutic agents are associated with organ-specific toxicities, including cardiovascular disease (e.g., doxorubicin), interstitial lung disease (e.g., bleomycin) and occasionally secondary neoplasm (e.g., MOPP therapy for Hodgkin's disease).\nHand-foot syndrome is another side effect to cytotoxic chemotherapy.\nNutritional problems are also frequently seen in cancer patients at diagnosis and through chemotherapy treatment. Research suggests that in children and young people undergoing cancer treatment, parenteral nutrition may help with this leading to weight gain and increased calorie and protein intake, when compared to enteral nutrition.\nLimitations.\nChemotherapy does not always work, and even when it is useful, it may not completely destroy the cancer. People frequently fail to understand its limitations. In one study of people who had been newly diagnosed with incurable, stage 4 cancer, more than two-thirds of people with lung cancer and more than four-fifths of people with colorectal cancer still believed that chemotherapy was likely to cure their cancer.\nThe blood\u2013brain barrier poses an obstacle to delivery of chemotherapy to the brain. This is because the brain has an extensive system in place to protect it from harmful chemicals. Drug transporters can pump out drugs from the brain and brain's blood vessel cells into the cerebrospinal fluid and blood circulation. These transporters pump out most chemotherapy drugs, which reduces their efficacy for treatment of brain tumors. Only small lipophilic alkylating agents such as lomustine or temozolomide are able to cross this blood\u2013brain barrier.\nBlood vessels in tumors are very different from those seen in normal tissues. As a tumor grows, tumor cells furthest away from the blood vessels become low in oxygen (hypoxic). To counteract this they then signal for new blood vessels to grow. The newly formed tumor vasculature is poorly formed and does not deliver an adequate blood supply to all areas of the tumor. This leads to issues with drug delivery because many drugs will be delivered to the tumor by the circulatory system.\nResistance.\nResistance is a major cause of treatment failure in chemotherapeutic drugs. There are a few possible causes of resistance in cancer, one of which is the presence of small pumps on the surface of cancer cells that actively move chemotherapy from inside the cell to the outside. Cancer cells produce high amounts of these pumps, known as p-glycoprotein, in order to protect themselves from chemotherapeutics. Research on p-glycoprotein and other such chemotherapy efflux pumps is currently ongoing. Medications to inhibit the function of p-glycoprotein are undergoing investigation, but due to toxicities and interactions with anti-cancer drugs their development has been difficult. Another mechanism of resistance is gene amplification, a process in which multiple copies of a gene are produced by cancer cells. This overcomes the effect of drugs that reduce the expression of genes involved in replication. With more copies of the gene, the drug can not prevent all expression of the gene and therefore the cell can restore its proliferative ability. Cancer cells can also cause defects in the cellular pathways of apoptosis (programmed cell death). As most chemotherapy drugs kill cancer cells in this manner, defective apoptosis allows survival of these cells, making them resistant. Many chemotherapy drugs also cause DNA damage, which can be repaired by enzymes in the cell that carry out DNA repair. Upregulation of these genes can overcome the DNA damage and prevent the induction of apoptosis. Mutations in genes that produce drug target proteins, such as tubulin, can occur which prevent the drugs from binding to the protein, leading to resistance to these types of drugs. Drugs used in chemotherapy can induce cell stress, which can kill a cancer cell; however, under certain conditions, cells stress can induce changes in gene expression that enables resistance to several types of drugs. In lung cancer, the transcription factor NF\u03baB is thought to play a role in resistance to chemotherapy, via inflammatory pathways.\nCytotoxics and targeted therapies.\nTargeted therapies are a relatively new class of cancer drugs that can overcome many of the issues seen with the use of cytotoxics. They are divided into two groups: small molecule and antibodies. The massive toxicity seen with the use of cytotoxics is due to the lack of cell specificity of the drugs. They will kill any rapidly dividing cell, tumor or normal. Targeted therapies are designed to affect cellular proteins or processes that are utilised by the cancer cells. This allows a high dose to cancer tissues with a relatively low dose to other tissues. Although the side effects are often less severe than that seen of cytotoxic chemotherapeutics, life-threatening effects can occur. Initially, the targeted therapeutics were supposed to be solely selective for one protein. Now it is clear that there is often a range of protein targets that the drug can bind. An example target for targeted therapy is the BCR-ABL1 protein produced from the Philadelphia chromosome, a genetic lesion found commonly in chronic myelogenous leukemia and in some patients with acute lymphoblastic leukemia. This fusion protein has enzyme activity that can be inhibited by imatinib, a small molecule drug.\nMechanism of action.\nCancer is the uncontrolled growth of cells coupled with malignant behaviour: invasion and metastasis (among other features). It is caused by the interaction between genetic susceptibility and environmental factors. These factors lead to accumulations of genetic mutations in oncogenes (genes that control the growth rate of cells) and tumor suppressor genes (genes that help to prevent cancer), which gives cancer cells their malignant characteristics, such as uncontrolled growth.\nIn the broad sense, most chemotherapeutic drugs work by impairing mitosis (cell division), effectively targeting fast-dividing cells. As these drugs cause damage to cells, they are termed \"cytotoxic\". They prevent mitosis by various mechanisms including damaging DNA and inhibition of the cellular machinery involved in cell division. One theory as to why these drugs kill cancer cells is that they induce a programmed form of cell death known as apoptosis.\nAs chemotherapy affects cell division, tumors with high growth rates (such as acute myelogenous leukemia and the aggressive lymphomas, including Hodgkin's disease) are more sensitive to chemotherapy, as a larger proportion of the targeted cells are undergoing cell division at any time. Malignancies with slower growth rates, such as indolent lymphomas, tend to respond to chemotherapy much more modestly. Heterogeneic tumours may also display varying sensitivities to chemotherapy agents, depending on the subclonal populations within the tumor.\nCells from the immune system also make crucial contributions to the antitumor effects of chemotherapy. For example, the chemotherapeutic drugs oxaliplatin and cyclophosphamide can cause tumor cells to die in a way that is detectable by the immune system (called immunogenic cell death), which mobilizes immune cells with antitumor functions. Chemotherapeutic drugs that cause cancer immunogenic tumor cell death can make unresponsive tumors sensitive to immune checkpoint therapy.\nOther uses.\nSome chemotherapy drugs are used in diseases other than cancer, such as in autoimmune disorders, and noncancerous plasma cell dyscrasia. In some cases they are often used at lower doses, which means that the side effects are minimized, while in other cases doses similar to ones used to treat cancer are used. Methotrexate is used in the treatment of rheumatoid arthritis (RA), psoriasis, ankylosing spondylitis and multiple sclerosis. The anti-inflammatory response seen in RA is thought to be due to increases in adenosine, which causes immunosuppression; effects on immuno-regulatory cyclooxygenase-2 enzyme pathways; reduction in pro-inflammatory cytokines; and anti-proliferative properties. Although methotrexate is used to treat both multiple sclerosis and ankylosing spondylitis, its efficacy in these diseases is still uncertain. Cyclophosphamide is sometimes used to treat lupus nephritis, a common symptom of systemic lupus erythematosus. Dexamethasone along with either bortezomib or melphalan is commonly used as a treatment for AL amyloidosis. Recently, bortezomid in combination with cyclophosphamide and dexamethasone has also shown promise as a treatment for AL amyloidosis. Other drugs used to treat myeloma such as lenalidomide have shown promise in treating AL amyloidosis.\nChemotherapy drugs are also used in conditioning regimens prior to bone marrow transplant (hematopoietic stem cell transplant). Conditioning regimens are used to suppress the recipient's immune system in order to allow a transplant to engraft. Cyclophosphamide is a common cytotoxic drug used in this manner, and is often used in conjunction with total body irradiation. Chemotherapeutic drugs may be used at high doses to permanently remove the recipient's bone marrow cells (myeloablative conditioning) or at lower doses that will prevent permanent bone marrow loss (non-myeloablative and reduced intensity conditioning). When used in non-cancer setting, the treatment is still called \"chemotherapy\", and is often done in the same treatment centers used for people with cancer.\nOccupational exposure and safe handling.\nIn the 1970s, antineoplastic (chemotherapy) drugs were identified as hazardous, and the American Society of Health-System Pharmacists (ASHP) has since then introduced the concept of hazardous drugs after publishing a recommendation in 1983 regarding handling hazardous drugs. The adaptation of federal regulations came when the U.S. Occupational Safety and Health Administration (OSHA) first released its guidelines in 1986 and then updated them in 1996, 1999, and, most recently, 2006.\nThe National Institute for Occupational Safety and Health (NIOSH) has been conducting an assessment in the workplace since then regarding these drugs. Occupational exposure to antineoplastic drugs has been linked to multiple health effects, including infertility and possible carcinogenic effects. A few cases have been reported by the NIOSH alert report, such as one in which a female pharmacist was diagnosed with papillary transitional cell carcinoma. Twelve years before the pharmacist was diagnosed with the condition, she had worked for 20 months in a hospital where she was responsible for preparing multiple antineoplastic drugs. The pharmacist didn't have any other risk factor for cancer, and therefore, her cancer was attributed to the exposure to the antineoplastic drugs, although a cause-and-effect relationship has not been established in the literature. Another case happened when a malfunction in biosafety cabinetry is believed to have exposed nursing personnel to antineoplastic drugs. Investigations revealed evidence of genotoxic biomarkers two and nine months after that exposure.\nRoutes of exposure.\nAntineoplastic drugs are usually given through intravenous, intramuscular, intrathecal, or subcutaneous administration. In most cases, before the medication is administered to the patient, it needs to be prepared and handled by several workers. Any worker who is involved in handling, preparing, or administering the drugs, or with cleaning objects that have come into contact with antineoplastic drugs, is potentially exposed to hazardous drugs. Health care workers are exposed to drugs in different circumstances, such as when pharmacists and pharmacy technicians prepare and handle antineoplastic drugs and when nurses and physicians administer the drugs to patients. Additionally, those who are responsible for disposing antineoplastic drugs in health care facilities are also at risk of exposure.\nDermal exposure is thought to be the main route of exposure due to the fact that significant amounts of the antineoplastic agents have been found in the gloves worn by healthcare workers who prepare, handle, and administer the agents. Another noteworthy route of exposure is inhalation of the drugs' vapors. Multiple studies have investigated inhalation as a route of exposure, and although air sampling has not shown any dangerous levels, it is still a potential route of exposure. Ingestion by hand to mouth is a route of exposure that is less likely compared to others because of the enforced hygienic standard in the health institutions. However, it is still a potential route, especially in the workplace, outside of a health institute. One can also be exposed to these hazardous drugs through injection by needle sticks. Research conducted in this area has established that occupational exposure occurs by examining evidence in multiple urine samples from health care workers.\nHazards.\nHazardous drugs expose health care workers to serious health risks. Many studies show that antineoplastic drugs could have many side effects on the reproductive system, such as fetal loss, congenital malformation, and infertility. Health care workers who are exposed to antineoplastic drugs on many occasions have adverse reproductive outcomes such as spontaneous abortions, stillbirths, and congenital malformations. Moreover, studies have shown that exposure to these drugs leads to menstrual cycle irregularities. Antineoplastic drugs may also increase the risk of learning disabilities among children of health care workers who are exposed to these hazardous substances.\nMoreover, these drugs have carcinogenic effects. In the past five decades, multiple studies have shown the carcinogenic effects of exposure to antineoplastic drugs. Similarly, there have been research studies that linked alkylating agents with humans developing leukemias. Studies have reported elevated risk of breast cancer, nonmelanoma skin cancer, and cancer of the rectum among nurses who are exposed to these drugs. Other investigations revealed that there is a potential genotoxic effect from anti-neoplastic drugs to workers in health care settings.\nSafe handling in health care settings.\nAs of 2018, there were no occupational exposure limits set for antineoplastic drugs, i.e., OSHA or the American Conference of Governmental Industrial Hygienists (ACGIH) have not set workplace safety guidelines.\nPreparation.\nNIOSH recommends using a ventilated cabinet that is designed to decrease worker exposure. Additionally, it recommends training of all staff, the use of cabinets, implementing an initial evaluation of the technique of the safety program, and wearing protective gloves and gowns when opening drug packaging, handling vials, or labeling. When wearing personal protective equipment, one should inspect gloves for physical defects before use and always wear double gloves and protective gowns. Health care workers are also required to wash their hands with water and soap before and after working with antineoplastic drugs, change gloves every 30 minutes or whenever punctured, and discard them immediately in a chemotherapy waste container.\nThe gowns used should be disposable gowns made of polyethylene-coated polypropylene. When wearing gowns, individuals should make sure that the gowns are closed and have long sleeves. When preparation is done, the final product should be completely sealed in a plastic bag.\nThe health care worker should also wipe all waste containers inside the ventilated cabinet before removing them from the cabinet. Finally, workers should remove all protective wear and put them in a bag for their disposal inside the ventilated cabinet.\nAdministration.\nDrugs should only be administered using protective medical devices such as needle lists and closed systems and techniques such as priming of IV tubing by pharmacy personnel inside a ventilated cabinet. Workers should always wear personal protective equipment such as double gloves, goggles, and protective gowns when opening the outer bag and assembling the delivery system to deliver the drug to the patient, and when disposing of all material used in the administration of the drugs.\nHospital workers should never remove tubing from an IV bag that contains an antineoplastic drug, and when disconnecting the tubing in the system, they should make sure the tubing has been thoroughly flushed. After removing the IV bag, the workers should place it together with other disposable items directly in the yellow chemotherapy waste container with the lid closed. Protective equipment should be removed and put into a disposable chemotherapy waste container. After this has been done, one should double bag the chemotherapy waste before or after removing one's inner gloves. Moreover, one must always wash one's hands with soap and water before leaving the drug administration site.\nEmployee training.\nAll employees whose jobs in health care facilities expose them to hazardous drugs must receive training. Training should include shipping and receiving personnel, housekeepers, pharmacists, assistants, and all individuals involved in the transportation and storage of antineoplastic drugs. These individuals should receive information and training to inform them of the hazards of the drugs present in their areas of work. They should be informed and trained on operations and procedures in their work areas where they can encounter hazards, different methods used to detect the presence of hazardous drugs and how the hazards are released, and the physical and health hazards of the drugs, including their reproductive and carcinogenic hazard potential. Additionally, they should be informed and trained on the measures they should take to avoid and protect themselves from these hazards. This information ought to be provided when health care workers come into contact with the drugs, that is, perform the initial assignment in a work area with hazardous drugs. Moreover, training should also be provided when new hazards emerge as well as when new drugs, procedures, or equipment are introduced.\nHousekeeping and waste disposal.\nWhen performing cleaning and decontaminating the work area where antineoplastic drugs are used, one should make sure that there is sufficient ventilation to prevent the buildup of airborne drug concentrations. When cleaning the work surface, hospital workers should use deactivation and cleaning agents before and after each activity as well as at the end of their shifts. Cleaning should always be done using double protective gloves and disposable gowns. After employees finish up cleaning, they should dispose of the items used in the activity in a yellow chemotherapy waste container while still wearing protective gloves. After removing the gloves, they should thoroughly wash their hands with soap and water. Anything that comes into contact or has a trace of the antineoplastic drugs, such as needles, empty vials, syringes, gowns, and gloves, should be put in the chemotherapy waste container.\nSpill control.\nA written policy needs to be in place in case of a spill of antineoplastic products. The policy should address the possibility of various sizes of spills as well as the procedure and personal protective equipment required for each size. A trained worker should handle a large spill and always dispose of all cleanup materials in the chemical waste container according to EPA regulations, not in a yellow chemotherapy waste container.\nOccupational monitoring.\nA medical surveillance program must be established. In case of exposure, occupational health professionals need to ask for a detailed history and do a thorough physical exam. They should test the urine of the potentially exposed worker by doing a urine dipstick or microscopic examination, mainly looking for blood, as several antineoplastic drugs are known to cause bladder damage.\nUrinary mutagenicity is a marker of exposure to antineoplastic drugs that was first used by Falck and colleagues in 1979 and uses bacterial mutagenicity assays. Apart from being nonspecific, the test can be influenced by extraneous factors such as dietary intake and smoking and is, therefore, used sparingly. However, the test played a significant role in changing the use of horizontal flow cabinets to vertical flow biological safety cabinets during the preparation of antineoplastic drugs because the former exposed health care workers to high levels of drugs. This changed the handling of drugs and effectively reduced workers\u2019 exposure to antineoplastic drugs.\nBiomarkers of exposure to antineoplastic drugs commonly include urinary platinum, methotrexate, urinary cyclophosphamide and ifosfamide, and urinary metabolite of 5-fluorouracil. In addition to this, there are other drugs used to measure the drugs directly in the urine, although they are rarely used. A measurement of these drugs directly in one's urine is a sign of high exposure levels and that an uptake of the drugs is happening either through inhalation or dermally. \u00a0\nAvailable agents.\nThere is an extensive list of antineoplastic agents. Several classification schemes have been used to subdivide the medicines used for cancer into several different types.\nHistory.\nThe first use of small-molecule drugs to treat cancer was in the early 20th century, although the specific chemicals first used were not originally intended for that purpose. Mustard gas was used as a chemical warfare agent during World War I and was discovered to be a potent suppressor of hematopoiesis (blood production). A similar family of compounds known as nitrogen mustards were studied further during World War II at the Yale School of Medicine. It was reasoned that an agent that damaged the rapidly growing white blood cells might have a similar effect on cancer. Therefore, in December 1942, several people with advanced lymphomas (cancers of the lymphatic system and lymph nodes) were given the drug by vein, rather than by breathing the irritating gas. Their improvement, although temporary, was remarkable. Concurrently, during a military operation in World War II, following a German air raid on the Italian harbour of Bari, several hundred people were accidentally exposed to mustard gas, which had been transported there by the Allied forces to prepare for possible retaliation in the event of German use of chemical warfare. The survivors were later found to have very low white blood cell counts. After WWII was over and the reports declassified, the experiences converged and led researchers to look for other substances that might have similar effects against cancer. The first chemotherapy drug to be developed from this line of research was mustine. Since then, many other drugs have been developed to treat cancer, and drug development has exploded into a multibillion-dollar industry, although the principles and limitations of chemotherapy discovered by the early researchers still apply.\nThe term \"chemotherapy\".\nThe word \"chemotherapy\" without a modifier usually refers to cancer treatment, but its historical meaning was broader. The term was coined in the early 1900s by Paul Ehrlich as meaning any use of chemicals to treat any disease (\"chemo-\" + \"-therapy\"), such as the use of antibiotics (\"antibacterial chemotherapy\"). Ehrlich was not optimistic that effective chemotherapy drugs would be found for the treatment of cancer. The first modern chemotherapeutic agent was arsphenamine, an arsenic compound discovered in 1907 and used to treat syphilis. This was later followed by sulfonamides (sulfa drugs) and penicillin. In today's usage, the sense \"any treatment of disease with drugs\" is often expressed with the word \"pharmacotherapy\".\nSales.\nThe top 10 best-selling (in terms of revenue) cancer drugs of 2013:\nResearch.\nTargeted therapies.\nSpecially targeted delivery vehicles aim to increase effective levels of chemotherapy for tumor cells while reducing effective levels for other cells. This should result in an increased tumor kill or reduced toxicity or both.\nAntibody-drug conjugates.\nAntibody-drug conjugates (ADCs) comprise an antibody, drug and a linker between them. The antibody will be targeted at a preferentially expressed protein in the tumour cells (known as a tumor antigen) or on cells that the tumor can utilise, such as blood vessel endothelial cells. They bind to the tumor antigen and are internalised, where the linker releases the drug into the cell. These specially targeted delivery vehicles vary in their stability, selectivity, and choice of target, but, in essence, they all aim to increase the maximum effective dose that can be delivered to the tumor cells. Reduced systemic toxicity means that they can also be used in people who are sicker, and that they can carry new chemotherapeutic agents that would have been far too toxic to deliver via traditional systemic approaches.\nThe first approved drug of this type was gemtuzumab ozogamicin (Mylotarg), released by Wyeth (now Pfizer). The drug was approved to treat acute myeloid leukemia, but has now been withdrawn from the market because the drug did not meet efficacy targets in further clinical trials. Two other drugs, trastuzumab emtansine and brentuximab vedotin, are both in late clinical trials, and the latter has been granted accelerated approval for the treatment of refractory Hodgkin's lymphoma and systemic anaplastic large cell lymphoma.\nNanoparticles.\nNanoparticles are 1\u20131000 nanometer (nm) sized particles that can promote tumor selectivity and aid in delivering low-solubility drugs. Nanoparticles can be targeted passively or actively. Passive targeting exploits the difference between tumor blood vessels and normal blood vessels. Blood vessels in tumors are \"leaky\" because they have gaps from 200 to 2000\u00a0nm, which allow nanoparticles to escape into the tumor. Active targeting uses biological molecules (antibodies, proteins, DNA and receptor ligands) to preferentially target the nanoparticles to the tumor cells. There are many types of nanoparticle delivery systems, such as silica, polymers, liposomes and magnetic particles. Nanoparticles made of magnetic material can also be used to concentrate agents at tumor sites using an externally applied magnetic field. They have emerged as a useful vehicle in magnetic drug delivery for poorly soluble agents such as paclitaxel.\nElectrochemotherapy.\nElectrochemotherapy is the combined treatment in which injection of a chemotherapeutic drug is followed by application of high-voltage electric pulses locally to the tumor. The treatment enables the chemotherapeutic drugs, which otherwise cannot or hardly go through the membrane of cells (such as bleomycin and cisplatin), to enter the cancer cells. Hence, greater effectiveness of antitumor treatment is achieved.\nClinical electrochemotherapy has been successfully used for treatment of cutaneous and subcutaneous tumors irrespective of their histological origin. The method has been reported as safe, simple and highly effective in all reports on clinical use of electrochemotherapy. According to the ESOPE project (European Standard Operating Procedures of Electrochemotherapy), the Standard Operating Procedures (SOP) for electrochemotherapy were prepared, based on the experience of the leading European cancer centres on electrochemotherapy. Recently, new electrochemotherapy modalities have been developed for treatment of internal tumors using surgical procedures, endoscopic routes or percutaneous approaches to gain access to the treatment area.\nHyperthermia therapy.\nHyperthermia therapy is heat treatment for cancer that can be a powerful tool when used in combination with chemotherapy (thermochemotherapy) or radiation for the control of a variety of cancers. The heat can be applied locally to the tumor site, which will dilate blood vessels to the tumor, allowing more chemotherapeutic medication to enter the tumor. Additionally, the tumor cell membrane will become more porous, further allowing more of the chemotherapeutic medicine to enter the tumor cell.\nHyperthermia has also been shown to help prevent or reverse \"chemo-resistance.\" Chemotherapy resistance sometimes develops over time as the tumors adapt and can overcome the toxicity of the chemo medication. \"Overcoming chemoresistance has been extensively studied within the past, especially using CDDP-resistant cells. In regard to the potential benefit that drug-resistant cells can be recruited for effective therapy by combining chemotherapy with hyperthermia, it was important to show that chemoresistance against several anticancer drugs (e.g. mitomycin C, anthracyclines, BCNU, melphalan) including CDDP could be reversed at least partially by the addition of heat.\nOther animals.\nChemotherapy is used in veterinary medicine similar to how it is used in human medicine.\nExternal links.\n Institute for Occupational Safety and Health"}
{"id": "7174", "revid": "1839637", "url": "https://en.wikipedia.org/wiki?curid=7174", "title": "Chinese historiography", "text": "Chinese historiography is the study of the techniques and sources used by historians to develop the recorded history of China.\nOverview of Chinese history.\nThe recording of Chinese history dates back to the Shang dynasty (c. 1600\u20131046 BC). Many written examples survive of ceremonial inscriptions, divinations and records of family names, which were carved or painted onto tortoise shell or bones.\nThe oldest surviving history texts of China were compiled in the \"Book of Documents (Shujing)\". The \"Spring and Autumn Annals (Chunqiu)\", the official chronicle of the State of Lu, cover the period from 722 to 481 BC, and are among the earliest surviving Chinese historical texts to be arranged as annals. The compilations of both of these works are traditionally ascribed to Confucius. The \"Zuo zhuan\", attributed to Zuo Qiuming in the 5th century BC, is the earliest Chinese work of narrative history and covers the period from 722 to 468 BC. The anonymous \"Zhan Guo Ce\" was a renowned ancient Chinese historical work composed of sporadic materials on the Warring States period between the 3rd and 1st centuries BC.\nThe first systematic Chinese historical text, the \"Records of the Grand Historian (Shiji)\", was written by Sima Qian (c.145 or 135\u201386BC) based on work by his father, Sima Tan. It covers the period from the time of the Yellow Emperor until the author's own lifetime. Because of this highly praised and frequently copied work, Sima Qian is often regarded as the father of Chinese historiography. The \"Twenty-Four Histories\", the official histories of the dynasties considered legitimate by imperial Chinese historians, all copied Sima Qian's format. Typically, rulers initiating a new dynasty would employ scholars to compile a final history from the annals and records of the previous one.\nThe \"Shitong\" was the first Chinese work about historiography. It was compiled by Liu Zhiji between 708 and 710 AD. The book describes the general pattern of the official dynastic histories with regard to the structure, method, arrangement, sequence, caption, and commentary dating back to the Warring States period.\nThe \"Zizhi Tongjian\" was a pioneering reference work of Chinese historiography. Emperor Yingzong of Song ordered Sima Guang and other scholars to begin compiling this universal history of China in 1065, and they presented it to his successor Shenzong in 1084. It contains 294 volumes and about three million characters, and it narrates the history of China from 403 BC to the beginning of the Song dynasty in 959. This style broke the nearly thousand-year tradition of Sima Qian, which employed annals for imperial reigns but biographies or treatises for other topics. The more consistent style of the \"Zizhi Tongjian\" was not followed by later official histories. In the mid 13th century, Ouyang Xiu was heavily influenced by the work of Xue Juzheng. This led to the creation of the \"New History of the Five Dynasties\", which covered five dynasties in over 70 chapters.\nToward the end of the Qing dynasty in the early 20th century, scholars looked to Japan and the West for models. In the late 1890s, although deeply learned in the traditional forms, Liang Qichao began to publish extensive and influential studies and polemics that converted young readers to a new type of historiography that Liang regarded as more scientific. Liu Yizheng published several specialized history works including \"History of Chinese Culture\". This next generation became professional historians, training and teaching in universities. They included Chang Chi-yun, Gu Jiegang, Fu Sinian, and Tsiang Tingfu, who were PhDs from Columbia University; and Chen Yinke, who conducted his investigations into medieval Chinese history in both Europe and the United States. Other historians, such as Qian Mu, who was trained largely through independent study, were more conservative but remained innovative in their response to world trends. In the 1920s, wide-ranging scholars, such as Guo Moruo, adapted Marxism in order to portray China as a nation among nations, rather than having an exotic and isolated history. The ensuing years saw historians such as Wu Han master both Western theories, including Marxism, and Chinese learning.\nKey organizing concepts.\nDynastic cycle.\nLike the three ages of the Greek poet Hesiod, the oldest Chinese historiography viewed mankind as living in a fallen age of depravity, cut off from the virtues of the past, as Confucius and his disciples revered the sage kings Emperor Yao and Emperor Shun.\nUnlike Hesiod's system, however, the Duke of Zhou's idea of the Mandate of Heaven as a rationale for dethroning the supposedly divine Zi clan led subsequent historians to see man's fall as a cyclical pattern. In this view, a new dynasty is founded by a morally upright founder, but his successors cannot help but become increasingly corrupt and dissolute. This immorality removes the dynasty's divine favor and is manifested by natural disasters (particularly floods), rebellions, and foreign invasions. Eventually, the dynasty becomes weak enough to be replaced by a new one, whose founder is able to rectify many of society's problems and begin the cycle anew. Over time, many people felt a full correction was not possible, and that the golden age of Yao and Shun could not be attained.\nThis teleological theory implies that there can be only one rightful sovereign under heaven at a time. Thus, despite the fact that Chinese history has had many lengthy and contentious periods of disunity, a great effort was made by official historians to establish a legitimate precursor whose fall allowed a new dynasty to acquire its mandate. Similarly, regardless of the particular merits of individual emperors, founders would be portrayed in more laudatory terms, and the last ruler of a dynasty would always be castigated as depraved and unworthy, even when that was not the case. Such a narrative was employed after the fall of the empire, by those compiling the history of the Qing and by those who justified the attempted restorations of the imperial system by Yuan Shikai and Zhang Xun.\nMulti-ethnic history.\nAs early as the 1930s, the American scholar Owen Lattimore argued that China was the product of the interaction of farming and pastoral societies, rather than simply the expansion of the Han people. Lattimore did not accept the more extreme Sino-Babylonian theories that the essential elements of early Chinese technology and religion had come from Western Asia, but he was among the scholars to argue against the assumption they had all been indigenous.\nBoth the Republic of China and the People's Republic of China hold the view that Chinese history should include all the ethnic groups of the lands held by the Qing dynasty during its territorial peak, with these ethnicities forming part of the \"Zhonghua minzu\" (Chinese nation). This view is in contrast with Han chauvinism promoted by the Qing-era Tongmenghui. This expanded view encompasses internal and external tributary lands, as well as conquest dynasties in the history of a China seen as a coherent multi-ethnic nation since time immemorial, incorporating and accepting the contributions and cultures of non-Han ethnicities.\nThe acceptance of this view by ethnic minorities sometimes depends on their views on present-day issues. The 14th Dalai Lama, long insistent on Tibet's history being separate from that of China, conceded in 2005 that Tibet \"is a part of\" China's \"5,000-year history\" as part of a new proposal for Tibetan autonomy. Korean nationalists have virulently reacted against China's application to UNESCO for recognition of the Goguryeo tombs in Chinese territory. The absolute independence of Goguryeo is a central aspect of Korean identity, because, according to Korean legend, Goguryeo was independent of China and Japan, compared to subordinate states such as the Joseon dynasty and the Korean Empire. The legacy of Genghis Khan has been contested between China, Mongolia, and Russia, all three states having significant numbers of ethnic Mongols within their borders and holding territory that was conquered by the Khan.\nThe Jin dynasty tradition of a new dynasty composing the official history for its preceding dynasty/dynasties has been seen to foster an ethnically inclusive interpretation of Chinese history. The compilation of official histories usually involved monumental intellectual labor. The Yuan and Qing dynasties, ruled by the Mongols and Manchus, faithfully carried out this practice, composing the official Chinese-language histories of the Han-ruled Song and Ming dynasties, respectively. Had these two non-Han imperial dynasties not thought of themselves as continuing the Mandate of Heaven of China, it would be hard to explain why they retained the costly tradition. Thus, every non-Han dynasty saw itself as the legitimate holder of the Mandate of Heaven, which legitimized the dynastic cycle regardless of their social or ethnic background.\nRecent Western scholars have reacted against the ethnically inclusive narrative in Communist-sponsored history, by writing revisionist histories of China such as the New Qing History that feature, according to James A. Millward, \"a degree of 'partisanship' for the indigenous underdogs of frontier history\". Scholarly interest in writing about Chinese minorities from non-Chinese perspectives is growing.\nMarxism.\nMost Chinese history that is published in the People's Republic of China is based on a Marxist interpretation of history. These theories were first applied in the 1920s by Chinese scholars such as Guo Moruo, and became orthodoxy in academic study after 1949. The Marxist view of history is that history is governed by universal laws and that according to these laws, a society moves through a series of stages, with the transition between stages being driven by class struggle. These stages are:\nThe official historical view within the People's Republic of China associates each of these stages with a particular era in Chinese history.\nBecause of the strength of the Chinese Communist Party and the importance of the Marxist interpretation of history in legitimizing its rule, it was for many years difficult for historians within the PRC to actively argue in favor of non-Marxist and anti-Marxist interpretations of history. However, this political restriction is less confining than it may first appear in that the Marxist historical framework is surprisingly flexible, and it is a rather simple matter to modify an alternative historical theory to use language that at least does not challenge the Marxist interpretation of history.\nPartly because of the interest of Mao Zedong, historians in the 1950s took a special interest in the role of peasant rebellions in Chinese history and compiled documentary histories to examine them.\nThere are several problems associated with imposing Marx's European-based framework on Chinese history. First, slavery existed throughout China's history but never as the primary form of labor. While the Zhou and earlier dynasties may be labeled as feudal, later dynasties were much more centralized than how Marx analyzed their European counterparts as being. To account for the discrepancy, Chinese Marxists invented the term \"bureaucratic feudalism\". The placement of the Tang as the beginning of the bureaucratic phase rests largely on the replacement of patronage networks with the imperial examination. Some world-systems analysts, such as Janet Abu-Lughod, claim that analysis of Kondratiev waves shows that capitalism first arose in Song dynasty China, although widespread trade was subsequently disrupted and then curtailed.\nThe Japanese scholar Tanigawa Michio, writing in the 1970s and 1980s, set out to revise the generally Marxist views of China prevalent in post-war Japan. Tanigawa writes that historians in Japan fell into two schools. One held that China followed the set European pattern which Marxists thought to be universal; that is, from ancient slavery to medieval feudalism to modern capitalism; while another group argued that \"Chinese society was extraordinarily saturated with stagnancy, as compared to the West\" and assumed that China existed in a \"qualitatively different historical world from Western society\". That is, there is an argument between those who see \"unilinear, monistic world history\" and those who conceive of a \"two-tracked or multi-tracked world history\". Tanigawa reviewed the applications of these theories in Japanese writings about Chinese history and then tested them by analyzing the Six Dynasties 220\u2013589 CE period, which Marxist historians saw as feudal. His conclusion was that China did not have feudalism in the sense that Marxists use, that Chinese military governments did not lead to a European-style military aristocracy. The period established social and political patterns which shaped China's history from that point on.\nThere was a gradual relaxation of Marxist interpretation after the death of Mao in 1976, which was accelerated after the Tian'anmen Square protest and other revolutions in 1989, which damaged Marxism's ideological legitimacy in the eyes of Chinese academics.\nModernization.\nThis view of Chinese history sees Chinese society as a traditional society needing to become modern, usually with the implicit assumption of Western society as the model. Such a view was common among British and French scholars during the 19th and early 20th centuries but is now typically dismissed as eurocentrism, since such a view permits an implicit justification for breaking the society from its static past and bringing it into the modern world under European direction.\nBy the mid-20th century, it was increasingly clear to historians that the notion of \"changeless China\" was untenable. A new concept, popularized by John Fairbank, was the notion of \"change within tradition\", which argued that China did change in the pre-modern period but that this change existed within certain cultural traditions. This notion has also been subject to the criticism that to say \"China has not changed fundamentally\" is tautological, since it requires that one look for things that have not changed and then arbitrarily define those as fundamental.\nNonetheless, studies seeing China's interaction with Europe as the driving force behind its recent history are still common. Such studies may consider the First Opium War as the starting point for China's modern period. Examples include the works of H.B. Morse, who wrote chronicles of China's international relations such as \"Trade and Relations of the Chinese Empire\".\nIn the 1950s, several of Fairbank's students argued that Confucianism was incompatible with modernity. Joseph Levenson and Mary C. Wright, and Albert Feuerwerker argued in effect that traditional Chinese values were a barrier to modernity and would have to be abandoned before China could make progress. Wright concluded, \"The failure of the T'ung-chih [\"Tongzhi\"] Restoration demonstrated with a rare clarity that even in the most favorable circumstances there is no way in which an effective modern state can be grafted onto a Confucian society. Yet in the decades that followed, the political ideas that had been tested and, for all their grandeur, found wanting, were never given a decent burial.\"\nIn a different view of modernization, the Japanese historian Naito Torajiro argued that China reached modernity during its mid-Imperial period, centuries before Europe. He believed that the reform of the civil service into a meritocratic system and the disappearance of the ancient Chinese nobility from the bureaucracy constituted a modern society. The problem associated with this approach is the subjective meaning of modernity. The Chinese nobility had been in decline since the Qin dynasty, and while the exams were largely meritocratic, performance required time and resources that meant examinees were still typically from the gentry. Moreover, expertise in the Confucian classics did not guarantee competent bureaucrats when it came to managing public works or preparing a budget. Confucian hostility to commerce placed merchants at the bottom of the four occupations, itself an archaism maintained by devotion to classic texts. The social goal continued to be to invest in land and enter the gentry, ideas more like those of the physiocrats than those of Adam Smith.\nHydraulic despotism.\nWith ideas derived from Marx and Max Weber, Karl August Wittfogel argued that bureaucracy arose to manage irrigation systems. Despotism was needed to force the people into building canals, dikes, and waterways to increase agriculture. Yu the Great, one of China's legendary founders, is known for his control of the floods of the Yellow River. The hydraulic empire produces wealth from its stability; while dynasties may change, the structure remains intact until destroyed by modern powers. In Europe abundant rainfall meant less dependence on irrigation. In the Orient natural conditions were such that the bulk of the land could not be cultivated without large-scale irrigation works. As only a centralized administration could organize the building and maintenance of large-scale systems of irrigation, the need for such systems made bureaucratic despotism inevitable in Oriental lands.\nWhen Wittfogel published his \"\", critics pointed out that water management was given the high status China accorded to officials concerned with taxes, rituals, or fighting off bandits. The theory also has a strong orientalist bent, regarding all Asian states as generally the same while finding reasons for European polities not fitting the pattern.\nWhile Wittfogel's theories were not popular among Marxist historians in China, the economist Chi Ch'ao-ting used them in his influential 1936 book, \"Key Economic Areas in Chinese History, as Revealed in the Development of Public Works for Water-Control\". The book identified key areas of grain production which, when controlled by a strong political power, permitted that power to dominate the rest of the country and enforce periods of stability.\nConvergence.\nConvergence theory, including Hu Shih and Ray Huang's involution theory, holds that the past 150 years have been a period in which Chinese and Western civilization have been in the process of converging into a world civilization. Such a view is heavily influenced by modernization theory but, in China's case, it is also strongly influenced by indigenous sources such as the notion of \"Shijie Datong\" or \"Great Unity\". It has tended to be less popular among more recent historians, as postmodern Western historians discount overarching narratives, and nationalist Chinese historians feel similar about narratives failing to account for some special or unique characteristics of Chinese culture.\nAnti-imperialism.\nClosely related are colonial and anti-imperialist narratives. These often merge or are part of Marxist critiques from within China or the former Soviet Union, or are postmodern critiques such as Edward Said's \"Orientalism\", which fault traditional scholarship for trying to fit West, South, and East Asia's histories into European categories unsuited to them. With regard to China particularly, T.F. Tsiang and John Fairbank used newly opened archives in the 1930s to write modern history from a Chinese point of view. Fairbank and Teng Ssu-yu then edited the influential volume \"China's Response to the West\" (1953). This approach was attacked for ascribing the change in China to outside forces. In the 1980s, Paul Cohen, a student of Fairbank's, issued a call for a more \"China-Centered history of China\".\nRepublican.\nThe schools of thought on the 1911 Revolution have evolved from the early years of the Republic. The Marxist view saw the events of 1911 as a bourgeois revolution. In the 1920s, the Nationalist Party issued a theory of three political stages based on Sun Yatsen's writings:\nThe most obvious criticism is the near-identical nature of \"political tutelage\" and of a \"constitutional democracy\" consisting only of the one-party rule until the 1990s. Against this, Chen Shui-bian proposed his own four-stage theory.\nPostmodernism.\nPostmodern interpretations of Chinese history tend to reject narrative history and instead focus on a small subset of Chinese history, particularly the daily lives of ordinary people in particular locations or settings.\nRecent trends.\nFrom the beginning of Communist rule in 1949 until the 1980s, Chinese historical scholarship focused largely on the officially sanctioned Marxist theory of class struggle. From the time of Deng Xiaoping (1978\u20131992) on, there has been a drift towards a Marxist-inspired nationalist perspective, and consideration of China's contemporary international status has become of paramount importance in historical studies. The current focus tends to be on specifics of civilization in ancient China, and the general paradigm of how China has responded to the dual challenges of interactions with the outside world and modernization in the post-1700 era. Long abandoned as a research focus among most Western scholars due to postmodernism's influence, this remains the primary interest for most historians inside China.\nThe late 20th century and early 21st century have seen numerous studies of Chinese history that challenge traditional paradigms. The field is rapidly evolving, with much new scholarship, often based on the realization that there is much about Chinese history that is unknown or controversial. For example, an active topic concerns whether the typical Chinese peasant in 1900 was seeing his life improve. In addition to the realization that there are major gaps in our knowledge of Chinese history is the equal realization that there are tremendous quantities of primary source material that have not yet been analyzed. Scholars are using previously overlooked documentary evidence, such as masses of government and family archives, and economic records such as census tax rolls, price records, and land surveys. In addition, artifacts such as vernacular novels, how-to manuals, and children's books are analyzed for clues about day-to-day life.\nRecent Western scholarship of China has been heavily influenced by postmodernism, and has questioned modernist narratives of China's backwardness and lack of development. The desire to challenge the preconception that 19th-century China was weak, for instance, has led to a scholarly interest in Qing expansion into Central Asia. Postmodern scholarship largely rejects grand narratives altogether, preferring to publish empirical studies on the socioeconomics, and political or cultural dynamics, of smaller communities within China.\nNationalism.\nIn China, historical scholarship remains largely nationalist and modernist or even traditionalist. The legacies of the modernist school (such as Lo Hsiang-lin) and the traditionalist school (such as Qian Mu (Chien Mu)) remain strong in Chinese circles. The more modernist works focus on imperial systems in China and employ the scientific method to analyze epochs of Chinese dynasties from geographical, genealogical, and cultural artifacts: for example, using Carbon-14 dating and geographical records to correlate climates with cycles of calm and calamity in Chinese history. The traditionalist school of scholarship resorts to official imperial records and colloquial historical works, and analyzes the rise and fall of dynasties using Confucian philosophy, albeit modified by an institutional administration perspective.\nAfter 1911, writers, historians and scholars in China and abroad generally deprecated the late imperial system and its failures. However, in the 21st century, a highly favorable revisionism has emerged in the popular culture, in both the media and social media. Building pride in Chinese history, nationalists have portray Imperial China as benevolent, strong and more advanced than the West. They blame ugly wars and diplomatic controversies on imperialist exploitation by Western nations and Japan. Although officially still communist and Maoist, in practice China's rulers have used this grassroots settlement to proclaim their current policies are restoring China's historical glory. General Secretary Xi Jinping has, \"sought nothing less than parity between Beijing and Washington--and promised to restore China to its historical glory.\" \nFlorian Schneider argues that nationalism in China in the early twenty-first century is largely a product of the digital revolution and that a large fraction of the population participates as readers and commentators who relate ideas to their friends over the internet."}
{"id": "7175", "revid": "8761551", "url": "https://en.wikipedia.org/wiki?curid=7175", "title": "Chinese Communist Party", "text": "The Communist Party of China (CPC), commonly known as the Chinese Communist Party (CCP), is the founding and sole governing political party of the People's Republic of China (PRC). The CCP leads eight other legally permitted subordinate minor parties together as the United Front. The CCP was founded in 1921, with the help of the Far Eastern Bureau of the Communist Party of the Soviet Union and Far Eastern Secretariat of the Communist International under Lenin. The party grew quickly, and by 1949 it had driven the Kuomintang (KMT)'s Nationalist Government from mainland China to Taiwan after the Chinese Civil War, leading to the establishment of the People's Republic of China on 1 October 1949. It controls the country's armed forces, the People's Liberation Army (PLA).\nThe CCP is officially organized on the basis of democratic centralism, a principle conceived by Russian Marxist theoretician Vladimir Lenin which entails a democratic and open discussion on policy on the condition of unity in upholding the agreed-upon policies. Theoretically, the highest body of the CCP is the National Congress, convened every fifth year. When the National Congress is not in session, the Central Committee is the highest body, but since the body meets normally only once a year most duties and responsibilities are vested in the Politburo and its Standing Committee, members of the latter seen as the top leadership of the Party and the State. The party's leader recently holds the offices of general secretary (responsible for civilian party duties), Chairman of the Central Military Commission (CMC) (responsible for military affairs) and State President (a largely ceremonial position). Through these posts, the party leader is the country's paramount leader. The current leader is general secretary Xi Jinping, elected at the 18th Central Committee held on 15 November 2012.\nOfficially, the CCP is committed to communism and continues to participate in the International Meeting of Communist and Workers' Parties each year. According to the party constitution, the CCP adheres to Marxism\u2013Leninism, Mao Zedong Thought, socialism with Chinese characteristics, Deng Xiaoping Theory, the Three Represents, the Scientific Outlook on Development, and Xi Jinping Thought. The official explanation for China's economic reforms is that the country is in the primary stage of socialism, a developmental stage similar to the capitalist mode of production. The command economy established under Mao Zedong was replaced by the socialist market economy under Deng Xiaoping, the current economic system, on the basis that \"Practice is the Sole Criterion for the Truth\".\nSince the collapse of Eastern European communist governments in 1989\u20131990 and the dissolution of the Soviet Union in 1991, the CCP has emphasized its party-to-party relations with the ruling parties of the remaining socialist states. While the CCP still maintains party-to-party relations with non-ruling communist parties around the world, since the 1980s it has established relations with several non-communist parties, most notably with ruling parties of one-party states (whatever their ideology), dominant parties in democracies (whatever their ideology) and social democratic parties. With more than 91 million members, the CCP is the second largest political party in the world after India's Bharatiya Janata Party.\nHistory.\nFounding and early history (1921\u20131927).\nThe CCP claims its origins in the May Fourth Movement of 1919, during which radical Western ideologies like Marxism and anarchism gained traction among Chinese intellectuals. Other influences stemming from the Bolshevik revolution and Marxist theory inspired the Communist Party of China. Chen Duxiu and Li Dazhao were among the first leading Chinese intellectuals who publicly supported Leninism and world revolution. In contrast to Chen, Li did not renounce participation in the affairs of the Republic of China. Both of them regarded the October Revolution in Russia as groundbreaking, believing it to herald a new era for oppressed countries everywhere. The CCP was modeled on Vladimir Lenin's theory of a vanguard party. Study circles were, according to Cai Hesen, \"the rudiments [of our party]\". Several study circles were established during the New Culture Movement, but \"by 1920 skepticism about their suitability as vehicles for reform had become widespread.\"\nIn the summer of 1919, the Russian Communist Party (Bolsheviks) decided to assist people of the Far East. In April 1920, the Foreign Affairs Division of its Vladivostok Branch sent Voitinsky to develop Marxism in China, Korea and Japan. Voitinsky met Li, and then successfully turned Chen into a communist. Voitinsky found the Far Eastern Secretariat of the Communist International (Comintern) at Shanghai. On 5 July, he attended a meeting of Russian communists in China to promote the establishment of the CCP. He helped Chen found the Shanghai Revolutionary Bureau, also known as the Shanghai Communist Group. Stojanovic went to Guangzhou, Mamaev went to Wuhan, and Broway went to Beijing to help Chinese establish communist groups. Voitinsky provided these groups with promotional, conference and study abroad expenses.\nThe founding National Congress of the CCP was held on 23\u201331 July 1921. With only 50 members in the beginning of 1921, the CCP organization and authorities grew tremendously. While it was originally held in a house in the Shanghai French Concession, French police interrupted the meeting on 30 July and the congress was moved to a tourist boat on South Lake in Jiaxing, Zhejiang province. A dozen delegates attended the congress, with neither Li nor Chen being able to attend, the latter sending a personal representative in his stead. The resolutions of the congress called for the establishment of a communist party (as a branch of the Communist International) and elected Chen as its leader. Chen then served as the first general secretary of the Communist Party and was referred to as \"China's Lenin\".\nThe Russians eagerly wanted to foster pro-Russian forces in the Far East to fight against anti-communist countries, especially Japan. They tried to contact Wu Peifu, but failed. Then they found the KMT, a party founded by Sun Yat-sen, which was leading the Guangzhou government to confront the Northern government. On 6 October 1923, the Comintern sent Mikhail Borodin to Guangzhou. Then the communists fully supported the KMT. The Central Committee of the CCP, Joseph Stalin, and the Comintern all hoped that the CCP would control the KMT and called their opponents \"rightists\". Sun eased the conflict between the communists and their opponents. CCP members grew tremendously after the 4th congress, from 900 to 2,428 in year 1925. The CCP still treats Sun Yat-sen as one of the founders of their movement and claim descent from him as he is viewed as a proto communist and the economic element of Sun's ideology was socialism. Sun stated, \u201cOur Principle of Livelihood is a form of communism\u201d.\nThe communists dominated the left-wing of the KMT, a party organized on Leninist lines, struggling for power with the party's right wing. When KMT leader Sun Yat-sen died in March 1925, he was succeeded by a rightist, Chiang Kai-shek, who initiated moves to marginalize the position of the communists. Chiang, Sun's former assistant, was not actively anti-communist at that time, even though he hated the theory of class struggle and the CCP's seizure of power. The communists proposed removing Chiang's power. When Chiang gradually gained the support of Western countries, the conflict between him and the communists became more and more intense. Chiang asked the Kuomintang to join the Communist International in order to rule out the secret expansion of communists in the KMT, while Chen Duxiu hoped that the communists would completely withdraw from the KMT.\nIn April 1927, both Chiang and the CCP were preparing for combat. Fresh from the success of the Northern Expedition to overthrow the warlords, Chiang Kai-shek turned on the communists, who by now numbered in the tens of thousands across China. Ignoring the orders of the Wuhan-based KMT government, he marched on Shanghai, a city controlled by communist militias. Although the communists welcomed Chiang's arrival, he turned on them, massacring 5,000 with the aid of the Green Gang. Chiang's army then marched on Wuhan, but was prevented from taking the city by CCP General Ye Ting and his troops. Chiang's allies also attacked communists; in Beijing, Li Dazhao and 19 other leading communists were executed by Zhang Zuolin, while in Changsha, He Jian's forces machine gunned hundreds of peasant militiamen. Affected by this stimulus, the peasant movement supported by the CCP became more cruel. , a famous scholar, was killed by the communists. gunned hundreds of peasant militiamen, as revenge. That May, tens of thousands of communists and their sympathizers were killed by nationalists, with the CCP losing approximately of its members.\nChinese Civil War and Second Sino-Japanese War (1927\u20131949).\nThe CCP continued supporting the Wuhan KMT government, but on 15 July 1927 the Wuhan government expelled all communists from the KMT. The CCP reacted by founding the Workers' and Peasants' Red Army of China, better known as the \"Red Army\", to battle the KMT. A battalion led by General Zhu De was ordered to take the city of Nanchang on 1 August 1927 in what became known as the Nanchang uprising; initially successful, they were forced into retreat after five days, marching south to Shantou, and from there being driven into the wilderness of Fujian. Mao Zedong was appointed commander-in-chief of the Red Army, and led four regiments against Changsha in the Autumn Harvest Uprising, hoping to spark peasant uprisings across Hunan. His plan was to attack the KMT-held city from three directions on 9 September, but the Fourth Regiment deserted to the KMT cause, attacking the Third Regiment. Mao's army made it to Changsha, but could not take it; by 15 September, he accepted defeat, with 1,000 survivors marching east to the Jinggang Mountains of Jiangxi.\nThe near-destruction of the CCP's urban organizational apparatus led to institutional changes within the party. The party adopted democratic centralism, a way to organize revolutionary parties, and established a Politburo (to function as the standing committee of the Central Committee). The result was increased centralization of power within the party . At every level of the party this was duplicated, with standing committees now in effective control. After being expelled from the party, Chen Duxiu went on to lead China's Trotskyist movement. Li Lisan was able to assume \"de facto\" control of the party organization by 1929\u201330. Li Lisan's leadership was a failure, leaving the CCP on the brink of destruction. The Comintern became involved, and by late 1930, his powers had been taken away. By 1935 Mao had become the party's Politburo Standing Committee member and informal military leader, with Zhou Enlai and Zhang Wentian, the formal head of the party, serving as his informal deputies. The conflict with the KMT led to the reorganization of the Red Army, with power now centralized in the leadership through the creation of CCP political departments charged with supervising the army.\nThe Second Sino-Japanese War caused a pause in the conflict between the CCP and the KMT. The Second United Front was established between the CCP and the KMT to tackle the invasion. While the front formally existed until 1945, all collaboration between the two parties had ended by 1940. Despite their formal alliance, the CCP used the opportunity to expand and carve out independent bases of operations to prepare for the coming war with the KMT. In 1939 the KMT began to restrict CCP expansion within China. This led to frequent clashes between CCP and KMT forces but which subsided rapidly on the realisation on both sides that civil war was not an option. Yet, by 1943, the CCP was again actively expanding its territory at the expense of the KMT.\nMao Zedong became the Chairman of the Chinese Communist Party in 1945. From 1945 until 1949, the war had been reduced to two parties; the CCP and the KMT. This period lasted through four stages; the first was from August 1945 (when the Japanese surrendered) to June 1946 (when the peace talks between the CCP and the KMT ended). By 1945, the KMT had three-times more soldiers under its command than the CCP and initially appeared to be prevailing. With the cooperation of the Americans and the Japanese, the KMT was able to retake major parts of the country. However, KMT rule over the reconquered territories would prove unpopular because of endemic party corruption. Notwithstanding its huge numerical superiority, the KMT failed to reconquer the rural territories which made up the CCP's stronghold. Around the same time, the CCP launched an invasion of Manchuria, where they were assisted by the Soviet Union. The second stage, lasting from July 1946 to June 1947, saw the KMT extend its control over major cities, such as Yan'an (the CCP headquarters for much of the war). The KMT's successes were hollow; the CCP had tactically withdrawn from the cities, and instead attacked KMT authorities by instigating protests amongst students and intellectuals in the cities (the KMT responded to these events with heavy-handed repression). In the meantime, the KMT was struggling with factional infighting and Chiang Kai-shek's autocratic control over the party, which weakened the KMT's ability to respond to attacks. The third stage, lasting from July 1947 to August 1948, saw a limited counteroffensive by the CCP. The objective was clearing \"Central China, strengthening North China, and recovering Northeast China.\" This policy, coupled with desertions from the KMT military force (by the spring of 1948, the KMT military had lost an estimated 2 of its 3 million troops) and declining popularity of KMT rule. The result was that the CCP was able to cut off KMT garrisons in Manchuria and retake several lost territories. The last stage, lasting from September 1948 to December 1949, saw the communists take the initiative and the collapse of KMT rule in mainland China as a whole. On 1 October 1949, Mao declared the establishment of the PRC, which signified the end of the Chinese Revolution (as it is officially described by the CCP).\nFounding the PRC (1949) and becoming the sole ruling party (1954\u2013present).\nOn 1 October 1949, Chairman Mao Zedong formally proclaimed the establishment of the PRC before a massive crowd at Tiananmen Square. The CCP headed the Central People's Government. From this time through the 1980s, top leaders of the CCP (like Mao Zedong, Lin Biao, Zhou Enlai and Deng Xiaoping) were largely the same military leaders prior to the PRC's founding. As a result, informal personal ties between political and military leaders dominated civil-military relations.\nStalin proposed a one-party constitution when Liu Shaoqi visited the Soviet Union in 1952. Then the Constitution of the PRC in 1954 changed the previous coalition government and established the CCP's sole ruling system. Mao said that China should implement a multi-party system under the leadership of the working class revolutionary party (CCP) on the CCP's 8th Congress in 1956. He had not proposed that other parties should be led before, although the CCP had actually controlled the most political power since 1949. In 1957, the CCP launched the Anti-Rightist Campaign against the political dissents and figures of the other minor parties which resulted in the political persecution of at least 550,000 people. The campaign significantly damaged the limited pluralistic nature in the socialist republic and turned the country into a \"de facto\" one-party state. The event led to the catastrophic results of the Second Five Year from 1958 when the CCP attempted at transforming the country from an agrarian into an industrialized economy through the formation of people's communes by launching the Great Leap Forward campaign. The Great Leap resulted in tens of millions of deaths, with estimates ranging between 15 and 55\u00a0million deaths, making the Great Chinese Famine the largest in human history.\nDuring the 1960s and 1970s, the CCP experienced a significant ideological separation from the Communist Party of the Soviet Union which was going through the De-Stalinization under Nikita Khrushchev. By that time, Mao had begun saying that the \"continued revolution under the dictatorship of the proletariat\" stipulated that class enemies continued to exist even though the socialist revolution seemed to be complete, leading to the Cultural Revolution in which millions were persecuted and killed. In the Cultural Revolution, party leaders such as Liu Shaoqi, Deng Xiaoping, Peng Dehuai, and He Long, were purged or exiled and the power were fallen into the Gang of Four led by Jiang Qing, Mao's wife.\nFollowing Mao's death in 1976, a power struggle between CCP chairman Hua Guofeng and vice-chairman Deng Xiaoping erupted. Deng won the struggle, and became the \"paramount leader\" in 1978. Deng, alongside Hu Yaobang and Zhao Ziyang, spearheaded the Reform and opening policy, and introduced the ideological concept of socialism with Chinese characteristics, opening China to the world's markets. In reversing some of Mao's \"leftist\" policies, Deng argued that a socialist state could use the market economy without itself being capitalist. While asserting the political power of the Party, the change in policy generated significant economic growth. The new ideology, however, was contested on both sides of the spectrum, by Maoists as well as by those supporting political liberalization. With other social factors, the conflicts culminated in the 1989 Tiananmen Square protests. The protests having been crushed and the reformist party general secretary Zhao Ziyang under house arrest, Deng's economic policies resumed and by the early 1990s the concept of a socialist market economy had been introduced. In 1997, Deng's beliefs (Deng Xiaoping Theory), were embedded in the CCP constitution.\nCCP general secretary Jiang Zemin succeeded Deng as \"paramount leader\" in the 1990s, and continued most of his policies. In the 1990s, the CCP transformed from a veteran revolutionary leadership that was both leading militarily and politically, to a political elite increasingly regenerated according to institutionalized norms in the civil bureaucracy. Leadership was largely selected based on rules and norms on promotion and retirement, educational background, and managerial and technical expertise. There is a largely separate group of professionalized military officers, serving under top CCP leadership largely through formal relationships within institutional channels.\nAs part of Jiang Zemin's nominal legacy, the CCP ratified the Three Represents for the 2003 revision of the party's constitution, as a \"guiding ideology\" to encourage the party to represent \"advanced productive forces, the progressive course of China's culture, and the fundamental interests of the people.\" The theory legitimized the entry of private business owners and bourgeois elements into the party. Hu Jintao, Jiang Zemin's successor as general secretary, took office in 2002. Unlike Mao, Deng and Jiang Zemin, Hu laid emphasis on collective leadership and opposed one-man dominance of the political system. The insistence on focusing on economic growth led to a wide range of serious social problems. To address these, Hu introduced two main ideological concepts: the Scientific Outlook on Development and Harmonious Socialist Society. Hu resigned from his post as CCP general secretary and Chairman of the CMC at the 18th National Congress held in 2012, and was succeeded in both posts by Xi Jinping.\nSince taking power, Xi has initiated a wide-reaching anti-corruption campaign, while centralizing powers in the office of CCP general secretary at the expense of the collective leadership of prior decades. Commentators have described the campaign as a defining part of Xi's leadership as well as \"the principal reason why he has been able to consolidate his power so quickly and effectively.\" Foreign commentators have likened him to Mao. Xi's leadership has also overseen an increase in the Party's role in China. Xi has added his ideology, named after himself, into the CCP constitution in 2017. As has been speculated, Xi Jinping may not retire from his top posts after serving for 10 years in 2022.\nOn 1 October 2020, U.S. Congressman Scott Perry introduced legislation to add the CCP to the Top International Criminal Organizations Target (TICOT) List and provide the United States law enforcement agencies a strategic directive to target the CCP's activity.\nOn 21 October 2020, the Subcommittee on International Human Rights (SDIR) of the Canadian House of Commons Standing Committee on Foreign Affairs and International Development condemned the persecution of Uyghurs and other Turkic Muslims in Xinjiang by the Government of China and concluded that the Chinese Communist Party's actions amount to genocide of the Uyghurs per the Genocide Convention.\nIdeology.\nIt has been argued in recent years, mainly by foreign commentators, that the CCP does not have an ideology, and that the party organization is pragmatic and interested only in what works. The party itself, however, argues otherwise. For instance, Hu Jintao stated in 2012 that the Western world is \"threatening to divide us\" and that \"the international culture of the West is strong while we are weak ... Ideological and cultural fields are our main targets\". The CCP puts a great deal of effort into the party schools and into crafting its ideological message. Before the \"\" campaign, the relationship between ideology and decision-making was a deductive one, meaning that policy-making was derived from ideological knowledge. Under Deng this relationship was turned upside down, with decision-making justifying ideology and not the other way around. Lastly, Chinese policy-makers believe that the Soviet Union's state ideology was \"rigid, unimaginative, ossified, and disconnected from reality\" and that this was one of the reasons for the dissolution of the Soviet Union. They therefore believe that their party ideology must be dynamic to safeguard the party's rule.\nFormal ideology.\nMarxism\u2013Leninism was the first official ideology of the Communist Party of China. According to the CCP, \"Marxism\u2013Leninism reveals the universal laws governing the development of history of human society.\" To the CCP, Marxism\u2013Leninism provides a \"vision of the contradictions in capitalist society and of the inevitability of a future socialist and communist societies\". According to the \"People's Daily\", Mao Zedong Thought \"is Marxism\u2013Leninism applied and developed in China\". Mao Zedong Thought was conceived not only by Mao Zedong, but by leading party officials.\nWhile non-Chinese analysts generally agree that the CCP has rejected orthodox Marxism\u2013Leninism and Mao Zedong Thought (or at least basic thoughts within orthodox thinking), the CCP itself disagrees. Certain groups argue that Jiang Zemin ended the CCP's formal commitment to Marxism with the introduction of the ideological theory, the Three Represents. However, party theorist Leng Rong disagrees, claiming that \"President Jiang rid the Party of the ideological obstacles to different kinds of ownership [...] He did not give up Marxism or socialism. He strengthened the Party by providing a modern understanding of Marxism and socialism\u2014which is why we talk about a 'socialist market economy' with Chinese characteristics.\" The attainment of true \"communism\" is still described as the CCP's and China's \"ultimate goal\". While the CCP claims that China is in the primary stage of socialism, party theorists argue that the current development stage \"looks a lot like capitalism\". Alternatively, certain party theorists argue that \"capitalism is the early or first stage of communism.\" Some have dismissed the concept of a primary stage of socialism as intellectual cynicism. According to Robert Lawrence Kuhn, a China analyst, \"When I first heard this rationale, I thought it more comic than clever\u2014a wry caricature of hack propagandists leaked by intellectual cynics. But the 100-year horizon comes from serious political theorists\".\nDeng Xiaoping Theory was added to the party constitution at the 14th National Congress. The concepts of \"socialism with Chinese characteristics\" and \"the primary stage of socialism\" were credited to the theory. Deng Xiaoping Theory can be defined as a belief that state socialism and state planning is not by definition communist, and that market mechanisms are class neutral. In addition, the party needs to react to the changing situation dynamically; to know if a certain policy is obsolete or not, the party had to \"seek truth from facts\" and follow the slogan \"practice is the sole criterion for the truth\". At the 14th National Congress, Jiang reiterated Deng's mantra that it was unnecessary to ask if something was socialist or capitalist, since the important factor was whether it worked.\nThe \"Three Represents\", Jiang Zemin's contribution to the party's ideology, was adopted by the party at the 16th National Congress. The Three Represents defines the role of the Communist Party of China, and stresses that the Party must always represent the requirements for developing China's advanced productive forces, the orientation of China's advanced culture and the fundamental interests of the overwhelming majority of the Chinese people.\" Certain segments within the CCP criticized the Three Represents as being un-Marxist and a betrayal of basic Marxist values. Supporters viewed it as a further development of socialism with Chinese characteristics. Jiang disagreed, and had concluded that attaining the communist mode of production, as formulated by earlier communists, was more complex than had been realized, and that it was useless to try to force a change in the mode of production, as it had to develop naturally, by following the economic laws of history. The theory is most notable for allowing capitalists, officially referred to as the \"new social strata\", to join the party on the grounds that they engaged in \"honest labor and work\" and through their labour contributed \"to build[ing] socialism with Chinese characteristics.\"\nThe 3rd Plenary Session of the 16th Central Committee conceived and formulated the ideology of the Scientific Outlook on Development (SOD). It is considered to be Hu Jintao's contribution to the official ideological discourse. The SOD incorporates scientific socialism, sustainable development, social welfare, a humanistic society, increased democracy, and, ultimately, the creation of a Socialist Harmonious Society. According to official statements by the CCP, the concept integrates \"Marxism with the reality of contemporary China and with the underlying features of our times, and it fully embodies the Marxist worldview on and methodology for development.\"\nXi Jinping Thought on Socialism with Chinese Characteristics for a New Era, commonly known as Xi Jinping Thought, was added to the party constitution in the 19th National Congress. Xi himself has described the thought as part of the broad framework created around socialism with Chinese characteristics. In official party documentation and pronouncements by Xi's colleagues, the thought is said to be a continuation of previous party ideologies as part of a series of guiding ideologies that embody \"Marxism adapted to Chinese conditions\" and contemporary considerations.\nEconomics.\nDeng did not believe that the fundamental difference between the capitalist mode of production and the socialist mode of production was central planning versus free markets. He said, \"A planned economy is not the definition of socialism, because there is planning under capitalism; the market economy happens under socialism, too. Planning and market forces are both ways of controlling economic activity\". Jiang Zemin supported Deng's thinking, and stated in a party gathering that it did not matter if a certain mechanism was capitalist or socialist, because the only thing that mattered was whether it worked. It was at this gathering that Jiang Zemin introduced the term socialist market economy, which replaced Chen Yun's \"planned socialist market economy\". In his report to the 14th National Congress Jiang Zemin told the delegates that the socialist state would \"let market forces play a basic role in resource allocation.\" At the 15th National Congress, the party line was changed to \"make market forces further play their role in resource allocation\"; this line continued until the of the 18th Central Committee, when it was amended to \"let market forces play a \"decisive\" role in resource allocation.\" Despite this, the 3rd Plenary Session of the 18th Central Committee upheld the creed \"Maintain the dominance of the public sector and strengthen the economic vitality of the State-owned economy.\"\nThe CCP views the world as organized into two opposing camps; socialist and capitalist. They insist that socialism, on the basis of historical materialism, will eventually triumph over capitalism. In recent years, when the party has been asked to explain the capitalist globalization occurring, the party has returned to the writings of Karl Marx. Despite admitting that globalization developed through the capitalist system, the party's leaders and theorists argue that globalization is not intrinsically capitalist. The reason being that if globalization was purely capitalist, it would exclude an alternative socialist form of modernity. Globalization, as with the market economy, therefore does not have one specific class character (neither socialist nor capitalist) according to the party. The insistence that globalization is not fixed in nature comes from Deng's insistence that China can pursue socialist modernization by incorporating elements of capitalism. Because of this there is considerable optimism within the CCP that despite the current capitalist dominance of globalization, globalization can be turned into a vehicle supporting socialism.\nGovernance.\nCollective leadership.\nCollective leadership, the idea that decisions will be taken through consensus, is the ideal in the CCP. The concept has its origins back to Vladimir Lenin and the Russian Bolshevik Party. At the level of the central party leadership this means that, for instance, all members of the Politburo Standing Committee are of equal standing (each member having only one vote). A member of the Politburo Standing Committee often represents a sector; during Mao's reign, he controlled the People's Liberation Army, Kang Sheng, the security apparatus, and Zhou Enlai, the State Council and the Ministry of Foreign Affairs. This counts as informal power. Despite this, in a paradoxical relation, members of a body are ranked hierarchically (despite the fact that members are in theory equal to one another). Informally, the collective leadership is headed by a \"leadership core\"; that is, the paramount leader, the person who holds the offices of CCP general secretary, CMC chairman and PRC president. Before Jiang Zemin's tenure as paramount leader, the party core and collective leadership were indistinguishable. In practice, the core was not responsible to the collective leadership. However, by the time of Jiang, the party had begun propagating a responsibility system, referring to it in official pronouncements as the \"core of the collective leadership\".\nDemocratic centralism.\nThe CCP's organizational principle is democratic centralism, which is based on two principles: democracy (synonymous in official discourse with \"socialist democracy\" and \"inner-party democracy\") and centralism. This has been the guiding organizational principle of the party since the 5th National Congress, held in 1927. In the words of the party constitution, \"The Party is an integral body organized under its program and constitution and on the basis of democratic centralism\". Mao once quipped that democratic centralism was \"at once democratic and centralized, with the two seeming opposites of democracy and centralization united in a definite form.\" Mao claimed that the superiority of democratic centralism lay in its internal contradictions, between democracy and centralism, and freedom and discipline. Currently, the CCP is claiming that \"democracy is the lifeline of the Party, the lifeline of socialism\". But for democracy to be implemented, and functioning properly, there needs to be centralization. The goal of democratic centralism was not to obliterate capitalism or its policies but instead it is the movement towards regulating capitalism while involving socialism and democracy. Democracy in any form, the CCP claims, needs centralism, since without centralism there will be no order. According to Mao, democratic centralism \"is centralized on the basis of democracy and democratic under centralized guidance. This is the only system that can give full expression to democracy with full powers vested in the people's congresses at all levels and, at the same time, guarantee centralized administration with the governments at each level exercising centralized management of all the affairs entrusted to them by the people's congresses at the corresponding level and safeguarding whatever is essential to the democratic life of the people\".\nShuanggui.\n\"Shuanggui\" is an intra-party disciplinary process conducted by the Central Commission for Discipline Inspection (CCDI). This formally independent internal control institution conducts \"shuanggui\" on members accused of \"disciplinary violations\", a charge which generally refers to political corruption. The process, which literally translates to \"double regulation\", aims to extract confessions from members accused of violating party rules. According to the Dui Hua Foundation, tactics such as cigarette burns, beatings and simulated drowning are among those used to extract confessions. Other reported techniques include the use of induced hallucinations, with one subject of this method reporting that \"In the end I was so exhausted, I agreed to all the accusations against me even though they were false.\"\nMulti-Party Cooperation System.\nThe Multi-Party Cooperation and Political Consultation System is led by the CCP in cooperation and consultation with the eight parties which make up the United Front. Consultation takes place under the leadership of the CCP, with mass organizations, the United Front parties, and \"representatives from all walks of life\". These consultations contribute, at least in theory, to the formation of the country's basic policy in the fields of political, economic, cultural and social affairs. The CCP's relationship with other parties is based on the principle of \"long-term coexistence and mutual supervision, treating each other with full sincerity and sharing weal or woe.\" This process is institutionalized in the Chinese People's Political Consultative Conference (CPPCC). All the parties in the United Front support China's road to socialism, and hold steadfast to the leadership of the CCP. Despite all this, the CPPCC is a body without any real power. While discussions do take place, they are all supervised by the CCP.\nOrganization.\nCentral organization.\nThe National Congress is the party's highest body, and, since the 9th National Congress in 1969, has been convened every five years (prior to the 9th Congress they were convened on an irregular basis). According to the party's constitution, a congress may not be postponed except \"under extraordinary circumstances.\" The party constitution gives the National Congress six responsibilities:\nIn practice, the delegates rarely discuss issues at length at the National Congresses. Most substantive discussion takes place before the congress, in the preparation period, among a group of top party leaders. In between National Congresses, the Central Committee is the highest decision-making institution. The CCDI is responsible for supervising party's internal anti-corruption and ethics system. In between congresses the CCDI is under the authority of the Central Committee.\nThe Central Committee, as the party's highest decision-making institution between national congresses, elects several bodies to carry out its work. The first plenary session of a newly elected central committee elects the general secretary of the Central Committee, the party's leader; the Central Military Commission (CMC); the Politburo; the Politburo Standing Committee (PSC); and since 2013, the Central National Security Commission (CNSC). The first plenum also endorses the composition of the Secretariat and the leadership of the CCDI. According to the party constitution, the general secretary must be a member of the Politburo Standing Committee (PSC), and is responsible for convening meetings of the PSC and the Politburo, while also presiding over the work of the Secretariat. The Politburo \"exercises the functions and powers of the Central Committee when a plenum is not in session\". The PSC is the party's highest decision-making institution when the Politburo, the Central Committee and the National Congress are not in session. It convenes at least once a week. It was established at the 8th National Congress, in 1958, to take over the policy-making role formerly assumed by the Secretariat. The Secretariat is the top implementation body of the Central Committee, and can make decisions within the policy framework established by the Politburo; it is also responsible for supervising the work of organizations that report directly into the Central Committee, for example departments, commissions, publications, and so on. The CMC is the highest decision-making institution on military affairs within the party, and controls the operations of the People's Liberation Army. The general secretary has, since Jiang Zemin, also served as Chairman of the CMC. Unlike the collective leadership ideal of other party organs, the CMC chairman acts as commander-in-chief with full authority to appoint or dismiss top military officers at will. The CNSC \"co-ordinates security strategies across various departments, including intelligence, the military, foreign affairs and the police in order to cope with growing challenges to stability at home and abroad.\" The general secretary serves as the Chairman of the CNSC.\nA first plenum of the Central Committee also elects heads of departments, bureaus, central leading groups and other institutions to pursue its work during a term (a \"term\" being the period elapsing between national congresses, usually five years). The General Office is the party's \"nerve centre\", in charge of day-to-day administrative work, including communications, protocol, and setting agendas for meetings. The CCP currently has four main central departments: the Organization Department, responsible for overseeing provincial appointments and vetting cadres for future appointments, the Publicity Department (formerly \"Propaganda Department\"), which oversees the media and formulates the party line to the media, the International Department, functioning as the party's \"foreign affairs ministry\" with other parties, and the United Front Work Department, which oversees work with the country's non-communist parties, mass organizations, and influence groups outside of the country. The CC also has direct control over the Central Policy Research Office, which is responsible for researching issues of significant interest to the party leadership, the Central Party School, which provides political training and ideological indoctrination in communist thought for high-ranking and rising cadres, the Party History Research Centre, which sets priorities for scholarly research in state-run universities and the Central Party School, and the Compilation and Translation Bureau, which studies and translates the classical works of Marxism. The party's newspaper, the \"People's Daily\", is under the direct control of the Central Committee and is published with the objectives \"to tell good stories about China and the (Party)\" and to promote its party leader. The theoretical magazines \"Seeking Truth from Facts\" and \"Study Times\" are published by the Central Party School. The various offices of the \"Central Leading Groups\", such as the Hong Kong and Macau Affairs Office, the Taiwan Affairs Office, and the Central Finance Office, also report to the central committee during a plenary session.\nLower-level organizations.\nAfter seizing political power, the CCP extended the dual party-state command system to all government institutions, social organizations, and economic entities. The State Council and the Supreme Court each has a party core group (\u515a\u7ec4), established since November 1949. Party committees permeate in every state administrative organ as well as the People's Consultation Conferences and mass organizations at all levels. Party committees exist inside of companies, both private and state-owned. Modeled after the Soviet Nomenklatura system, the party committee's organization department at each level has the power to recruit, train, monitor, appoint, and relocate these officials.\nParty committees exist at the level of provinces, cities, counties, and neighborhoods. These committees play a key role in directing local policy by selecting local leaders and assigning critical tasks. The Party secretary at each level is more senior than that of the leader of the government, with the CCP standing committee being the main source of power. Party committee members in each level are selected by the leadership in the level above, with provincial leaders selected by the central Organizational Department, and not removable by the local party secretary.\nIn theory, however, party committees are elected by party congresses at their own level. Local party congresses are supposed to be held every fifth year, but under extraordinary circumstances they may be held earlier or postponed. However that decision must be approved by the next higher level of the local party committee. The number of delegates and the procedures for their election are decided by the local party committee, but must also have the approval of the next higher party committee.\nA local party congress has many of the same duties as the National Congress, and it is responsible for examining the report of the local Party Committee at the corresponding level; examining the report of the local Commission for Discipline Inspection at the corresponding level; discussing and adopting resolutions on major issues in the given area; and electing the local Party Committee and the local Commission for Discipline Inspection at the corresponding level. Party committees of \"a province, autonomous region, municipality directly under the central government, city divided into districts, or autonomous prefecture [are] elected for a term of five years\", and include full and alternate members. The party committees \"of a county (banner), autonomous county, city not divided into districts, or municipal district [are] elected for a term of five years\", but full and alternate members \"must have a Party standing of three years or more.\" If a local Party Congress is held before or after the given date, the term of the members of the Party Committee shall be correspondingly shortened or lengthened.\nVacancies in a Party Committee shall be filled by an alternate members according to the order of precedence, which is decided by the number of votes an alternate member got during his or hers election. A Party Committee must convene for at least two plenary meetings a year. During its tenure, a Party Committee shall \"carry out the directives of the next higher Party organizations and the resolutions of the Party congresses at the corresponding levels.\" The local Standing Committee (analogous to the Central Politburo) is elected at the first plenum of the corresponding Party Committee after the local party congress. A Standing Committee is responsible to the Party Committee at the corresponding level and the Party Committee at the next higher level. A Standing Committee exercises the duties and responsibilities of the corresponding Party Committee when it is not in session.\nFunding.\nThe funding of all CCP organizations mainly comes from state fiscal revenue. Data for the proportion of total CCP organizations\u2019 expenditures in total China fiscal revenue is unavailable. However, occasionally small local governments in China release such data. For example, on 10 October 2016, the local government of Mengmao Township, Ruili City, Yunnan Province released a concise fiscal revenue and expenditure report for the year 2014. According to this report, the fiscal Revenue amounted to RMB 29,498,933.58, and CCP organization' expenditures amounted to RMB 1,660,115.50, that is, 5.63% of fiscal revenue is used by the CCP for its own operation. This value is similar to the social security and employment expenditure of the whole town\u2014RMB 1,683,064.90.\nMembers.\nTo join the party, an applicant must be approved by the communist party. In 2014, only 2 million applications were accepted out of some 22\u00a0million applicants.\n Admitted members then spend a year as a probationary member.\nIn contrast to the past, when emphasis was placed on the applicants' ideological criteria, the current CCP stresses technical and educational qualifications. To become a probationary member, the applicant must take an admission oath before the party flag. The relevant CCP organization is responsible for observing and educating probationary members. Probationary members have duties similar to those of full members, with the exception that they may not vote in party elections nor stand for election. Many join the CCP through the Communist Youth League. Under Jiang Zemin, private entrepreneurs were allowed to become party members. According to the CCP constitution, a member, in short, must follow orders, be disciplined, uphold unity, serve the Party and the people, and promote the socialist way of life. Members enjoy the privilege of attending Party meetings, reading relevant Party documents, receiving Party education, participating in Party discussions through the Party's newspapers and journals, making suggestions and proposal, making \"well-grounded criticism of any Party organization or member at Party meetings\" (even of the central party leadership), voting and standing for election, and of opposing and criticizing Party resolutions (\"provided that they resolutely carry out the resolution or policy while it is in force\"); and they have the ability \"to put forward any request, appeal, or complaint to higher Party organizations, even up to the Central Committee, and ask the organizations concerned for a responsible reply.\" No party organization, including the CCP central leadership, can deprive a member of these rights.\nAs of 30 June 2016, individuals who identify as farmers, herdsmen and fishermen make up 26 million members; members identifying as workers totalled 7.2\u00a0million. Another group, the \"Managing, professional and technical staff in enterprises and public institutions\", made up 12.5\u00a0million, 9\u00a0million identified as working in administrative staff and 7.4\u00a0million described themselves as party cadres. 22.3\u00a0million women are CCP members. The CCP currently has 90.59\u00a0million members, making it the second largest political party in the world after India's Bharatiya Janata Party.\nWomen in China have low participation rates as political leaders. Women's disadvantage is most evident in their severe under representation in the more powerful political positions. At the top level of decision making, no woman has ever been among the nine members of the Standing Committee of the Communist Party's Politburo. Just 3 of 27 government ministers are women, and importantly, since 1997, China has fallen to 53rd place from 16th in the world in terms of female representation at its parliament, the National People's Congress, according to the Inter-Parliamentary Union. Party leaders such as Zhao Ziyang have vigorously opposed the participation of women in the political process. Within the party women face a glass ceiling.\nCommunist Youth League.\nThe Communist Youth League (CYL) is the CCP's youth wing, and the largest mass organization for youth in China. According to the CCP's constitution the CYL is a \"mass organization of advanced young people under the leadership of the Communist Party of China; it functions as a party school where a large number of young people learn about socialism with Chinese characteristics and about communism through practice; it is the Party's assistant and reserve force.\" To join, an applicant has to be between the ages of 14 and 28. It controls and supervises Young Pioneers, a youth organization for children below the age of 14. The organizational structure of CYL is an exact copy of the CCP's; the highest body is the National Congress, followed by the , Politburo and the Politburo Standing Committee. However, the Central Committee (and all central organs) of the CYL work under the guidance of the CCP central leadership. Therefore, in a peculiar situation, CYL bodies are both responsible to higher bodies within CYL and the CCP, a distinct organization. As of the 17th National Congress (held in 2013), CYL had 89 million members.\nSymbols.\nAccording to Article 53 of the CCP constitution, \"the Party emblem and flag are the symbol and sign of the Communist Party of China.\" At the beginning of its history, the CCP did not have a single official standard for the flag, but instead allowed individual party committees to copy the flag of the Communist Party of the Soviet Union. On 28 April 1942, the Central Politburo decreed the establishment of a sole official flag. \"The flag of the CPC has the length-to-width proportion of 3:2 with a hammer and sickle in the upper-left corner, and with no five-pointed star. The Political Bureau authorizes the General Office to custom-make a number of standard flags and distribute them to all major organs\". According to \"People's Daily\", \"The standard party flag is 120\u00a0centimeters (cm) in length and 80\u00a0cm in width. In the center of the upper-left corner (a quarter of the length and width to the border) is a yellow hammer-and-sickle 30\u00a0cm in diameter. The flag sleeve (pole hem) is in white and 6.5\u00a0cm in width. The dimension of the pole hem is not included in the measure of the flag. The red color symbolizes revolution; the hammer-and-sickle are tools of workers and peasants, meaning that the Communist Party of China represents the interests of the masses and the people; the yellow color signifies brightness.\" In total the flag has five dimensions, the sizes are \"no. 1: 388\u00a0cm in length and 192\u00a0cm in width; no. 2: 240\u00a0cm in length and 160\u00a0cm in width; no. 3: 192\u00a0cm in length and 128\u00a0cm in width; no. 4: 144\u00a0cm in length and 96\u00a0cm in width; no. 5: 96\u00a0cm in length and 64\u00a0cm in width.\" On 21 September 1966, the CCP General Office issued \"Regulations on the Production and Use of the CCP Flag and Emblem\", which stated that the emblem and flag were the official symbols and signs of the party.\nParty-to-party relations.\nCommunist parties.\nThe CCP continues to have relations with non-ruling communist and workers' parties and attends international communist conferences, most notably the International Meeting of Communist and Workers' Parties. Delegates of foreign communist parties still visit China; in 2013, for instance, the General Secretary of the Portuguese Communist Party (PCP), Jeronimo de Sousa, personally met with Liu Qibao, a member of the Central Politburo. In another instance, Pierre Laurent, the National Secretary of the French Communist Party (PCF), met with Liu Yunshan, a Politburo Standing Committee member. In 2014 Xi Jinping, the CCP general secretary, personally met with Gennady Zyuganov, the First Secretary of the Communist Party of the Russian Federation (CPRF), to discuss party-to-party relations. While the CCP retains contact with major parties such as the PCP, PCF, the CPRF, the Communist Party of Bohemia and Moravia, the Communist Party of Brazil, the Communist Party of Nepal and the Communist Party of Spain, the party retains relations with minor communist and workers' parties, such as the Communist Party of Australia, the Workers Party of Bangladesh, the Communist Party of Bangladesh (Marxist\u2013Leninist) (Barua), the Communist Party of Sri Lanka, the Workers' Party of Belgium, the Hungarian Workers' Party, the Dominican Workers' Party and the Party for the Transformation of Honduras, for instance. In recent years, noting the self-reform of the European social democratic movement in the 1980s and 1990s, the CCP \"has noted the increased marginalization of West European communist parties.\"\nRuling parties of socialist states.\nThe CCP has retained close relations with the remaining socialist states still espousing communism: Cuba, Laos, and Vietnam and their respective ruling parties as well as North Korea and its ruling party, which officially abandoned communism in 2009. It spends a fair amount of time analyzing the situation in the remaining socialist states, trying to reach conclusions as to why these states survived when so many did not, following the collapse of the Eastern European socialist states in 1989 and the dissolution of the Soviet Union in 1991. In general, the analyses of the remaining socialist states and their chances of survival have been positive, and the CCP believes that the socialist movement will be revitalized sometime in the future.\nThe ruling party which the CCP is most interested in is the Communist Party of Vietnam (CPV). In general the CPV is considered a model example of socialist development in the post-Soviet era. Chinese analysts on Vietnam believe that the introduction of the Doi Moi reform policy at the 6th CPV National Congress is the key reason for Vietnam's current success.\nWhile the CCP is probably the organization with most access to North Korea, writing about North Korea is tightly circumscribed. The few reports accessible to the general public are those about North Korean economic reforms. While Chinese analysts of North Korea tend to speak positively of North Korea in public, in official discussions circa 2008 they show much disdain for North Korea's economic system, the cult of personality which pervades society, the Kim family, the idea of hereditary succession in a socialist state, the security state, the use of scarce resources on the Korean People's Army and the general impoverishment of the North Korean people. Circa 2008 there are those analysts who compare the current situation of North Korea with that of China during the Cultural Revolution. Over the years, the CCP has tried to persuade the Workers' Party of Korea (or WPK, North Korea's ruling party) to introduce economic reforms by showing them key economic infrastructure in China. For instance, in 2006 the CCP invited the WPK general secretary Kim Jong-il to Guangdong province to showcase the success economic reforms have brought China. In general, the CCP considers the WPK and North Korea to be negative examples of a communist ruling party and socialist state.\nThere is a considerable degree of interest in Cuba within the CCP. Fidel Castro, the former First Secretary of the Communist Party of Cuba (PCC), is greatly admired, and books have been written focusing on the successes of the Cuban Revolution. Communication between the CCP and the PCC has increased considerably since the 1990s, hardly a month going by without a diplomatic exchange. At the 4th Plenary Session of the 16th Central Committee, which discussed the possibility of the CCP learning from other ruling parties, praise was heaped on the PCC. When Wu Guanzheng, a Central Politburo member, met with Fidel Castro in 2007, he gave him a personal letter written by Hu Jintao: \"Facts have shown that China and Cuba are trustworthy good friends, good comrades, and good brothers who treat each other with sincerity. The two countries' friendship has withstood the test of a changeable international situation, and the friendship has been further strengthened and consolidated.\"\nNon-communist parties.\nSince the decline and fall of communism in Eastern Europe, the CCP has begun establishing party-to-party relations with non-communist parties. These relations are sought so that the CCP can learn from them. For instance, the CCP has been eager to understand how the People's Action Party of Singapore (PAP) maintains its total domination over Singaporean politics through its \"low-key presence, but total control.\" According to the CCP's own analysis of Singapore, the PAP's dominance can be explained by its \"well-developed social network, which controls constituencies effectively by extending its tentacles deeply into society through branches of government and party-controlled groups.\" While the CCP accepts that Singapore is a liberal democracy, they view it as a guided democracy led by the PAP. Other differences are, according to the CCP, \"that it is not a political party based on the working class\u2014instead it is a political party of the elite. [...] It is also a political party of the parliamentary system, not a revolutionary party.\" Other parties which the CCP studies and maintains strong party-to-party relations with are the United Malays National Organisation, which has ruled Malaysia (1957\u20132018), and the Liberal Democratic Party in Japan, which dominated Japanese politics since 1955.\nSince Jiang Zemin's time, the CCP has made friendly overtures to its erstwhile foe, the Kuomintang. The CCP emphasizes strong party-to-party relations with the KMT so as to strengthen the probability of the reunification of Taiwan with mainland China. However, several studies have been written on the KMT's loss of power in 2000 after having ruled Taiwan since 1949 (the KMT officially ruled mainland China from 1928 to 1949). In general, one-party states or dominant-party states are of special interest to the party and party-to-party relations are formed so that the CCP can study them. The longevity of the Syrian Regional Branch of the Arab Socialist Ba'ath Party is attributed to the personalization of power in the al-Assad family, the strong presidential system, the inheritance of power, which passed from Hafez al-Assad to his son Bashar al-Assad, and the role given to the Syrian military in politics.\nCirca 2008, the CCP has been especially interested in Latin America, as shown by the increasing number of delegates sent to and received from these countries. Of special fascination for the CCP is the 71-year-long rule of the Institutional Revolutionary Party (PRI) in Mexico. While the CCP attributed the PRI's long reign in power to the strong presidential system, tapping into the machismo culture of the country, its nationalist posture, its close identification with the rural populace and the implementation of nationalization alongside the marketization of the economy, the CCP concluded that the PRI failed because of the lack of inner-party democracy, its pursuit of social democracy, its rigid party structures that could not be reformed, its political corruption, the pressure of globalization, and American interference in Mexican politics. While the CCP was slow to recognize the pink tide in Latin America, it has strengthened party-to-party relations with several socialist and anti-American political parties over the years. The CCP has occasionally expressed some irritation over Hugo Ch\u00e1vez's anti-capitalist and anti-American rhetoric. Despite this, the CCP reached an agreement in 2013 with the United Socialist Party of Venezuela (PSUV), which was founded by Ch\u00e1vez, for the CCP to educate PSUV cadres in political and social fields. By 2008, the CCP claimed to have established relations with 99 political parties in 29 Latin American countries.\nSocial democratic movements in Europe have been of great interest to the CCP since the early 1980s. With the exception of a short period in which the CCP forged party-to-party relations with far-right parties during the 1970s in an effort to halt \"Soviet expansionism\", the CCP's relations with European social democratic parties were its first serious efforts to establish cordial party-to-party relations with non-communist parties. The CCP credits the European social democrats with creating a \"capitalism with a human face\". Before the 1980s, the CCP had a highly negative and dismissive view of social democracy, a view dating back to the Second International and the Marxist\u2013Leninist view on the social democratic movement. By the 1980s, that view had changed and the CCP concluded that it could actually learn something from the social democratic movement. CCP delegates were sent all over Europe to observe. By the 1980s, most European social democratic parties were facing electoral decline and in a period of self-reform. The CCP followed this with great interest, laying most weight on reform efforts within the British Labour Party and the Social Democratic Party of Germany. The CCP concluded that both parties were re-elected because they modernized, replacing traditional state socialist tenets with new ones supporting privatization, shedding the belief in big government, conceiving a new view of the welfare state, changing their negative views of the market and moving from their traditional support base of trade unions to entrepreneurs, the young and students.\nControversy.\nIn June 2020, Cai Xia, a retired professor of CCP's Central Party School, voiced criticisms against Xi Jinping, the General Secretary of the CCP, in which she compared Xi to a \"mafia boss\" and the ruling Communist Party a \"political zombie\". In a 20-minute audio on social networking sites, she said that everyone is Xi's slave, and that there is no human rights or rule of law. She suggested that Xi should retire. On 17 August 2020, Cai Xia was expelled from the CCP's Central Party School and her retirement pensions were cancelled.\nOn 24 July 2020 the CCP expelled an outspoken and influential property tycoon, Ren Zhiqiang, who denounced CCP general secretary Xi. He went missing in March after criticizing Xi, and later his case was passed to the judiciary system for criminal investigation.\nOn 1 October 2020, U.S. Congressman Scott Perry introduced legislation to add the CCP to the Top International Criminal Organizations Target (TICOT) List and provide the United States law enforcement agencies a strategic directive to target the CCP's activity.\nOn 21 October 2020, the Subcommittee on International Human Rights (SDIR) of the Canadian House of Commons Standing Committee on Foreign Affairs and International Development condemned the persecution of Uyghurs and other Turkic Muslims in Xinjiang by the Government of China and concluded that the Chinese Communist Party's actions amount to genocide of the Uyghurs per the Genocide Convention."}
{"id": "7176", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=7176", "title": "Cryogenics", "text": "In physics, cryogenics is the production and behaviour of materials at very low temperatures.\nThe 13th IIR International Congress of Refrigeration (held in Washington DC in 1971) endorsed a universal definition of \u201ccryogenics\u201d and \u201ccryogenic\u201d by accepting a threshold of 120 K (or \u2013153 \u00b0C) to distinguish these terms from the conventional refrigeration. This is a logical dividing line, since the normal boiling points of the so-called permanent gases (such as helium, hydrogen, neon, nitrogen, oxygen, and normal air) lie below \u2212120\u00a0\u00b0C while the Freon refrigerants, hydrocarbons, and other common refrigerants have boiling points above \u2212120\u00a0\u00b0C. The U.S. National Institute of Standards and Technology considers the field of cryogenics as that involving temperatures below . \nDiscovery of superconducting materials with critical temperatures significantly above the boiling point of liquid nitrogen has provided new interest in reliable, low cost methods of producing high temperature cryogenic refrigeration. The term \"high temperature cryogenic\" describes temperatures ranging from above the boiling point of liquid nitrogen, , up to .\nCryogenicists use the Kelvin or Rankine temperature scale, both of which measure from absolute zero, rather than more usual scales such as Celsius which measures from the freezing point of water at sea level or Fahrenheit with its zero at an arbitrary temperature.\nEtymology.\nThe word \"cryogenics\" stems from Greek \"\u03ba\u03c1\u03cd\u03bf\u03c2 (cryos)\" \u2013 \"cold\" + \"\u03b3\u03b5\u03bd\u03ae\u03c2 (genis)\" \u2013 \"generating\".\nCryogenic fluids.\nCryogenic fluids with their boiling point in kelvins.\nIndustrial applications.\nLiquefied gases, such as liquid nitrogen and liquid helium, are used in many cryogenic applications. Liquid nitrogen is the most commonly used element in cryogenics and is legally purchasable around the world. Liquid helium is also commonly used and allows for the lowest attainable temperatures to be reached.\nThese liquids may be stored in Dewar flasks, which are double-walled containers with a high vacuum between the walls to reduce heat transfer into the liquid. Typical laboratory Dewar flasks are spherical, made of glass and protected in a metal outer container. Dewar flasks for extremely cold liquids such as liquid helium have another double-walled container filled with liquid nitrogen. Dewar flasks are named after their inventor, James Dewar, the man who first liquefied hydrogen. Thermos bottles are smaller vacuum flasks fitted in a protective casing.\nCryogenic barcode labels are used to mark Dewar flasks containing these liquids, and will not frost over down to \u2212195 degrees Celsius.\nCryogenic transfer pumps are the pumps used on LNG piers to transfer liquefied natural gas from LNG carriers to LNG storage tanks, as are cryogenic valves.\nCryogenic processing.\nThe field of cryogenics advanced during World War II when scientists found that metals frozen to low temperatures showed more resistance to wear. Based on this theory of cryogenic hardening, the commercial cryogenic processing industry was founded in 1966 by Ed Busch. With a background in the heat treating industry, Busch founded a company in Detroit called CryoTech in 1966 which merged with 300 Below in 1999 to become the world's largest and oldest commercial cryogenic processing company. Busch originally experimented with the possibility of increasing the life of metal tools to anywhere between 200% and 400% of the original life expectancy using cryogenic tempering instead of heat treating. This evolved in the late 1990s into the treatment of other parts.\nCryogens, such as liquid nitrogen, are further used for specialty chilling and freezing applications. Some chemical reactions, like those used to produce the active ingredients for the popular statin drugs, must occur at low temperatures of approximately . Special cryogenic chemical reactors are used to remove reaction heat and provide a low temperature environment. The freezing of foods and biotechnology products, like vaccines, requires nitrogen in blast freezing or immersion freezing systems. Certain soft or elastic materials become hard and brittle at very low temperatures, which makes cryogenic milling (cryomilling) an option for some materials that cannot easily be milled at higher temperatures.\nCryogenic processing is not a substitute for heat treatment, but rather an extension of the heating\u2013quenching\u2013tempering cycle. Normally, when an item is quenched, the final temperature is ambient. The only reason for this is that most heat treaters do not have cooling equipment. There is nothing metallurgically significant about ambient temperature. The cryogenic process continues this action from ambient temperature down to .\nIn most instances the cryogenic cycle is followed by a heat tempering procedure. As all alloys do not have the same chemical constituents, the tempering procedure varies according to the material's chemical composition, thermal history and/or a tool's particular service application.\nThe entire process takes 3\u20134 days.\nFuels.\nAnother use of cryogenics is cryogenic fuels for rockets with liquid hydrogen as the most widely used example. Liquid oxygen (LOX) is even more widely used but as an oxidizer, not a fuel. NASA's workhorse Space Shuttle used cryogenic hydrogen/oxygen propellant as its primary means of getting into orbit. LOX is also widely used with RP-1 kerosene, a non-cryogenic hydrocarbon, such as in the rockets built for the Soviet space program by Sergei Korolev.\nRussian aircraft manufacturer Tupolev developed a version of its popular design Tu-154 with a cryogenic fuel system, known as the Tu-155. The plane uses a fuel referred to as liquefied natural gas or LNG, and made its first flight in 1989.\nOther applications.\nSome applications of cryogenics:\nProduction.\nCryogenic cooling of devices and material is usually achieved via the use of liquid nitrogen, liquid helium, or a mechanical cryocooler (which uses high-pressure helium lines). Gifford-McMahon cryocoolers, pulse tube cryocoolers and Stirling cryocoolers are in wide use with selection based on required base temperature and cooling capacity. The most recent development in cryogenics is the use of magnets as regenerators as well as refrigerators. These devices work on the principle known as the magnetocaloric effect.\nDetectors.\nThere are various cryogenic detectors which are used to detect cryogenic particles.\nFor cryogenic temperature measurement down to 30K, Pt100 sensors, a resistance temperature detector (RTD), are used. For temperatures lower than 30K it is necessary to use a silicon diode for accuracy."}
{"id": "7179", "revid": "39247988", "url": "https://en.wikipedia.org/wiki?curid=7179", "title": "Cary Elwes", "text": "Cary Elwes (; born 26 October 1962) is an English actor, author and screenwriter. He is best known for his leading film roles as Westley in \"The Princess Bride\" (1987), Robin Hood in \"\" (1993), and Dr. Lawrence Gordon in the \"Saw\" film series. He is also known for his performances in films like \"Glory\" (1989), \"Hot Shots!\" (1991), \"The Jungle Book\" (1994), \"Days of Thunder\" (1990), \"Bram Stoker's Dracula\" (1992), \"Twister\" (1996), \"Kiss the Girls\" (1997), \"Liar Liar\" (1997), \"Cradle Will Rock\" (1999), \"Shadow of the Vampire\" (2000), \"The Cat's Meow\" (2001), \"Ella Enchanted\" (2004), \"A Christmas Carol\" (2009), and \"No Strings Attached\" (2011). He has appeared on television in a number of series including \"The X-Files\", \"Seinfeld\", \"From the Earth to the Moon\", \"Psych\", \"Life in Pieces\", \"Stranger Things\" and \"The Marvelous Mrs. Maisel\".\nEarly life.\nBorn Ivan Simon Cary Elwes on 26 October 1962 in Westminster, London, he is the youngest of three sons of portrait painter Dominic Elwes and interior designer and socialite Tessa Kennedy. He is the brother of artist Damian Elwes and film producers Cassian Elwes and Milica Kastner. His stepfather, Elliott Kastner, was an American film producer and the first American to set up independent film production in the United Kingdom. His paternal grandfather was the portrait painter Simon Elwes, whose own father was the diplomat and tenor Gervase Elwes (1866\u20131921). His other great-grandfathers include the diplomat Rennell Rodd, 1st Baron Rennell and industrialist Ivan Rikard Ivanovi\u0107. Elwes has English, Irish, Scottish, Croatian-Jewish, and Serbian ancestry, the latter two from his maternal grandmother, Da\u0161ka McLean, whose second husband, Billy McLean, was an operative for Special Operations Executive during World War II.\nOne of Elwes's relatives is the famed British miser John Elwes, who was the inspiration for Ebenezer Scrooge in \"A Christmas Carol\" (1843), having been referenced by Charles Dickens himself in chapter six of his last completed novel, \"Our Mutual Friend\". Elwes himself played five roles in the 2009 film adaptation of \"A Christmas Carol\". Through his maternal grandfather, Elwes is also related to Sir Alexander William \"Blackie\" Kennedy, one of the first photographers to document the archaeological site of Petra following the collapse of the Ottoman Empire.\nElwes was brought up as a Roman Catholic and was an altar boy at Westminster Cathedral, although he did not attend denominational schools as most of the men on his father's side of the family had, including his father. His paternal relatives include such clerics as Dudley Charles Cary-Elwes (1868\u20131932), the Roman Catholic Bishop of Northampton, Abbot Columba Cary-Elwes (Ampleforth Abbey, Saint Louis Abbey), and Father Luke Cary-Elwes (Fort Augustus Abbey). He discussed this in an interview while he was filming the 2005 CBS television film \"Pope John Paul II\", in which he played the young priest Karol Wojty\u0142a.\nElwes's parents divorced when he was four years old. In 1975, when Elwes was 13, his father died by suicide. He was educated at Harrow School, and the London Academy of Music and Dramatic Art. In 1981, he moved to the United States to study acting at Sarah Lawrence College in Bronxville, New York. While living there, Elwes studied acting at both the Actors Studio and the Lee Strasberg Theatre and Film Institute under the tutelage of Al Pacino's mentor, Charlie Laughton (not to be confused with English actor Charles Laughton). As a teenager, he also worked as a production assistant on the films \"Absolution\", \"Octopussy\", and \"Superman\", where he was assigned to Marlon Brando. When Elwes introduced himself to the famous actor, Brando insisted on calling him \"Rocky\" after Rocky Marciano.\nCareer.\nFilm.\nElwes made his acting debut in 1984 with Marek Kanievska's film \"Another Country\", which was loosely based on the English boarding school exploits of British spies, Burgess, Philby and MacLean. He played James Harcourt, a gay student. He went on to play Guilford Dudley in the British historical drama film \"Lady Jane\", opposite Helena Bonham Carter. He was then cast as a stable-boy-turned-swashbuckler Westley in Rob Reiner's fantasy-comedy \"The Princess Bride\", which was based on the novel of the same name by William Goldman. It was a modest box office success, but received critical acclaim, earning a score of 97% on the review aggregation website Rotten Tomatoes. Since being released on home video and television, the film has become a cult classic.\nElwes continued to work steadily, varying between dramatic roles, such as in the Oscar-winning \"Glory\" (1989), and comedic roles, as in \"Hot Shots!\" (1991). He played a rival driver to Tom Cruise in Days of Thunder (1990). In 1993, he starred as Robin Hood in Mel Brooks's comedy, \"\". Elwes then appeared in supporting roles in such films as Francis Ford Coppola's adaptation of \"Bram Stoker's Dracula\", \"The Crush\", \"The Jungle Book\" (1994), \"Twister\", \"Liar Liar\", and \"Kiss the Girls\". In 1999, he portrayed famed theatre and film producer John Houseman for Tim Robbins in his ensemble film based on Orson Welles's musical, \"Cradle Will Rock\". Following that, he travelled to Luxembourg to work with John Malkovich and Willem Dafoe in \"Shadow of the Vampire\". In 2001, he co-starred in Peter Bogdanovich's ensemble film \"The Cat's Meow\" portraying movie mogul Thomas Ince, who died mysteriously while vacationing with William Randolph Hearst on his yacht.\nIn 2004, Elwes starred in the horror\u2013thriller \"Saw\" which, at a budget of a little over $1\u00a0million, grossed over $100\u00a0million worldwide. The same year he appeared in \"Ella Enchanted\", this time as the villain, not the hero. He made an uncredited appearance as Sam Green, the man who introduced Andy Warhol to Edie Sedgwick, in the 2006 film \"Factory Girl\". In 2007, he appeared in Garry Marshall's \"Georgia Rule\" opposite Jane Fonda.\nIn 2010, he returned to the \"Saw\" franchise in \"Saw 3D\" (2010), the seventh film in the series, as Dr. Lawrence Gordon. In 2011, he was selected by Ivan Reitman to star alongside Natalie Portman in \"No Strings Attached\". That same year, Elwes and Garry Marshall teamed up again in the ensemble romantic comedy \"New Year's Eve\" opposite Robert de Niro and Halle Berry.\nIn 2012, Elwes starred in the independent drama \"The Citizen\". and the following year Elwes joined Selena Gomez for the comedy ensemble, \"Behaving Badly\" directed by Tim Garrick. In 2015, he completed \"Sugar Mountain\" directed by Richard Gray; the drama \"We Don't Belong Here\", opposite Anton Yelchin and Catherine Keener directed by Peer Pedersen, and \"Being Charlie\" which reunited Elwes with director Rob Reiner after 28 years and premiered at the Toronto International Film Festival. In 2016, Elwes starred opposite Penelope Cruz in Fernando Trueba's Spanish-language period pic \"The Queen of Spain\", a sequel to Trueba's 1998 drama \"The Girl of Your Dreams\". This also re-united Elwes with his Princess Bride co-star, Mandy Patinkin. \nTelevision.\nElwes made his first television appearance in 1996 as David Lookner on \"Seinfeld\". Two years later he played astronaut Michael Collins in the Golden Globe Award-winning HBO miniseries \"From the Earth To the Moon\". The following year Elwes was nominated for a Golden Satellite Award for Best Performance by an Actor in a Mini-Series or Motion Picture Made for Television for his portrayal of Colonel James Burton in \"The Pentagon Wars\" directed by Richard Benjamin. In 1999, he guest starred as Dr. John York in an episode of the television series \"The Outer Limits\". Shortly afterward he received another Golden Satellite Award nomination for his work on the ensemble NBC Television movie \"Uprising\" opposite Jon Voight directed by Jon Avnet. Elwes had a recurring role in the final season (from 2001 to 2002) of Chris Carter's hit series \"The X-Files\" as FBI Assistant Director Brad Follmer.\nIn 2004, he portrayed serial killer Ted Bundy in the A&amp;E Network film \"The Riverman\", which became one of the highest rated original movies in the network's history and garnered a prestigious BANFF Rockie Award nomination. The following year, Elwes played the young Karol Wojty\u0142a in the CBS television film \"Pope John Paul II\". The TV film was highly successful not only in North America but also in Europe, where it broke box office records in the late Pope's native Poland and became the first film ever to break $1\u00a0million (GBP 739,075.00 current) in three days.\nIn 2007, he made a guest appearance on the \" episode \" as a Mafia lawyer. In 2009, he played the role of Pierre Despereaux, an international art thief, in the fourth-season premiere of \"Psych\". In 2010, he returned to \"Psych\", reprising his role in the second half of the fifth season, again in the show's sixth season, and again in the show's eighth season premiere. In 2014, Elwes played Hugh Ashmeade, Director of the CIA, in the second season of the BYUtv series \"Granite Flats\".\nIn May 2015, Elwes was cast as Arthur Davenport, a shrewd and eccentric world-class collector of illegal art and antiquities in Crackle's first streaming network series drama, \"The Art of More\", which explored the cutthroat world of premium auction houses. The series debuted on 19 November and was picked up for a second season.\nIn April 2018 Elwes potrayed Larry Kline, Mayor of Hawkins, for the third season of the Netflix series \"Stranger Things\", which premiered in July 2019. In May 2019, it was announced that he would be joining the third season of the Amazon series \"The Marvelous Mrs. Maisel\" as Gavin Hawk.\nVoice-over work.\nElwes's voice-over work includes the narrator in James Patterson's audiobook \"The Jester\", as well as characters in film and television animations such as \"Quest for Camelot\", \"Pinky and The Brain\", \"Batman Beyond\", and the English versions of the Studio Ghibli films, \"Porco Rosso\", \"Whisper of the Heart\" and \"The Cat Returns\". For the 2004 video game \"The Bard's Tale\", he served as screenwriter, improviser, and voice actor of the main character The Bard. In 2009, Elwes reunited with Jason Alexander for the Indian film, \"Delhi Safari\". The following year Elwes portrayed the part of Gremlin Gus in Disney's video game, '. In 2014, he appeared in ' as the voice of scientists Edmond Halley and Robert Hooke.\nMotion capture work.\nIn 2009 Elwes joined the cast of Robert Zemeckis's motion capture adaptation of Charles Dickens' \"A Christmas Carol\" portraying five roles. That same year he was chosen by Steven Spielberg to appear in his motion capture adaptation of Belgian artist Herg\u00e9's popular comic strip \"\".\nTheatre.\nIn 2003 Elwes portrayed Kerry Max Cook in the off-Broadway play \"The Exonerated\" in New York, directed by Bob Balaban (18\u201323 March 2003).\nLiterature.\nIn October 2014 Touchstone (Simon &amp; Schuster) published Elwes's memoir of the making of \"The Princess Bride\", entitled \"As You Wish: Inconceivable Tales from the Making of The Princess Bride\", which Elwes co-wrote with Joe Layden. The book featured never-before-told stories, exclusive behind-the-scenes photographs, and interviews with co-stars Robin Wright, Wallace Shawn, Billy Crystal, Christopher Guest, Fred Savage and Mandy Patinkin, as well as author and screenwriter William Goldman, producer Norman Lear, and director Rob Reiner. The book debuted on \"The New York Times Best Seller list\".\nOther projects.\nIn 2014, Elwes co-wrote the screenplay for a film entitled \"Elvis &amp; Nixon\", about the pair's famous meeting at the White House in 1970. The film, which starred Michael Shannon and Kevin Spacey, was bought by Amazon as their first theatrical feature and was released on 22 April 2016.\nLawsuit.\nIn August 2005, Elwes filed a lawsuit against Evolution Entertainment, his management firm and producer of \"Saw\". Elwes said he was promised a minimum of one percent of the producers' net profits of the film and did not receive the full amount. The case was settled out of court. In 2010, he reprised his role in \"Saw 3D\".\nPersonal life.\nElwes met photographer Lisa Marie Kurbikoff in 1991 at a chili cook-off in Malibu, California, and they became engaged in 1997. They married in 2000 and have one daughter together.\nIn March 2021, Elwes posted on his social media accounts that his younger sister Milica had passed away after battling Stage 4 cancer for more than a year. "}
{"id": "7180", "revid": "32920364", "url": "https://en.wikipedia.org/wiki?curid=7180", "title": "Chris Sarandon", "text": "Chris Sarandon Jr. (; born July 24, 1942) is an American actor. He is best known for playing Jerry Dandrige in \"Fright Night\" (1985), Prince Humperdinck in \"The Princess Bride\" (1987), Detective Mike Norris in \"Child's Play\" (1988), and for providing the speaking voice of Jack Skellington in \"The Nightmare Before Christmas\" (1993). He was nominated for an Academy Award for Best Supporting Actor for his performance as Leon Shermer in \"Dog Day Afternoon\" (1975).\nEarly life.\nSarandon was born and raised in Beckley, West Virginia, the son of restaurateurs Chris Sarandon and Cliffie (n\u00e9e Cardullias). His father, whose surname was originally \"Sarondonethes\", was born in Istanbul, Turkey, of Greek ancestry; his mother is also of Greek descent.\nSarandon graduated from Woodrow Wilson High School in Beckley. He earned a degree in speech at West Virginia University. He earned his master's degree in theater from The Catholic University of America (CUA) in Washington, D.C..\nCareer.\nAfter graduation, he toured with numerous improvisational companies and became much involved with regional theatre, making his professional debut in the play \"The Rose Tattoo\" during 1965. In 1968, Sarandon moved to New York City, where he obtained his first television role as Dr. Tom Halverson for the series \"The Guiding Light\" (1973\u20131974). He appeared in the primetime television movies \"The Satan Murders\" (1974) and \"Thursday's Game\" before obtaining the role in \"Dog Day Afternoon\" (1975), a performance which earned him nominations for Best New Male Star of the Year at the Golden Globes and the Academy Award for Best Supporting Actor.\nSarandon appeared in the Broadway play \"The Rothschilds\" and \"The Two Gentlemen of Verona\", as well making regular appearances at numerous Shakespeare and George Bernard Shaw festivals in the United States and Canada. He also had a series of television roles, some of which (such as \"A Tale of Two Cities\" in 1980) corresponded to his affinity for the classics. He also had roles in the thriller movie \"Lipstick\" (1976) and as a demon in the movie \"The Sentinel\" (1977).\nTo avoid being typecast in villainous roles, Sarandon accepted various roles of other types during the years to come, portraying the title role of Christ in the made-for-television movie \"The Day Christ Died\" (1980). He received accolades for his portrayal of Sydney Carton in a TV-movie version of \"A Tale of Two Cities\" (1980), co-starred with Dennis Hopper in the 1983 movie \"The Osterman Weekend\", which was based on the Robert Ludlum novel of the same name, and co-starred with Goldie Hawn in the movie \"Protocol\" (1984). These were followed by another mainstream success as the vampire-next-door in the horror movie \"Fright Night\" (1985). He starred in the 1986 TV movie \"Liberty\", which addressed the making of New York City's Statue of Liberty.\nHe is best known in the film industry for his role as Prince Humperdinck in Rob Reiner's 1987 movie \"The Princess Bride\", though he also has had supporting parts in other successful movies such as the original \"Child's Play\" (1988). In 1992, he played Joseph Curwen/Charles Dexter Ward in \"The Resurrected\". He also provided the voice of Jack Skellington, the main character of Tim Burton's animated Disney movie \"The Nightmare Before Christmas\" (1993), and has since reprised the role in other productions, including the Disney/Square video games \"Kingdom Hearts\" and \"Kingdom Hearts II\" and the Capcom sequel to the original movie, \"\". Sarandon also reprised his role as Jack Skellington for several Disneyland Halloween events and attractions including; \"Halloween Screams\", the \"Frightfully Fun Parade,\" and the Haunted Mansion Holiday, a three-month overlay of the Haunted Mansion, where Jack and his friends take control of a mansion in an attempt to introduce Christmas, much as his character did in the movie.\nSarandon appeared in TV again with a recurring role as Dr. Burke on NBC's long-running medical drama \"ER\".\nIn 1991 he performed on Broadway in the short-lived musical \"Nick &amp; Nora\" (based on the movie \"The Thin Man\") with Joanna Gleason, the daughter of Monty Hall. Sarandon married Gleason in 1994. They have appeared together in a number of movies, including \"Edie &amp; Pen\" (1996), \"American Perfekt\" (1997), and \"Let the Devil Wear Black\" (1999). During the 2000s he made guest appearances in several TV series, notably as the Necromancer demon, Armand, in \"Charmed\", and as superior court judge Barry Krumble for six episodes of \"Judging Amy\".\nIn 2006 he played Signor Naccarelli in the six-time Tony award-winning Broadway musical play \"The Light in the Piazza\" at Lincoln Center. Most recently he appeared in \"Cyrano de Bergerac\" as Antoine de Guiche, with Kevin Kline, Jennifer Garner, and Daniel Sunjata. He is on the Advisory Board for the Greenbrier Valley Theatre in Lewisburg, West Virginia. In 2016 he performed in the Off-Broadway production of the Dave Malloy musical \"Preludes\" as Anton Chekhov, Tchaikovsky, Alexander Glazunov, Leo Tolstoy, Tsar Nicholas II, and The Master.\nPersonal life.\nSarandon has been married three times: he married actress Susan Sarandon in 1967, the couple first met while attending The Catholic University of America together in Washington, D.C.. The marriage lasted for twelve years; the pair divorced in 1979. After divorcing from Susan, he married his second wife, fashion model Lisa Ann Cooper, in 1980. The couple had two daughters and one son: Stephanie (born 1982), Alexis (born 1984) and Michael (born 1988). The marriage ended in divorce in 1989 after nine years of marriage. In 1994, he married his third wife, actress and singer Joanna Gleason. The couple met while performing in Broadway's short-lived 1991 musical \"Nick &amp; Nora\"; they returned to the stage together in 1998's \"Thorn and Bloom\". They also collaborated in several films together, such as \"Road Ends\", \"Edie &amp; Pen\", \"Let the Devil Wear Black\", and \"American Perfekt\"."}
{"id": "7182", "revid": "12406635", "url": "https://en.wikipedia.org/wiki?curid=7182", "title": "Christopher Guest", "text": "Christopher Haden-Guest, 5th Baron Haden-Guest (born February 5, 1948), is an American\u2013British screenwriter, composer, musician, director, actor, and comedian. Guest is most widely known in Hollywood for having written, directed, and starred in his series of comedy films shot in mock-documentary (mockumentary) style. Many scenes and character backgrounds in Guest's films are written and directed, although actors have no rehearsal time and the ensemble improvise scenes while filming them. The series of films began with \"This Is Spinal Tap\" (which he did not direct) and continued with \"Waiting for Guffman\", \"Best in Show\", \"A Mighty Wind\", \"For Your Consideration\", and \"Mascots\".\nGuest holds a hereditary British peerage as the 5th Baron Haden-Guest, and has publicly expressed a desire to see the House of Lords reformed as a democratically elected chamber. Though he was initially active in the Lords, his career there was cut short by the House of Lords Act 1999, which removed the right of most hereditary peers to a seat in the parliament. When using his title, he is normally styled as Lord Haden-Guest. Guest is married to actress and author Jamie Lee Curtis.\nEarly years.\nGuest was born in New York City, the son of Peter Haden-Guest, a British United Nations diplomat who later became the 4th Baron Haden-Guest, and his second wife, Jean Pauline Hindes, an American former vice president of casting at CBS. Guest's paternal grandfather, Leslie, Baron Haden-Guest, was a Labour Party politician, who was a convert to Judaism. Guest's paternal grandmother, a descendant of the Dutch Jewish Goldsmid family, was the daughter of Colonel Albert Goldsmid, a British officer who founded the Jewish Lads' and Girls' Brigade and the Maccabaeans. Guest's maternal grandparents were Jewish emigrants from Russia. Both of Guest's parents had become atheists, and Guest had no religious upbringing. Nearly a decade before he was born, his uncle, David Guest, a lecturer and Communist Party member, was killed in the Spanish Civil War, fighting in the International Brigades.\nGuest spent parts of his childhood in his father's native United Kingdom. He attended The High School of Music &amp; Art (New York City), studying classical music (clarinet) at the Stockbridge School in Interlaken, Massachusetts. He later took up the mandolin, became interested in country music, and played guitar with Arlo Guthrie, a fellow student at Stockbridge School. Guest later began performing with bluegrass bands until he took up rock and roll. Guest went to Bard College for a year and then studied acting at New York University's Graduate Acting Program at the Tisch School of the Arts, graduating in 1971.\nCareer.\n1970s.\nGuest began his career in theatre during the early 1970s with one of his earliest professional performances being the role of Norman in Michael Weller's \"Moonchildren\" for the play's American premiere at the Arena Stage in Washington, DC, in November 1971. Guest continued with the production when it moved to Broadway in 1972. The following year, he began making contributions to \"The National Lampoon Radio Hour\" for a variety of National Lampoon audio recordings. He both performed comic characters (Flash Bazbo\u2014Space Explorer, Mr. Rogers, music critic Roger de Swans, and sleazy record company rep Ron Fields) and wrote, arranged, and performed numerous musical parodies (of Bob Dylan, James Taylor, and others). He was featured alongside Chevy Chase and John Belushi in the off-Broadway revue \"National Lampoon's Lemmings\". Two of his earliest film roles were small parts as uniformed police officers in the 1972 film \"The Hot Rock\" and 1974's \"Death Wish\".\nGuest played a small role in the 1977 \"All in the Family\" episode \"\", where in a flashback sequence Mike and Gloria recall their first blind date, set up by Michael's college buddy Jim (Guest), who dated Gloria's girlfriend Debbie (Priscilla Lopez).\n1980s.\nGuest's biggest role of the first two decades of his career is likely that of Nigel Tufnel in the 1984 Rob Reiner film \"This Is Spinal Tap\". Guest made his first appearance as Tufnel on the 1978 sketch comedy program \"The TV Show\".\nAlong with Martin Short, Billy Crystal, and Harry Shearer, Guest was hired as a one-year-only cast member for the 1984\u201385 season on NBC's \"Saturday Night Live\". Recurring characters on SNL played by Guest include Frankie, of Willie and Frankie (coworkers who recount in detail physically painful situations in which they have found themselves, remarking laconically \"I hate when that happens\"); Herb Minkman, a shady novelty toymaker with a brother named Al (played by Crystal); Rajeev Vindaloo, an eccentric foreign man in the same vein as Andy Kaufman's Latka character from \"Taxi\"; and Se\u00f1or Cosa, a Spanish ventriloquist often seen on the recurring spoof of \"The Joe Franklin Show\". He also experimented behind the camera with prefilmed sketches, notably directing a documentary-style short starring Shearer and Short as synchronized swimmers. In another short film from SNL, Guest and Crystal appear as retired Negro league baseball players, \"The Rooster and the King\".\nHe appeared as Count Rugen (the \"six-fingered man\") in \"The Princess Bride\". He had a cameo role as the first customer, a pedestrian, in the 1986 musical remake of \"The Little Shop of Horrors\", that also featured Steve Martin. As a co-writer and director, Guest made the Hollywood satire \"The Big Picture\".\nUpon his father succeeding to the family peerage in 1987, he was known as 'the Hon. Christopher Haden-Guest. This was his official style and name until he inherited the barony in 1996.\n1990\u2013present.\nThe experience of making \"This is Spinal Tap\" directly informed the second phase of his career. Starting in 1996, Guest began writing, directing, and acting in his own series of substantially improvised films. Many of them came to be definitive examples of what came to be known as \"mockumentaries\"\u2014not a term Guest appreciates in describing his unusual approach to exploring the passions that make the characters in his films so interesting. He maintains that his intention is not to mock anyone, but to explore insular, perhaps obscure communities through his method of filmmaking.\nHis frequent writing partner is Eugene Levy. Together, Levy, Guest and a small band of other actors have formed a loose repertory group, which appear across several films. These include Catherine O'Hara, Michael McKean, Parker Posey, Bob Balaban, Jane Lynch, John Michael Higgins, Harry Shearer, Jennifer Coolidge, Ed Begley, Jr., and Fred Willard. Guest and Levy write backgrounds for each of the characters and notecards for each specific scene, outlining the plot, and then leave it up to the actors to improvise the dialogue, which is supposed to result in a much more natural conversation than scripted dialogue would. Typically, everyone who appears in these movies receives the same fee and the same portion of profits.\nGuest had a guest voice-over role in the animated comedy series \"SpongeBob SquarePants\" as SpongeBob's cousin, Stanley.\nGuest again collaborated with Reiner in \"A Few Good Men\" (1992), appearing as Dr. Stone. In the 2000s, Guest appeared in the 2005 biographical musical \"Mrs Henderson Presents\" and in the 2009 comedy \"The Invention of Lying\".\nHe is also currently a member of the musical group The Beyman Bros, which he formed with childhood friend David Nichtern and Spinal Tap's current keyboardist C. J. Vanston. Their debut album \"Memories of Summer as a Child\" was released on January 20, 2009.\nIn 2010, the United States Census Bureau paid $2.5\u00a0million to have a television commercial directed by Guest shown during television coverage of Super Bowl XLIV.\nGuest holds an honorary doctorate from and is a member of the board of trustees for Berklee College of Music in Boston.\nIn 2013, Guest was the writer and producer of the HBO series \"Family Tree,\" a lighthearted story in the style he made famous in \"This is Spinal Tap\", in which the main character, Tom Chadwick, inherits a box of curios from his great aunt, spurring interest in his ancestry.\nOn August 11, 2015, Netflix announced that \"Mascots\", a film directed by Guest about the competition for the World Mascot Association championships's Gold Fluffy Award, would debut in 2016.\nFamily.\nGuest became the 5th Baron Haden-Guest, of Great Saling, in the County of Essex, when his father died in 1996. He succeeded upon the ineligibility of his older half-brother, Anthony Haden-Guest, who was born prior to the marriage of his parents. According to an article in \"The Guardian\", Guest attended the House of Lords regularly until the House of Lords Act 1999 barred most hereditary peers from their seats. In the article Guest remarked:\nPersonal life.\nGuest married actress Jamie Lee Curtis in 1984 at the home of their mutual friend, Rob Reiner. They have two adopted children: Anne (born 1986) and Thomas (born 1996). Because Guest's children are adopted, they cannot inherit the family barony under the terms of the letters patent that created it, though a 2004 Royal Warrant addressing the style of a peer's adopted children states that they can use courtesy titles. The current heir presumptive to the barony is Guest's younger brother, actor Nicholas Guest.\nOff-stage demeanor.\nAs reported by Louis B. Hobson, \"On film, Guest is a hilariously droll comedian. In person he is serious and almost dour.\" He quotes Guest as saying, \"People want me to be funny all the time. They think I'm being funny no matter what I say or do and that's not the case. I rarely joke unless I'm in front of a camera. It's not what I am in real life. It's what I do for a living.\"\nGuest was played by Seth Green in the film \"A Futile and Stupid Gesture.\""}
{"id": "7183", "revid": "1009289090", "url": "https://en.wikipedia.org/wiki?curid=7183", "title": "Carol Kane", "text": "Carolyn Laurie Kane (born June 18, 1952) is an American actress and comedian. She became known in the 1970s and 1980s in films such as \"Hester Street\", for which she received an Academy Award nomination for Best Actress, \"Annie Hall\", and \"The Princess Bride\". She appeared on the television series \"Taxi\" in the early 1980s, as Simka Gravas, the wife of Latka, the character played by Andy Kaufman, winning two Emmy Awards for her work. She has played the character of Madame Morrible in the musical \"Wicked\", both in touring productions and on Broadway from 2005 to 2014. From 2015 to 2020, she was a main cast member on the Netflix series \"Unbreakable Kimmy Schmidt\", in which she played Lillian Kaushtupper.\nEarly life.\nKane was born in Cleveland, Ohio, the daughter of Joy, a jazz singer, teacher, dancer, and pianist, and Michael Kane. Her family is Jewish, and her grandparents emigrated from Russia, Austria, and Poland. Her parents divorced when she was 12 years old. She attended the Cherry Lawn School, a boarding school in Darien, Connecticut, until 1965. She studied theatre at HB Studio and also went to the Professional Children's School, in New York City, and made her professional theatre debut in a 1966 production of \"The Prime of Miss Jean Brodie\", starring Tammy Grimes.\nCareer.\nTelevision.\nKane portrayed Simka Dahblitz-Gravas, wife of Latka Gravas (Andy Kaufman), on the American television series \"Taxi\" from 1981 to 1983. She received two Emmy Awards for her work in the series.\nIn 1984, Kane appeared in episode 12, season 3 of \"Cheers\" as Amanda, an acquaintance of Diane Chambers from her time spent in a mental institution.\nKane was a regular on the 1986 series \"All Is Forgiven\", a regular on the 1990\u20131991 series \"American Dreamer\", guest-starred on a 1994 episode of \"Seinfeld\", a 1996 episode of \"Ellen\" and had a supporting role in the short-lived sitcom \"Pearl\".\nIn 1988, Kane appeared in the Cinemax Comedy Experiment \"Rap Master Ronnie: A Report Card\" alongside Jon Cryer and the Smothers Brothers.\nIn January 2009, she appeared in the television series \"Two and a Half Men\" as the mother of Alan Harper's receptionist. In March 2010, Kane appeared in the television series \"Ugly Betty\" as Justin Suarez's acting teacher. In 2014, she had a recurring role in the TV series \"Gotham\" as Gertrude Kapelput, Oswald Cobblepot's (Penguin's) mother.\nIn 2015, she was cast as Lillian Kaushtupper, the landlord to the title character of Netflix's series \"Unbreakable Kimmy Schmidt\". She reprised the role in the television movie \"\".\nIn 2020, Kane was part of the ensemble cast of the Amazon show \"Hunters\", which includes Al Pacino and Logan Lerman.\nFilms.\nKane appeared in \"The Last Detail\" (1973), \"Hester Street\" (1975), \"Dog Day Afternoon\" (1975), \"Annie Hall\" (1977), \"The World's Greatest Lover\" (1977), \"When a Stranger Calls\" (1979), \"Norman Loves Rose\" (1982), \"Transylvania 6-5000\" (1985), \"The Princess Bride\" (1987), \"Scrooged\" (1988), in which \"Variety\" called her \"unquestionably [the] pic's comic highlight,\" and \"Flashback\" (1989) with Dennis Hopper.\nIn 1998, she played Mother Duck on the cartoon movie \"The First Snow of Winter\". In 1999, she made a cameo in the movie \"Man On The Moon\" as her character she played on \"Taxi\".\nAt the 48th Academy Awards, Kane was nominated for an Academy Award for Best Actress for her role in the film \"Hester Street\".\nKane also starred in the Addams Family remakes as grandma.\nTheatre.\nShe starred in the off-Broadway play \"Love, Loss, and What I Wore\" in February 2010.\nKane made her West End debut in January 2011 in a major revival of Lillian Hellman's drama \"The Children's Hour\" at London's Comedy Theatre. She starred alongside Keira Knightley, Elisabeth Moss and Ellen Burstyn.\nIn May 2012, Kane appeared on Broadway as Betty Chumley in a revival of the play \"Harvey\".\n\"Wicked\".\nKane is also known for her portrayal of the evil headmistress Madame Morrible in the Broadway musical \"Wicked\", whom she played in various productions from 2005 to 2014.\nKane made her \"Wicked\" debut on the 1st National Tour, playing the role from March 9 through December 19, 2005. She then reprised the role in the Broadway production from January 10 through November 12, 2006. She again played the role for the Los Angeles production which began performances on February 7, 2007. She left the production on December 30, 2007, and later returned from August 26, 2008, until the production closed on January 11, 2009.\nShe then transferred with the L.A. company, to play the role once again, in the San Francisco production which began performances January 27, 2009. She ended her limited engagement on March 22, 2009. Kane returned to the Broadway company of \"Wicked\" from July 1, 2013, through February 22, 2014 (a period that included the show's 10th anniversary).\nAwards and nominations.\n!Year\n!Award\n!Category\n!Work\n!Result"}
{"id": "7184", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=7184", "title": "C*-algebra", "text": "In mathematics, specifically in functional analysis, a C\u2217-algebra (pronounced \"C-star\") is a Banach algebra together with an involution satisfying the properties of the adjoint. A particular case is that of a complex algebra \"A\" of continuous linear operators on a complex Hilbert space with two additional properties:\nAnother important class of non-Hilbert C*-algebra include the algebra of continuous functions formula_1.\nC*-algebras were first considered primarily for their use in quantum mechanics to model algebras of physical observables. This line of research began with Werner Heisenberg's matrix mechanics and in a more mathematically developed form with Pascual Jordan around 1933. Subsequently, John von Neumann attempted to establish a general framework for these algebras which culminated in a series of papers on rings of operators. These papers considered a special class of C*-algebras which are now known as von Neumann algebras.\nAround 1943, the work of Israel Gelfand and Mark Naimark yielded an abstract characterisation of C*-algebras making no reference to operators on a Hilbert space.\nC*-algebras are now an important tool in the theory of unitary representations of locally compact groups, and are also used in algebraic formulations of quantum mechanics. Another active area of research is the program to obtain classification, or to determine the extent of which classification is possible, for separable simple nuclear C*-algebras.\nAbstract characterization.\nWe begin with the abstract characterization of C*-algebras given in the 1943 paper by Gelfand and Naimark.\nA C*-algebra, \"A\", is a Banach algebra over the field of complex numbers, together with a map formula_2 for formula_3 with the following properties:\nRemark. The first three identities say that \"A\" is a *-algebra. The last identity is called the C* identity and is equivalent to:\nformula_9\nwhich is sometimes called the B*-identity. For history behind the names C*- and B*-algebras, see the section below.\nThe C*-identity is a very strong requirement. For instance, together with the spectral radius formula, it implies that the C*-norm is uniquely determined by the algebraic structure:\nA bounded linear map, \"\u03c0\" : \"A\" \u2192 \"B\", between C*-algebras \"A\" and \"B\" is called a *-homomorphism if\nIn the case of C*-algebras, any *-homomorphism \"\u03c0\" between C*-algebras is contractive, i.e. bounded with norm \u2264 1. Furthermore, an injective *-homomorphism between C*-algebras is isometric. These are consequences of the C*-identity.\nA bijective *-homomorphism \"\u03c0\" is called a C*-isomorphism, in which case \"A\" and \"B\" are said to be isomorphic.\nSome history: B*-algebras and C*-algebras.\nThe term B*-algebra was introduced by C. E. Rickart in 1946 to describe Banach *-algebras that satisfy the condition:\nThis condition automatically implies that the *-involution is isometric, that is, formula_14. Hence, formula_15, and therefore, a B*-algebra is also a C*-algebra. Conversely, the C*-condition implies the B*-condition. This is nontrivial, and can be proved without using the condition formula_16. For these reasons, the term B*-algebra is rarely used in current terminology, and has been replaced by the term 'C*-algebra'.\nThe term C*-algebra was introduced by I. E. Segal in 1947 to describe norm-closed subalgebras of \"B\"(\"H\"), namely, the space of bounded operators on some Hilbert space \"H\". 'C' stood for 'closed'. In his paper Segal defines a C*-algebra as a \"uniformly closed, self-adjoint algebra of bounded operators on a Hilbert space\".\nStructure of C*-algebras.\nC*-algebras have a large number of properties that are technically convenient. Some of these properties can be established by using the continuous functional calculus or by reduction to commutative C*-algebras. In the latter case, we can use the fact that the structure of these is completely determined by the Gelfand isomorphism.\nSelf-adjoint elements.\nSelf-adjoint elements are those of the form \"x\"=\"x\"*. The set of elements of a C*-algebra \"A\" of the form \"x*x\" forms a closed convex cone. This cone is identical to the elements of the form \"xx*\". Elements of this cone are called \"non-negative\" (or sometimes \"positive\", even though this terminology conflicts with its use for elements of R.)\nThe set of self-adjoint elements of a C*-algebra \"A\" naturally has the structure of a partially ordered vector space; the ordering is usually denoted \u2265. In this ordering, a self-adjoint element \"x\" of \"A\" satisfies \"x\" \u2265 0 if and only if the spectrum of \"x\" is non-negative, if and only if \"x\" = \"s*s\" for some \"s\". Two self-adjoint elements \"x\" and \"y\" of \"A\" satisfy \"x\" \u2265 \"y\" if \"x\"\u2212\"y\" \u2265 0.\nThis partially ordered subspace allows the definition of a positive linear functional on a C*-algebra, which in turn is used to define the states of a C*-algebra, which in turn can be used to construct the spectrum of a C*-algebra using the GNS construction.\nQuotients and approximate identities.\nAny C*-algebra \"A\" has an approximate identity. In fact, there is a directed family {\"e\"\u03bb}\u03bb\u2208I of self-adjoint elements of \"A\" such that\nUsing approximate identities, one can show that the algebraic quotient of a C*-algebra by a closed proper two-sided ideal, with the natural norm, is a C*-algebra.\nSimilarly, a closed two-sided ideal of a C*-algebra is itself a C*-algebra.\nExamples.\nFinite-dimensional C*-algebras.\nThe algebra M(\"n\", C) of \"n\" \u00d7 \"n\" matrices over C becomes a C*-algebra if we consider matrices as operators on the Euclidean space, C\"n\", and use the operator norm ||\u00b7|| on matrices. The involution is given by the conjugate transpose. More generally, one can consider finite direct sums of matrix algebras. In fact, all C*-algebras that are finite dimensional as vector spaces are of this form, up to isomorphism. The self-adjoint requirement means finite-dimensional C*-algebras are semisimple, from which fact one can deduce the following theorem of Artin\u2013Wedderburn type:\nTheorem. A finite-dimensional C*-algebra, \"A\", is canonically isomorphic to a finite direct sum\nwhere min \"A\" is the set of minimal nonzero self-adjoint central projections of \"A\".\nEach C*-algebra, \"Ae\", is isomorphic (in a noncanonical way) to the full matrix algebra M(dim(\"e\"), C). The finite family indexed on min \"A\" given by {dim(\"e\")}\"e\" is called the \"dimension vector\" of \"A\". This vector uniquely determines the isomorphism class of a finite-dimensional C*-algebra. In the language of K-theory, this vector is the positive cone of the \"K\"0 group of \"A\".\nA \u2020-algebra (or, more explicitly, a \"\u2020-closed algebra\") is the name occasionally used in physics for a finite-dimensional C*-algebra. The dagger, \u2020, is used in the name because physicists typically use the symbol to denote a Hermitian adjoint, and are often not worried about the subtleties associated with an infinite number of dimensions. (Mathematicians usually use the asterisk, *, to denote the Hermitian adjoint.) \u2020-algebras feature prominently in quantum mechanics, and especially quantum information science.\nAn immediate generalization of finite dimensional C*-algebras are the approximately finite dimensional C*-algebras.\nC*-algebras of operators.\nThe prototypical example of a C*-algebra is the algebra \"B(H)\" of bounded (equivalently continuous) linear operators defined on a complex Hilbert space \"H\"; here \"x*\" denotes the adjoint operator of the operator \"x\" : \"H\" \u2192 \"H\". In fact, every C*-algebra, \"A\", is *-isomorphic to a norm-closed adjoint closed subalgebra of \"B\"(\"H\") for a suitable Hilbert space, \"H\"; this is the content of the Gelfand\u2013Naimark theorem.\nC*-algebras of compact operators.\nLet \"H\" be a separable infinite-dimensional Hilbert space. The algebra \"K\"(\"H\") of compact operators on \"H\" is a norm closed subalgebra of \"B\"(\"H\"). It is also closed under involution; hence it is a C*-algebra.\nConcrete C*-algebras of compact operators admit a characterization similar to Wedderburn's theorem for finite dimensional C*-algebras:\nTheorem. If \"A\" is a C*-subalgebra of \"K\"(\"H\"), then there exists Hilbert spaces {\"Hi\"}\"i\"\u2208\"I\" such that\nwhere the (C*-)direct sum consists of elements (\"Ti\") of the Cartesian product \u03a0 \"K\"(\"Hi\") with ||\"Ti\"|| \u2192 0.\nThough \"K\"(\"H\") does not have an identity element, a sequential approximate identity for \"K\"(\"H\") can be developed. To be specific, \"H\" is isomorphic to the space of square summable sequences \"l\"2; we may assume that \"H\" = \"l\"2. For each natural number \"n\" let \"Hn\" be the subspace of sequences of \"l\"2 which vanish for indices \"k\" \u2264 \"n\" and let \"en\" be the orthogonal projection onto \"Hn\". The sequence {\"en\"}\"n\" is an approximate identity for \"K\"(\"H\").\n\"K\"(\"H\") is a two-sided closed ideal of \"B\"(\"H\"). For separable Hilbert spaces, it is the unique ideal. The quotient of \"B\"(\"H\") by \"K\"(\"H\") is the Calkin algebra.\nCommutative C*-algebras.\nLet \"X\" be a locally compact Hausdorff space. The space formula_1 of complex-valued continuous functions on \"X\" that \"vanish at infinity\" (defined in the article on local compactness) form a commutative C*-algebra formula_1 under pointwise multiplication and addition. The involution is pointwise conjugation. formula_1 has a multiplicative unit element if and only if formula_24 is compact. As does any C*-algebra, formula_1 has an approximate identity. In the case of formula_1 this is immediate: consider the directed set of compact subsets of formula_24, and for each compact formula_28 let formula_29 be a function of compact support which is identically 1 on formula_28. Such functions exist by the Tietze extension theorem which applies to locally compact Hausdorff spaces. Any such sequence of functions formula_31 is an approximate identity.\nThe Gelfand representation states that every commutative C*-algebra is *-isomorphic to the algebra formula_1, where formula_24 is the space of characters equipped with the weak* topology. Furthermore, if formula_1 is isomorphic to formula_35 as C*-algebras, it follows that formula_24 and formula_37 are homeomorphic. This characterization is one of the motivations for the noncommutative topology and noncommutative geometry programs.\nC*-enveloping algebra.\nGiven a Banach *-algebra \"A\" with an approximate identity, there is a unique (up to C*-isomorphism) C*-algebra E(\"A\") and *-morphism \u03c0 from \"A\" into E(\"A\") which is universal, that is, every other continuous *-morphism factors uniquely through \u03c0. The algebra E(\"A\") is called the C*-enveloping algebra of the Banach *-algebra \"A\".\nOf particular importance is the C*-algebra of a locally compact group \"G\". This is defined as the enveloping C*-algebra of the group algebra of \"G\". The C*-algebra of \"G\" provides context for general harmonic analysis of \"G\" in the case \"G\" is non-abelian. In particular, the dual of a locally compact group is defined to be the primitive ideal space of the group C*-algebra. See spectrum of a C*-algebra.\nVon Neumann algebras.\nVon Neumann algebras, known as W* algebras before the 1960s, are a special kind of C*-algebra. They are required to be closed in the weak operator topology, which is weaker than the norm topology.\nThe Sherman\u2013Takeda theorem implies that any C*-algebra has a universal enveloping W*-algebra, such that any homomorphism to a W*-algebra factors through it.\nType for C*-algebras.\nA C*-algebra \"A\" is of type I if and only if for all non-degenerate representations \u03c0 of \"A\" the von Neumann algebra \u03c0(\"A\")\u2032\u2032 (that is, the bicommutant of \u03c0(\"A\")) is a type I von Neumann algebra. In fact it is sufficient to consider only factor representations, i.e. representations \u03c0 for which \u03c0(\"A\")\u2032\u2032 is a factor.\nA locally compact group is said to be of type I if and only if its group C*-algebra is type I.\nHowever, if a C*-algebra has non-type I representations, then by results of James Glimm it also has representations of type II and type III. Thus for C*-algebras and locally compact groups, it is only meaningful to speak of type I and non type I properties.\nC*-algebras and quantum field theory.\nIn quantum mechanics, one typically describes a physical system with a C*-algebra \"A\" with unit element; the self-adjoint elements of \"A\" (elements \"x\" with \"x*\" = \"x\") are thought of as the \"observables\", the measurable quantities, of the system. A \"state\" of the system is defined as a positive functional on \"A\" (a C-linear map \u03c6 : \"A\" \u2192 C with \u03c6(\"u*u\") \u2265 0 for all \"u\" \u2208 \"A\") such that \u03c6(1) = 1. The expected value of the observable \"x\", if the system is in state \u03c6, is then \u03c6(\"x\").\nThis C*-algebra approach is used in the Haag-Kastler axiomatization of local quantum field theory, where every open set of Minkowski spacetime is associated with a C*-algebra."}
{"id": "7185", "revid": "82835", "url": "https://en.wikipedia.org/wiki?curid=7185", "title": "London Borough of Croydon", "text": "The London Borough of Croydon () is a London borough in south London, part of Outer London. It covers an area of . It is the southernmost borough of London. At its centre is the historic town of Croydon from which the borough takes its name; while other urban centres include Coulsdon, Purley, South Norwood, Norbury, New Addington and Thornton Heath. Croydon is mentioned in Domesday Book, and from a small market town has expanded into one of the most populous areas on the fringe of London. The borough is now one of London's leading business, financial and cultural centres, and its influence in entertainment and the arts contribute to its status as a major metropolitan centre. Its population is 386,710, making it the second largest London borough and fifteenth largest English district.\nThe borough was formed in 1965 from the merger of the County Borough of Croydon with Coulsdon and Purley Urban District, both of which had been within Surrey. The local authority, Croydon London Borough Council, is now part of London Councils, the local government association for Greater London. The economic strength of Croydon dates back mainly to Croydon Airport which was a major factor in the development of Croydon as a business centre. Once London's main airport for all international flights to and from the capital, it was closed on 30 September 1959 due to the lack of expansion space needed for an airport to serve the growing city. It is now a Grade II listed building and tourist attraction. Croydon Council and its predecessor Croydon Corporation unsuccessfully applied for city status in 1954, 2000, 2002 and 2012. The area is currently going through a large regeneration project called Croydon Vision 2020 which is predicted to attract more businesses and tourists to the area as well as backing Croydon's bid to become \"London's Third City\" (after the City of London and Westminster). Croydon is mostly urban, though there are large suburban and rural uplands towards the south of the borough. Since 2003, Croydon has been certified as a Fairtrade borough by the Fairtrade Foundation. It was the first London borough to have Fairtrade status which is awarded on certain criteria.\nThe area is one of the hearts of culture in London and the South East of England. Institutions such as the major arts and entertainment centre Fairfield Halls add to the vibrancy of the borough. However, its famous fringe theatre, the Warehouse Theatre, went into administration in 2012 when the council withdrew funding, and the building itself was demolished in 2013. The Croydon Clocktower was opened by Queen Elizabeth II in 1994 as an arts venue featuring a library, the independent David Lean Cinema (closed by the council in 2011 after sixteen years of operating, but now partially reopened on a part-time and volunteer basis) and museum. From 2000 to 2010, Croydon staged an annual summer festival celebrating the area's black and Indian cultural diversity, with audiences reaching over 50,000 people. An internet radio station, Croydon Radio, is run by local people for the area. The borough is also home to its own local TV station, \"Croydon TV\". Premier League football club Crystal Palace F.C. play at Selhurst Park in Selhurst, a stadium they have been based in since 1924. Other landmarks in the borough include Addington Palace, an eighteenth-century mansion which became the official second residence of six Archbishops of Canterbury, Shirley Windmill, one of the few surviving large windmills in Greater London built in the 1850s, and the BRIT School, a creative arts institute run by the BRIT Trust which has produced artists such as Adele, Amy Winehouse and Leona Lewis.\nHistory.\nThe London Borough of Croydon was formed in 1965 from the Coulsdon and Purley Urban District and the County Borough of Croydon. The name Croydon comes from Crogdene or Croindone, named by the Saxons in the 8th century when they settled here, although the area had been inhabited since prehistoric times. It is thought to derive from the Anglo-Saxon \"croeas deanas\", meaning \"the valley of the crocuses\", indicating that, like Saffron Walden in Essex, it was a centre for the collection of saffron.\nBy the time of the Norman invasion Croydon had a church, a mill and around 365 inhabitants as recorded in the Domesday Book. The Archbishop of Canterbury, Archbishop Lanfranc lived at Croydon Palace which still stands. Visitors included Thomas Becket (another Archbishop), and royal figures such as Henry VIII of England and Elizabeth I.\nCroydon carried on through the ages as a prosperous market town, they produced charcoal, tanned leather, and ventured into brewing. Croydon was served by the Surrey Iron Railway, the first public railway (horse drawn) in the world, in 1803, and by the London to Brighton rail link in the mid-19th century, helping it to become the largest town in what was then Surrey.\nIn the 20th century Croydon became known for industries such as metal working, car manufacture and its aerodrome, Croydon Airport. Starting out during World War I as an airfield for protection against Zeppelins, an adjacent airfield was combined, and the new aerodrome opened on 29 March 1920. It became the largest in London, and was the main terminal for international air freight into the capital. It developed into one of the great airports of the world during the 1920s and 1930s, and welcomed the world's pioneer aviators in its heyday. British Airways Ltd used the airport for a short period after redirecting from Northolt Aerodrome, and Croydon was the operating base for Imperial Airways. It was partly due to the airport that Croydon suffered heavy bomb damage during World War II. As aviation technology progressed, however, and aircraft became larger and more numerous, it was recognised in 1952 that the airport would be too small to cope with the ever-increasing volume of air traffic. The last scheduled flight departed on 30 September 1959. It was superseded as the main airport by both London Heathrow and London Gatwick Airport (see below). The air terminal, now known as Airport House, has been restored, and has a hotel and museum in it.\nIn the late 1950s and through the 1960s the council commercialised the centre of Croydon with massive development of office blocks and the Whitgift Centre which was formerly the biggest in-town shopping centre in Europe. The centre was officially opened in October 1970 by the Duchess of Kent. The original Whitgift School there had moved to Haling Park, South Croydon in the 1930s; the replacement school on the site, Whitgift Middle School, now the Trinity School of John Whitgift, moved to Shirley Park in the 1960s, when the buildings were demolished.\nThe borough council unsuccessfully applied for city status in 1965, 2000 and again in 2002. If it had been successful, it would have been the third local authority in Greater London to hold that status, along with the City of London and the City of Westminster. At present the London Borough of Croydon is the second most populous local government district of England without city status, Kirklees being the first. Croydon's applications were refused as it was felt not to have an identity separate from the rest of Greater London. In 1965 it was described as \"...now just part of the London conurbation and almost indistinguishable from many of the other Greater London boroughs\" and in 2000 as having \"no particular identity of its own\".\nCroydon, in common with many other areas, was hit by extensive rioting in August 2011. Reeves, an historic furniture store established in 1867, that gave its name to a junction and tram stop in the town centre, was destroyed by arson.\nCroydon is currently going through a vigorous regeneration plan, called Croydon Vision 2020. This will change the urban planning of central Croydon completely. Its main aims are to make Croydon \"London's Third City\" and the hub of retail, business, culture and living in south London and South East England. The plan was showcased in a series of events called Croydon Expo. It was aimed at business and residents in the London Borough of Croydon, to demonstrate the \u00a33.5bn development projects the Council wishes to see in Croydon in the next ten years.\nThere have also been exhibitions for regional districts of Croydon, including Waddon, South Norwood and Woodside, Purley, New Addington and Coulsdon. Examples of upcoming architecture featured in the expo can easily be found to the centre of the borough, in the form of the Croydon Gateway site and the Cherry Orchard Road Towers.\nGovernance.\nPolitics of Croydon Council.\nCroydon London Borough Council has seventy councillors elected in 24 wards.\nCroydon is a cabinet-style council, and the Leader heads a ten-person cabinet, its members responsible for areas such as education or planning. There is a Shadow Cabinet drawn from the sole opposition party. A backbench cross-party scrutiny and overview committee is in place to hold the executive cabinet to account.\nFrom the borough's creation in 1965 until 1994 the council saw continuous control under first Conservatives and Residents' Ratepayers councillors up to 1986 and then Conservatives. From 1994 to 2006 Labour Party councillors controlled the council. After a further eight-year period of Conservative control the Labour group secured a ten-seat majority in the local council elections on 22 May 2014. Councillor Tony Newman returned to lead the council for Labour.\nIn the 2014 local elections the Labour party gained all the seats in the Ashburton and Waddon wards and gained the one seat held by the Conservatives in the New Addington ward. The election marked the first time that Ashburton ward had been represented by Labour. Elected as a Labour councillor in Waddon was Croydon Central's previous Conservative then Independent MP and leader of the Conservatives on Croydon council up to 2005, Andrew Pelling.\nAt the 2010 Croydon local elections seats lost previously in Addiscombe, South Norwood and Upper Norwood were retaken by Labour Party councillors; in New Addington the Conservative party gained a councillor, the first time that the Conservatives had taken a seat there since 1968. The composition of the council after the 2010 elections was Conservatives 37, Labour 33.\nMike Fisher, Conservative group leader since May 2005, was named as Council Leader following the Conservative victory in 2006.\nAt the 2006 local elections Conservative councillors regained control in gaining 12\u00a0councillors, taking ten seats from Labour in Addiscombe, Waddon, South Norwood and Upper Norwood and ousting the single Liberal Democrat councillor in Coulsdon. Between the 2006 and 2010 elections, a by-election in February 2007 saw a large swing to Labour from the Conservatives. Whereas 6% Conservative to Labour swings were produced in the two previous by-elections to 2006, won by a councillor from the incumbent party (in both cases the party of a councillor who had died).\nCrossover has occurred in political affiliation, during 2002\u201306 one Conservative councillor defected to Labour, went back to the Conservatives and spent some time as an independent. In March 2008, the Labour councillor Mike Mogul joined the Conservatives while a Conservative councillor became an independent.\nCouncillor Jonathan Driver, who became Mayor in 2008, died unexpectedly at the close of the year, causing a by-election in highly marginal Waddon which was successfully held by the Conservatives.\nFrom February 2005 until May 2006 the Leader of Croydon Council was Labour Co-operative Councillor Tony Newman, succeeding Hugh Malyan.\nWestminster representation.\nThe borough is covered by three parliamentary constituencies: these are Croydon North, Croydon Central and Croydon South.\nCivic history.\nFor much of its history, Croydon Council was controlled by the Conservative Party or Conservative-leaning independents. Former Croydon councillors include former MPs Andrew Pelling, Vivian Bendall, David Congdon, Geraint Davies and Reg Prentice, London Assembly member Valerie Shawcross, Lord Bowness, John Donaldson, Baron Donaldson of Lymington (Master of the Rolls) and H.T. Muggeridge, MP and father of Malcolm Muggeridge. The first Mayor of the newly created county borough was Jabez Balfour, later a disgraced Member of Parliament. Former Conservative Director of Campaigning, Gavin Barwell, was a Croydon councillor between 1998 and 2010 and was the MP for Croydon Central from 2010 until 2017. Sarah Jones (politician) won the Croydon Central seat for Labour in 2017. Croydon North has a Labour MP, Steve Reed (politician), and Croydon South has a Conservative MP, Chris Philp.\nSome 10,000 people work directly or indirectly for the council, at its main offices at Bernard Weatherill House or in its schools, care homes, housing offices or work depots. The council is generally well regarded, having made important improvements in education and social services. However, there have been concerns over benefits, leisure services and waste collection. Although the council has one of London's lower rates of council tax, there are claims that it is too high and that resources are wasted.\nThe Mayor of Croydon for 2020-21 is Councillor Maddie Henson. The Leader is Cllr Hamida Ali and the Deputy Leader is Cllr Stuart King. The Chief Executive since 14 September 2020 has been Katherine Kerswell.\nGovernment buildings.\nCroydon Town Hall on Katharine Street in Central Croydon houses the committee rooms, the mayor's and other councillors' offices, electoral services and the arts and heritage services.\nThe present Town Hall is Croydon's third. The first town hall is thought to have been built in either 1566 or 1609. The second was built in 1808 to serve the growing town but was demolished after the present town hall was erected in 1895.\nThe 1808 building cost \u00a38,000, which was regarded as an enormous sum for those days and was perhaps as controversial as the administrative building Bernard Weatherill House opened for occupation in 2013 and reputed to have cost \u00a3220,000,000. The early 19th century building was known initially as \"Courthouse\" as, like its predecessor and successor, the local court met there. The building stood on the western side of the High Street near to the junction with Surrey Street, the location of the town's market. The building became inadequate for the growing local administrative responsibilities and stood at a narrow point of a High Street in need of widening.\nThe present town hall was designed by local architect Charles Henman and was officially opened by the Prince and Princess of Wales on 19 May 1896. It was constructed in red brick, sourced from Wrotham in Kent, with Portland stone dressings and green Westmoreland slates for the roof. It also housed the court and most central council employees.\nThe Borough's incorporation in 1883 and a desire to improve central Croydon with improvements to traffic flows and the removal of social deprivation in Middle Row prompted the move to a new configuration of town hall provision. The second closure of the Central Railway Station provided the Corporation with the opportunity to buy the station land from the London, Brighton and South Coast Railway Company for \u00a311,500 to provide the site for the new town hall. Indeed, the council hoped to be able to sell on some of the land purchased with enough for municipal needs and still \"leave a considerable margin of land which might be disposed of\". The purchase of the failed railway station came despite local leaders having successfully urged the re-opening of the poorly patronised railway station. The railway station re-opening had failed to be a success so freeing up the land for alternative use.\nParts, including the former court rooms, have been converted into the Museum of Croydon and exhibition galleries. The original public library was converted into the David Lean Cinema, part of the Croydon Clocktower. The Braithwaite Hall is used for events and performances. The town hall was renovated in the mid-1990s and the imposing central staircase, long closed to the public and kept for councillors only, was re-opened in 1994. The civic complex, meanwhile, was substantially added to, with buildings across Mint Walk and the 19-floor Taberner House to house the rapidly expanding corporation's employees.\nRuskin House is the headquarters of Croydon's Labour, Trade Union and Co-operative movements and is itself a co-operative with shareholders from organisations across the three movements. In the 19th century, Croydon was a bustling commercial centre of London. It was said that, at the turn of the 20th century, approximately \u00a310,000 was spent in Croydon's taverns and inns every week. For the early labour movement, then, it was natural to meet in the town's public houses, in this environment. However, the temperance movement was equally strong, and Georgina King Lewis, a keen member of the Croydon United Temperance Council, took it upon herself to establish a dry centre for the labour movement. The first Ruskin House was highly successful, and there has been two more since. The current house was officially opened in 1967 by the then Labour Prime Minister, Harold Wilson. Today, Ruskin House continues to serve as the headquarters of the Trade Union, Labour and Co-operative movements in Croydon, hosting a range of meetings and being the base for several labour movement groups. Office tenants include the headquarters of the Communist Party of Britain and Croydon Labour Party. Geraint Davies, the MP for Croydon Central, had offices in the building, until he was defeated by Andrew Pelling and is now the Labour representative standing for Swansea West in Wales.\nTaberner House was built between 1964 and 1967, designed by architect H. Thornley, with Allan Holt and Hugh Lea as borough engineers. Although the council had needed extra space since the 1920s, it was only with the imminent creation of the London Borough of Croydon that action was taken. The building, being demolished in 2014, was in classic 1960s style, praised at the time but subsequently much derided. It has its elegant upper slab block narrowing towards both ends, a formal device which has been compared to the famous Pirelli Tower in Milan. It was named after Ernest Taberner OBE, Town Clerk from 1937 to 1963. Until September 2013, Taberner House housed most of the council's central employees and was the main location for the public to access information and services, particularly with respect to housing.\nIn September 2013, Council staff moved into Bernard Weatherill House in Fell Road, (named after the former Speaker of the House and Member of Parliament for Croydon North-East). Staff from the Met Police, NHS, Jobcentre Plus, Croydon Credit Union, Citizens Advice Bureau as well as 75 services from the council all moved to the new building.\nGeography and climate.\nThe borough is in the far south of London, with the M25 orbital motorway stretching to the south of it, between Croydon and Tandridge. To the north and east, the borough mainly borders the London Borough of Bromley, and in the north west the boroughs of Lambeth and Southwark. The boroughs of Sutton and Merton are located directly to the west. It is at the head of the River Wandle, just to the north of a significant gap in the North Downs. It lies south of Central London, and the earliest settlement may have been a Roman staging post on the London-Portslade road, although conclusive evidence has not yet been found. The main town centre houses a great variety of well-known stores on North End and two shopping centres. It was pedestrianised in 1989 to attract people back to the town centre. Another shopping centre called Park Place was due to open in 2012 but has since been scrapped.\nTownscape description.\nThe CR postcode area covers most of the south and centre of the borough while the SE and SW postcodes cover the northern parts, including Crystal Palace, Upper Norwood, South Norwood, Selhurst (part), Thornton Heath (part), Norbury and Pollards Hill (part).\nDistricts in the London Borough of Croydon include Addington, a village to the east of Croydon which until 2000 was poorly linked to the rest of the borough as it was without any railway or light rail stations, with only a few patchy bus services. Addiscombe is a district just northeast of the centre of Croydon, and is popular with commuters to central London as it is close to the busy East Croydon station. Ashburton, to the northeast of Croydon, is mostly home to residential houses and flats, being named after Ashburton House, one of the three big houses in the Addiscombe area. Broad Green is a small district, centred on a large green with many homes and local shops in West Croydon. Coombe is an area, just east of Croydon, which has barely been urbanised and has retained its collection of large houses fairly intact. Coulsdon, south west of Central Croydon, which has retained a good mix of traditional high street shops as well as a large number of restaurants for its size. Croydon is the principal area of the borough, Crystal Palace is an area north of Croydon, which is shared with the London Boroughs of Lambeth, Southwark, Lewisham and Bromley. Fairfield, just northeast of Croydon, holds the Fairfield Halls and the village of Forestdale, to the east of Croydon's main area, commenced work in the late 1960s and completed in the mid-70s to create a larger town on what was previously open ground. Hamsey Green is a place on the plateau of the North Downs, south of Croydon. Kenley, again south of the centre, lie within the London Green Belt and features a landscape dominated by green space. New Addington, to the east, is a large local council estate surrounded by open countryside and golf courses. Norbury, to the northwest, is a suburb with a large ethnic population. Norwood New Town is a part of the Norwood triangle, to the north of Croydon. Monks Orchard is a small district made up of large houses and open space in the northeast of the borough. Pollards Hill is a residential district with houses on roads, which are lined with pollarded lime trees, stretching to Norbury. Purley, to the south, is a main town whose name derives from \"pirlea\", which means 'Peartree lea'. Sanderstead, to the south, is a village mainly on high ground at the edge of suburban development in Greater London. Selhurst is a town, to the north of Croydon, which holds the nationally known school, The BRIT School. Selsdon is a suburb which was developed during the inter-war period in the 1920s and 1930s, and is remarkable for its many Art Deco houses, to the southeast of Croydon Centre. Shirley, is to the east of Croydon, and holds Shirley Windmill. South Croydon, to the south of Croydon, is a locality which holds local landmarks such as The Swan and Sugarloaf public house and independent Whitgift School part of the Whitgift Foundation. South Norwood, to the north, is in common with West Norwood and Upper Norwood, named after a contraction of Great North Wood and has a population of around 14,590. Thornton Heath is a town, to the northwest of Croydon, which holds Croydon's principal hospital Mayday. Upper Norwood is north of Croydon, on a mainly elevated area of the borough. Waddon is a residential area, mainly based on the Purley Way retail area, to the west of the borough. Woodside is located to the northeast of the borough, with streets based on Woodside Green, a small sized area of green land. And finally Whyteleafe is a town, right to the edge of Croydon with some areas in the Surrey district of Tandridge.\nCroydon is a gateway to the south from central London, with some major roads running through it. Purley Way, part of the A23, was built to by-pass Croydon town centre. It is one of the busiest roads in the borough, and is the site of several major retail developments including one of only 18 IKEA stores in the country, built on the site of the former power station. The A23 continues southward as Brighton Road, which is the main route running towards the south from Croydon to Purley. The centre of Croydon is very congested, and the urban planning has since become out of date and quite inadequate, due to the expansion of Croydon's main shopping area and office blocks. Wellesley Road is a north-south dual carriageway that cuts through the centre of the town, and makes it hard to walk between the town centre's two railway stations. Croydon Vision 2020 includes a plan for a more pedestrian-friendly replacement. It has also been named as one of the worst roads for cyclists in the area. Construction of the Croydon Underpass beneath the junction of George Street and Wellesley Road/Park Lane started in the early 1960s, mainly to alleviate traffic congestion on Park Lane, above the underpass. The Croydon Flyover is also near the underpass, and next to Taberner House. It mainly leads traffic on to Duppas Hill, towards Purley Way with links to Sutton and Kingston upon Thames. The major junction on the flyover is for Old Town, which is also a large three-lane road.\nTopography and climate.\nCroydon covers an area of 86.52\u00a0km2, the 256th largest district in England. Croydon's physical features consist of many hills and rivers that are spread out across the borough and into the North Downs, Surrey and the rest of south London. Addington Hills is a major hilly area to the south of London and is recognised as a significant obstacle to the growth of London from its origins as a port on the north side of the river, to a large circular city. The Great North Wood is a former natural oak forest that covered the Sydenham Ridge and the southern reaches of the River Effra and its tributaries. The most notable tree, called Vicar's Oak, marked the boundary of four ancient parishes; Lambeth, Camberwell, Croydon and Bromley. John Aubrey referred to this \"ancient remarkable tree\" in the past tense as early as 1718, but according to JB Wilson, the Vicar's Oak survived until 1825. The River Wandle is also a major tributary of the River Thames, where it stretches to Wandsworth and Putney for from its main source in Waddon.\nCroydon has a temperate climate in common with most areas of Great Britain, it is similar to that of Greenwich in Inner London: its K\u00f6ppen climate classification is \"Cfb\". Its mean annual temperature of 9.6\u00a0\u00b0C is similar to that experienced throughout the Weald, and slightly cooler than nearby areas such as the Sussex coast and central London. Rainfall is considerably below England's average (1971\u20132000) level of 838\u00a0mm, and every month is drier overall than the England average.\nThe nearest weather station is at Gatwick Airport.\nArchitecture.\nThe skyline of Croydon has significantly changed over the past 50 years. High rise buildings, mainly office blocks, now dominate the skyline. The most notable of these buildings include Croydon Council's headquarters Taberner House, which has been compared to the famous Pirelli Tower of Milan, and the Nestl\u00e9 Tower, the former UK headquarters of Nestl\u00e9.\nIn recent years, the development of tall buildings, such as the approved Croydon Vocational Tower and Wellesley Square, has been encouraged in the London Plan, which will lead to the erection of new skyscrapers over the next few years as London goes through a high-rise boom.\nNo. 1 Croydon, formerly the NLA Tower, Britain's 88th tallest tower, close to East Croydon station, is an example of 1970s architecture. The tower was originally nicknamed the \"Threepenny bit building\", as it resembles a stack of pre-decimalisation Threepence coins, which were 12-sided. It is now most commonly called The Octagon, being 8-sided.\nLunar House is another high-rise building. Like other government office buildings on Wellesley Road, such as Apollo House, the name of the building was inspired by the US moon landings (In the Croydon suburb of New Addington there is a public house, built during the same period, called \"The Man on the Moon\"). Lunar House houses the Home Office building for Visas and Immigration. Apollo House houses The Border Patrol Agency.\nA new generation of buildings are being considered by the council as part of Croydon Vision 2020, so that the borough doesn't lose its title of having the \"largest office space in the south east\", excluding central London. Projects such as Wellesley Square, which will be a mix of residential and retail with an eye-catching colour design and 100 George Street a proposed modern office block are incorporated in this vision.\nNotable events that have happened to Croydon's skyline include the Millennium project to create the largest single urban lighting project ever. It was created for the buildings of Croydon to illuminate them for the third millennium. Not only did this project give new lighting to the buildings, but it provided an opportunity to project onto them images and words, mixing art and poetry with coloured light, and also displaying public information after dark. Apart from increasing night time activity in Croydon and thereby reducing the fear of crime, it helped to promote the sustainable use of older buildings by displaying them in a more positive way.\nDemography.\nEthnicity.\nAccording to the 2011 census, Croydon had a population of 363,378, making Croydon the most populated borough in Greater London. The estimated population in 2017 was around 384,800. 186,900 were males, with 197,900 females. The density was 4,448 inhabitants per km\u00b2. 248,200 residents of Croydon were between the age of 16 and 64.\nIn 2011, white was the majority ethnicity with 55.1%. Black was the second-largest ethnicity with 20.2%; 16.4% were Asian and 8.3% stated to be something other.\nThe most common householder type were owner occupied with only a small percentage rented. Many new housing schemes and developments are currently taking place in Croydon, such as The Exchange and Bridge House, IYLO, Wellesley Square (now known as Saffron Square) and Altitude 25. In 2006, The Metropolitan Police recorded a 10% fall in the number of crimes committed in Croydon, better than the rate which crime in London as a whole is falling. Croydon has had the highest fall in the number of cases of violence against the person in south London, and is one of the top 10 safest local authorities in London. According to \"Your Croydon\" (a local community magazine) this is due to a stronger partnership struck between Croydon Council and the police. In 2007, overall crime figures across the borough saw decrease of 5%, with the number of incidents decreasing from 32,506 in 2006 to 30,862 in 2007. However, in the year ending April 2012, The Metropolitan Police recorded the highest rates for murder and rape throughout London in Croydon, accounting for almost 10% of all murders, and 7% of all rapes. Croydon has five police stations. Croydon police station is on Park Lane in the centre of the town near the Fairfield Halls; South Norwood police station is a newly refurbished building just off the High Street; Norbury police station is on London Road; Kenley station is on Godstone Road; and New Addington police station is on Addington Village road.\nPopulation change.\nThe table shows details on the population change since 1801, including the percentage change since the last available census data. Although the London Borough of Croydon has existed only since 1965, earlier figures have been generated by combining data from the towns, villages, and civil parishes that would later be absorbed into the authority.\nEconomy.\nThe main employment sectors of the Borough is retail and enterprise which is mainly based in Central Croydon. Major employers are well-known companies, who hold stores or offices in the town. Purley Way is a major employer of people, looking for jobs as sales assistants, sales consultants and store managerial jobs. IKEA Croydon, when it was built in 1992, brought many non-skilled jobs to Croydon. The store, which is a total size of 23,000 m2, took over the former site of Croydon Power station, which had led to the unemployment of many skilled workers. In May 2006, the extension of the IKEA made it the fifth biggest employer in Croydon, and includes the extension of the showroom, market hall and self-serve areas.\nOther big employers around Purley include the large Tesco Extra store in the town centre, along with other stores in Purley Way including Sainsbury's, B&amp;Q and Vue. Croydon town centre is also a major retail centre, and home to many high street and department stores as well as designer boutiques. The main town centre shopping areas are on the North End precinct, in the Whitgift Centre, Centrale and St George's Walk. Department stores in Croydon town centre include House of Fraser, Marks and Spencer, Allders, Debenhams and T.K. Maxx. Croydon's main market is Surrey Street Market, which has a royal charter dating back to 1276. Shopping areas outside the town centre include the Valley Park retail complex, Croydon Colonnades, Croydon Fiveways, and the Waddon Goods Park.\nIn research from 2010 on retail footprint, Croydon came out as 29th in terms of retail expenditure at \u00a3770 million. This puts it 6th in the Greater London area, falling behind Kingston upon Thames and Westfield London. In 2005, Croydon came 21st, second in London behind the West End, with \u00a3909 million, whilst Kingston was 24th with \u00a3864 million. In a 2004 survey on the top retail destinations, Croydon was 27th.\nIn 2007, Croydon leapt up the annual business growth league table, with a 14% rise in new firms trading in the borough after 125 new companies started up, increasing the number from 900 to 1,025, enabling the town, which has also won the Enterprising Britain Award and \"the most enterprising borough in London\" award, to jump from 31 to 14 in the table.\nTramlink created many jobs when it opened in 2000, not only drivers but engineers as well. Many of the people involved came from Croydon, which was the original hub of the system. Retail stores inside both Centrale and the Whitgift Centre as well as on North End employee people regularly and create many jobs, especially at Christmas. As well as the new building of Park Place, which will create yet more jobs, so will the regeneration of Croydon, called Croydon Vision 2020, highlighted in the Croydon Expo which includes the Croydon Gateway, Wellesley Square, Central One plus much more.\nCroydon is a major office area in the south east of England, being the largest outside of central London. Many powerful companies based in Europe and worldwide have European or British headquarters in the town. American International Group (AIG) have offices in No. 1 Croydon, formerly the NLA Tower, shared with Liberata, Pegasus and the Institute of Public Finance. AIG is the sixth-largest company in the world according to the 2007 Forbes Global 2000 list. The Swiss company Nestl\u00e9 has its UK headquarters in the Nestl\u00e9 Tower, on the site of the formerly proposed Park Place shopping centre. Real Digital International has developed a purpose built factory on Purley Way equipped with \"the most sophisticated production equipment and technical solutions\". ntl:Telewest, now Virgin Media, have offices at Communications House, from the Telewest side when it was known as Croydon Cable.\nThe Home Office UK Visas and Immigration department has its headquarters in Lunar House in Central Croydon. In 1981, Superdrug opened a distribution centre and office complex at Beddington Lane. The head office of international engineering and management consultant Mott MacDonald is located in Mott MacDonald House on Sydenham Road, one of four offices they occupy in the town centre. BT has large offices in Prospect East in Central Croydon. The Royal Bank of Scotland also has large offices in Purley, south of Croydon. Direct Line also has an office opposite Taberner House. Other companies with offices in Croydon include Lloyds TSB, Merrill Lynch and Balfour Beatty. Ann Summers used to have its headquarters in the borough but has moved to the Wapses Lodge Roundabout in Tandridge.\nLandmarks.\nThere are a large number of attractions and places of interest all across the borough of Croydon, ranging from historic sites in the north and south to modern towers in the centre.\nCroydon Airport was once London's main airport, but closed on 30 September 1959 due to the expansion of London and the need of more room at the airport which was impossible to provide, so Heathrow International Airport took over as London's main airport. It Has now been mostly converted to offices, although some important elements of the airport remain. It is a tourist attraction.\nThe Croydon Clocktower arts venue was opened by Elizabeth II in 1994. It includes the Braithwaite Hall (the former reference library - named after the Rev. Braithwaite who donated it to the town) for live events, David Lean Cinema (built in memory of David Lean), the Museum of Croydon and Croydon Central Library. The Museum of Croydon (formerly known as Croydon Lifetimes Museum) highlights Croydon in the past and the present and currently features high-profile exhibitions including the Riesco Collection, The Art of Dr Seuss and the Whatever the Weather gallery. Shirley Windmill is a working windmill and one of the few surviving large windmills in Surrey, built in 1854. It is Grade II listed and received a \u00a3218,100 grant from the Heritage Lottery Fund. Addington Palace is an 18th-century mansion in Addington which was originally built as Addington Place in the 16th century. The palace became the official second residence of six archbishops, five of whom are buried in St Mary's Church and churchyard nearby.\nNorth End is the main pedestrianised shopping road in Croydon, having Centrale to one side and the Whitgift Centre to the other. The Warehouse Theatre is a popular theatre for mostly young performers and is due to get a face-lift on the Croydon Gateway site.\nThe Nestl\u00e9 Tower was the UK headquarters of Nestl\u00e9 and is one of the tallest towers in England, which is due to be re-fitted during the Park Place development. The Fairfield Halls is a well known concert hall and exhibition centre, opened in 1962. It is frequently used for BBC recordings and was formerly the home of ITV's World of Sport. It includes the Ashcroft Theatre and the Arnhem Gallery.\nCroydon Palace was the summer residence of the Archbishop of Canterbury for over 500 years and included regular visitors such as Henry III and Queen Elizabeth I. It is thought to have been built around 960. Croydon Cemetery is a large cemetery and crematorium west of Croydon and is most famous for the gravestone of Derek Bentley, who was wrongly hanged in 1953. Mitcham Common is an area of common land partly shared with the boroughs of Sutton and Merton. Almost 500,000 years ago, Mitcham Common formed part of the river bed of the River Thames.\nThe BRIT School is a performing Arts &amp; Technology school, owned by the BRIT Trust (known for the BRIT Awards Music Ceremony). Famous former students include Kellie Shirley, Amy Winehouse, Leona Lewis, Adele, Kate Nash, Dane Bowers, Katie Melua and Lyndon David-Hall. Grants is an entertainment venue in the centre of Croydon which includes a Vue cinema.\nSurrey Street Market has roots in the 13th century, if not earlier, and was chartered by the Archbishop of Canterbury in 1276. The market is regularly used as a location for TV, film and advertising. Croydon Minster, formerly the parish church, was established in the Anglo-Saxon period, and parts of the surviving building (notably the tower) date from the 14th and 15th centuries. However, the church was largely destroyed by fire in 1867, so the present structure is a rebuild of 1867\u201369 to the designs of George Gilbert Scott. It is the burial place of six archbishops, and contains monuments to Archbishops Sheldon and Whitgift.\nTransport.\nRail.\nEast Croydon and West Croydon are the main stations in the borough.\nEast Croydon is served by Govia Thameslink Railway, operating under the Southern and Thameslink brands. Services travel via the Brighton Main Line north to London Victoria, London Bridge, London St Pancras, Luton Airport, Bedford, Cambridge, Peterborough and Milton Keynes Central, and south to Gatwick Airport, Ore, Brighton, Littlehampton, Bognor Regis, Southampton and Portsmouth. East Croydon is the largest and busiest station in Croydon and the third busiest in London, excluding Travelcard Zone 1.\nEast Croydon was served by long distance Arriva CrossCountry services to Birmingham and the North of England until they were withdrawn in December 2008.\nWest Croydon is served by London Overground and Southern services north to Highbury &amp; Islington, London Bridge and London Victoria, and south to Sutton and Epsom Downs.\nCroydon is one of only five London Boroughs not to have at least one London Underground station within its boundaries, with the closest tube station being Morden.\nBus.\nA sizeable bus infrastructure which is part of the London Buses network operates from a hub at West Croydon bus station. The original bus station opened in May 1985, closing in October 2014. A new bus station opened in October 2016.\nAddington Village Interchange is a regional bus terminal in Addington Village which has an interchange between Tramlink and bus services in the remote area. Services are operated under contract by Abellio London, Arriva London, London Central, Metrobus, Quality Line and Selkent.\nTram.\nThe Tramlink light rail system opened in 2000, serving the borough and surrounding areas. Its network consists of three lines, from Elmers End to West Croydon, from Beckenham to West Croydon, and from New Addington to Wimbledon, with all three lines running via the Croydon loop on which it is centred. It is also the only tram system in London but there is another light rail system, the Docklands Light Railway. It serves Mitcham, Woodside, Addiscombe and the Purley Way retail and industrial area amongst others.\nRoad.\nCroydon is linked into the national motorway network via the M23 and M25 orbital motorway. The M25 skirts the south of the borough, linking Croydon with other parts of London and the surrounding counties; the M23 branches from the M25 close to Coulsdon, linking the town with the south coast, Crawley, Reigate, and Gatwick Airport. The A23 connects the borough with the motorways. The A23 is the major trunk road through Croydon, linking it with central London, East Sussex, Horsham, and Littlehaven. The old London to Brighton road, passes through the west of the borough on Purley Way, bypassing the commercial centre of Croydon which it once did.\nThe A22 and A23 are the major trunk roads through Croydon. These both run north-south, connecting to each other in Purley. The A22 connects Croydon, its starting point, to East Grinstead, Tunbridge Wells, Uckfield, and Eastbourne. Other major roads generally radiate spoke-like from the town centre. Wellesley Road is an urban dual carriageway which cuts through the middle of the central business district. It was constructed in the 1960s as part of a planned ring road for Croydon and includes an underpass, which allows traffic to avoid going into the town centre.\nAir.\nThe closest international airport to Croydon is Gatwick Airport, which is located from the town centre. Gatwick Airport opened in August 1930 as an aerodrome and is a major international operational base for British Airways, EasyJet and Virgin Atlantic. It currently handles around 35 million passengers a year, making it London's second largest airport, and the second busiest airport in the United Kingdom after Heathrow. Heathrow, London City and Luton airports all lie within a two hours' drive of Croydon. Gatwick and Luton Airports are connected to Croydon by frequent direct trains, while Heathrow is accessible by the route X26 bus.\nCycling.\nAlthough hilly, Croydon is compact and has few major trunk roads running through it. It is on one of the Connect2 schemes which are part of the National Cycle Network route running around Croydon. The North Downs, an area of outstanding natural beauty popular with both on- and off-road cyclists, is so close to Croydon that part of the park lies within the borough boundary, and there are routes into the park almost from the civic centre.\nTravel to work.\nIn March 2011, the main forms of transport that residents used to travel to work were: driving a car or van, 20.2% of all residents aged 16\u201374; train, 59.5%; bus, minibus or coach, 7.5%; on foot, 5.1%; underground, metro, light rail, tram, 4.3%; work mainly at or from home, 2.9%; passenger in a car or van, 1.5%.\nPublic services.\nHome Office policing in Croydon is provided by the Metropolitan Police. The force's Croydon arm have their head offices for policing on Park Lane next to the Fairfield Halls and Croydon College in central Croydon. Public transport is co-ordinated by Transport for London. Statutory emergency fire and rescue service is provided by the London Fire Brigade, which has five stations in Croydon.\nHealth services.\nNHS South West London Clinical Commissioning Group (A merger of the previous NHS Croydon CCG and others in South West London) is the body responsible for public health and for planning and funding health services in the borough. Croydon has 227 GPs in 64 practices, 156 dentists in 51 practices, 166 pharmacists and 70 optometrists in 28 practices.\nCroydon University Hospital, formerly known as Mayday Hospital, built on a site in Thornton Heath at the west of Croydon's boundaries with Merton, is a large NHS hospital administrated by Croydon Health Services NHS Trust. Former names of the hospital include the Croydon Union Infirmary from 1885 to 1923 and the Mayday Road Hospital from 1923 to around 1930. It is a District General Hospital with a 24-hour accident and emergency department. NHS Direct has a regional centre based at the hospital. The NHS Trust also provides services at Purley War Memorial Hospital, in Purley. Croydon General Hospital was on London Road but services transferred to Mayday, as the size of this hospital was insufficient to cope with the growing population of the borough. Sickle Cell and Thalassaemia Centre and the Emergency Minor Treatment Centre are other smaller hospitals operated by the Mayday in the borough. Cane Hill was a psychiatric hospital in Coulsdon.\nWaste management.\nWaste management is co-ordinated by the local authority. Unlike other waste disposal authorities in Greater London, Croydon's rubbish is collected independently and isn't part of a waste authority unit. Locally produced inert waste for disposal is sent to landfill in the south of Croydon. There have recently been calls by the ODPM to bring waste management powers to the Greater London Authority, giving it a waste function. The Mayor of London has made repeated attempts to bring the different waste authorities together, to form a single waste authority in London. This has faced significant opposition from existing authorities. However, it has had significant support from all other sectors and the surrounding regions managing most of London's waste. Croydon has the joint best recycling rate in London, at 36%, but the refuse collectors have been criticised for their rushed performance lacking quality. Croydon's Distribution Network Operator for electricity is EDF Energy Networks; there are no power stations in the borough. Thames Water manages Croydon's drinking and waste water; water supplies being sourced from several local reservoirs, including Beckton and King George VI. Before 1971, Croydon Corporation was responsible for water treatment in the borough.\nLondon Fire Brigade.\nThe borough of Croydon is 86.52\u00a0kmsq, populating approximately 340,000 people. There are five fire stations within the borough; Addington (two pumping appliances), Croydon (two pumping appliances, incident response unit, fire rescue unit and a USAR appliance), Norbury (two pumping appliances), Purley (one pumping appliance) and Woodside (one pumping appliance). Purley has the largest station ground, but dealt with the fewest incidents during 2006/07.\nThe fire stations, as part of the Community Fire Safety scheme, visited 49 schools in 2006/2007.\nEducation.\nThe borough compared with the other London boroughs has the highest number of schools in it, with 26% of its population under 20 years old. They include primary schools (95), secondary schools (21) and four further education establishments. Croydon College has its main building in Central Croydon, it is a high rise building. John Ruskin College is one of the other colleges in the borough, located in Addington and Coulsdon College in Coulsdon. South Norwood has been the home of Spurgeon's College, a world-famous Baptist theological college, since 1923; Spurgeon's is located on South Norwood Hill and currently has some 1000 students. The London Borough of Croydon is the local education authority for the borough.\nOverall, Croydon was ranked 77th out of the all the local education authorities in the UK, up from 92nd in 2007. In 2007, the Croydon LEA was ranked 81st out of 149 in the country \u2013 and 21st in Greater London \u2013 based on the percentage of pupils attaining at least 5 A*\u2013C grades at GCSE including maths and English (37.8% compared with the national average of 46.7%). The most successful public sector schools in 2010 were Harris City Academy Crystal Palace and Coloma Convent Girls' School. The percentage of pupils achieving 5 A*-C GCSEs including maths and English was above the national average in 2010.\nLibraries.\nThe borough of Croydon has 14 libraries, a joint library and a mobile library. Many of the libraries were built a long time ago and therefore have become outdated, so the council started updating a few including Ashburton Library which moved from its former spot into the state-of-the-art Ashburton Learning Village complex which is on the former site of the old 'A Block' of Ashburton Community School which is now situated inside the centre. The library is now on one floor. This format was planned to be rolled out across all of the council's libraries but what was seen as costing too much.\nSouth Norwood Library, New Addington Library, Shirley Library, Selsdon Library, Sanderstead Library, Broad Green, Purley Library, Coulsdon Library and Bradmore Green Library are examples of older council libraries. The main library is Croydon Central Library which holds many references, newspaper archives and a tourist information point (one of three in southeast London). Upper Norwood Library is a joint library with the London Borough of Lambeth. This means that both councils fund the library and its resources, but even though Lambeth have nearly doubled their funding for the library in the past several years Croydon has kept it the same, doubting the future of the library.\nReligion.\nThe predominant religion of the borough is Christianity. According to the United Kingdom Census 2001, the borough has over 215,124 Christians, mainly Protestants. This is the largest religious following in the borough followed by Islam with 17,642 Muslims resident. This is a small portion of the more than 600,000 Muslims in London as a whole.\n48,615 Croydon residents stated that they are atheist or non-religious in the 2001 Census.\nThere are more than 35 churches in the borough, with Croydon Minster being the main one. This church was founded in Saxon times, since there is a record of \"a priest of Croydon\" in 960, although the first record of a church building is in the Domesday Book (1086). In its final medieval form, the church was mainly a Perpendicular-style structure, but this was severely damaged by fire in 1867, following which only the tower, south porch and outer walls remained. Under the direction of Sir George Gilbert Scott the church was rebuilt, incorporating the remains and essentially following the design of the medieval building, and was reconsecrated in 1870. It still contains several important monuments and fittings saved from the old church.\nCroydon has strong religious links, from a royal charter for Surrey Street Market dating back to 1276, to Croydon Palace which was the summer residence of the Archbishop of Canterbury for over 500 years, with visitors such as Henry III and Queen Elizabeth I. The Area Bishop of Croydon is a position as a suffragan Bishop in the Anglican Diocese of Southwark. The present bishop is the Right Reverend Jonathan Clark.\nSport and leisure.\nThe borough has been criticised in the past for not having enough leisure facilities, maintaining the position of Croydon as a three star borough. Thornton Heath's ageing sports centre has been demolished and replaced by a newer more modern leisure centre. South Norwood Leisure Centre was closed down in 2006 so that it could be demolished and re-designed from scratch like Thornton Heath, at an estimated cost of around \u00a310 million.\nIn May 2006 the Conservative Party took control of Croydon Council and decided a refurbishment would be more economical than rebuilding, this decision caused some controversy.\nSport Croydon, is the commercial arm for leisure in the borough. Fusion currently provides leisure services for the council, a contract previously held by Parkwood Leisure.\nFootball teams include Crystal Palace F.C., which play at Selhurst Park, and in the Premier League. AFC Croydon Athletic, whose nickname is The Rams, is a football club who play at Croydon Sports Arena along with Croydon F.C., both in the Combined Counties League and Holmesdale, who were founded in South Norwood but currently playing on Oakley Road in Bromley, currently in the Southern Counties East Football League.\nNon-football teams that play in Croydon are Streatham-Croydon RFC, a rugby union club in Thornton Heath who play at Frant Road, as well as South London Storm Rugby League Club, based at Streatham's ground, who compete in the Rugby League Conference. Another rugby union club that play in Croydon is Croydon RFC, who play at Addington Road. The London Olympians are an American Football team that play in Division 1 South in the British American Football League. The Croydon Pirates are one of the most successful teams in the British Baseball Federation, though their ground is actually just located outside the borough in Sutton.\nCroydon Amphibians SC plays in the Division 2 British Water Polo League. The team won the National League Division 2 in 2008.\nCroydon has over 120 parks and open spaces, ranging from the Selsdon Wood Nature Reserve to many recreation grounds and sports fields scattered throughout the Borough.\nCulture.\nCroydon has cut funding to the Warehouse Theatre.\nIn 2005, Croydon Council drew up a \"Public Art Strategy\", with a vision intended to be accessible and to enhance people's enjoyment of their surroundings. The public art strategy delivered a new event called \"Croydon's Summer Festival\" hosted in Lloyd Park. The festival consists of two days of events. The first is called \"Croydon's World Party\" which is a free one-day event with three stages featuring world, jazz and dance music from the UK and internationally. The final days event is the \"Croydon Mela\", a day of music with a mix of traditional Asian culture and east-meets-western club beats across four stages as well as dozens of food stalls and a funfair. It has attracted crowds of over 50,000 people. The strategy also created a creative industries hub in Old Town, ensured that public art is included in developments such as College Green and Ruskin Square and investigated the possibility of gallery space in the Cultural Quarter.\nFairfield Halls, Arnhem Gallery and the Ashcroft Theatre show productions that are held throughout the year such as drama, ballet, opera and pantomimes and can be converted to show films. It also contains the Arnhem Gallery civic hall and an art gallery. Other cultural activities, including shopping and exhibitions, are Surrey Street Market which is mainly a meat and vegetables market near the main shopping environment of Croydon. The market has a Royal Charter dating back to 1276. Airport House is a newly refurbished conference and exhibition centre inside part of Croydon Airport. The Whitgift Centre is the current main shopping centre in the borough. Centrale is a new shopping centre that houses many more familiar names, as well as Croydon's House of Fraser.\nMedia.\nThere are three local newspapers which operate within the borough. The Croydon Advertiser began life in 1869, and was in 2005 the third-best selling paid-for weekly newspaper in London. The Advertiser is Croydon's major paid-for weekly paper and is on sale every Friday in five geographical editions: Croydon; Sutton &amp; Epsom; Coulsdon &amp; Purley; New Addington; and Caterham. The paper converted from a broadsheet to a compact (tabloid) format on 31 March 2006. It was bought by Northcliffe Media which is part of the Daily Mail and General Trust group on 6 July 2007. The Croydon Post is a free newspaper available across the borough and is operated by the Advertiser group. The circulation of the newspaper was in 2008 more than the main title published by the Advertiser Group.\nThe Croydon Guardian is another local weekly paper, which is paid for at newsagents but free at Croydon Council libraries and via deliveries. It is one of the best circulated local newspapers in London and once had the highest circulation in Croydon with around one thousand more copies distributed than The Post.\nThe borough is served by the London regional versions of BBC and ITV coverage, from either the Crystal Palace or Croydon transmitters.\nCroydon Television is owned by Croydon broadcasting corporation. Broadcasting from studios in Croydon, the CBC is fully independent. It does not receive any government or local council grants or funding and is supported by donations, sponsorship and by commercial advertising.\nCapital Radio and Gold serve the borough. Local BBC radio is provided by BBC London 94.9. Other stations include Kiss 100, Absolute Radio and Magic 105.4 FM from Bauer Radio and Capital Xtra, Heart 106.2 and Smooth Radio from Global Radio. In 2012, Croydon Radio, an internet radio station, began serving the area.\nTwinning.\nThe London Borough of Croydon is twinned with the municipality of Arnhem which is located in the east of the Netherlands. The city of Arnhem is one of the 20 largest cities in the Netherlands. They have been twinned since 1946 after both towns had suffered extensive bomb damage during the recently ended war. There is also a Guyanese link supported by the council.\nInvestment in the tobacco industry.\nIn September 2009 it was revealed that Croydon Council had around \u00a320m of its pension fund for employees invested in shares in Imperial Tobacco and British American Tobacco. Members of the opposition Labour group on the council, who had banned such shareholdings when in control, described this as \"dealing in death\" and inconsistent with the council's tobacco control strategy."}
{"id": "7187", "revid": "999006735", "url": "https://en.wikipedia.org/wiki?curid=7187", "title": "Chick Publications", "text": ""}
{"id": "7188", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=7188", "title": "Carme (moon)", "text": "Carme is a retrograde irregular satellite of Jupiter. It was discovered by Seth Barnes Nicholson at Mount Wilson Observatory in California in July 1938. It is named after the mythological Carme, mother by Zeus of Britomartis, a Cretan goddess.\nHistory.\nCarme did not receive its present name until 1975; before then, it was simply known as . It was sometimes called \"Pan\" between 1955 and 1975 (Pan is now the name of a satellite of Saturn).\nIt gives its name to the Carme group, made up of irregular retrograde moons orbiting Jupiter at a distance ranging between 23 and 24 Gm and at an inclination of about 165\u00b0. Its orbital elements are as of January 2000. They are continuously changing due to solar and planetary perturbations."}
{"id": "7189", "revid": "300", "url": "https://en.wikipedia.org/wiki?curid=7189", "title": "Commedia del arte", "text": ""}
{"id": "7193", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=7193", "title": "Commutator", "text": "In mathematics, the commutator gives an indication of the extent to which a certain binary operation fails to be commutative. There are different definitions used in group theory and ring theory.\nGroup theory.\nThe commutator of two elements, and , of a group , is the element\nThis element is equal to the group's identity if and only if and commute (from the definition , being equal to the identity if and only if ).\nThe set of all commutators of a group is not in general closed under the group operation, but the subgroup of \"G\" generated by all commutators is closed and is called the \"derived group\" or the \"commutator subgroup\" of \"G\". Commutators are used to define nilpotent and solvable groups and the largest abelian quotient group.\nThe definition of the commutator above is used throughout this article, but many other group theorists define the commutator as\nIdentities (group theory).\nCommutator identities are an important tool in group theory. The expression denotes the conjugate of by , defined as .\nRelation (3) is called anticommutativity, while (4) is the Jacobi identity.\nAdditional identities.\nSome of the above identities can be extended to the anticommutator using the above \u00b1 subscript notation.\nFor example:\nExponential identities.\nConsider a ring or algebra in which the [[exponential function|exponential]] formula_25 can be meaningfully defined, such as a [[Banach algebra]], a ring of [[formal power series]], or the [[universal enveloping algebra]] of a [[Lie algebra]].\nIn such a ring, [[Hadamard's lemma]] applied to nested commutators gives:formula_26(For the last expression, see \"Adjoint derivation\" below.) This formula underlies the [[Baker\u2013Campbell\u2013Hausdorff formula#An important lemma|Baker\u2013Campbell\u2013Hausdorff expansion]] of log(exp(\"A\") exp(\"B\")).\nA similar expansion expresses the group commutator of expressions formula_27 (analogous to elements of a Lie group) in terms of a series of nested commutators (Lie brackets),\nformula_29\nGraded rings and algebras.\nWhen dealing with graded algebras, the commutator is usually replaced by the graded commutator, defined in homogeneous components as \nAdjoint derivation.\nEspecially if one deals with multiple commutators in a ring \"R\", another notation turns out to be useful. For an element formula_31, we define the adjoint mapping formula_32 by:\nThis mapping is a derivation on the ring \"R\": \nBy the Jacobi identity, it is also a derivation over the commutation operation: \nComposing such mappings, we get for example formula_36 andformula_37 We may consider formula_38 itself as a mapping, formula_39, where formula_40 is the ring of mappings from \"R\" to itself with composition as the multiplication operation. Then formula_38 is a Lie algebra homomorphism, preserving the commutator:\nBy contrast, it is not always a ring homomorphism: usually formula_43.\nGeneral Leibniz rule.\nThe general Leibniz rule, expanding repeated derivatives of a product, can be written abstractly using the adjoint representation:\nReplacing \"x\" by the differentiation operator formula_45, and \"y\" by the multiplication operator formula_46, we get formula_47, and applying both sides to a function \"g\", the identity becomes the usual Leibniz rule for the \"n\"th derivative formula_48."}
{"id": "7196", "revid": "16054025", "url": "https://en.wikipedia.org/wiki?curid=7196", "title": "Cairn", "text": "A cairn is a man-made pile (or stack) of stones. The word \"cairn\" comes from the (plural ).\nCairns have been and are used for a broad variety of purposes, from prehistoric times to the present.\nIn modern times, cairns are often erected as landmarks, a use they have had since ancient times. However, since prehistory, they have also been built and used as burial monuments; for defense and hunting; for ceremonial purposes, sometimes relating to astronomy; to locate buried items, such as caches of food or objects; and to mark trails, among other purposes.\nCairns are used as trail markers in many parts of the world, in uplands, on moorland, on mountaintops, near waterways and on sea cliffs, as well as in barren deserts and tundras. They vary in size from small stone markers to entire artificial hills, and in complexity from loose conical rock piles to delicately balanced sculptures and elaborate feats of megalithic engineering. Cairns may be painted or otherwise decorated, whether for increased visibility or for religious reasons. An ancient example is the inuksuk (plural inuksuit), used by the Inuit, Inupiat, Kalaallit, Yupik, and other peoples of the Arctic region of North America. Inuksuit are found from Alaska to Greenland. This region, above the Arctic Circle, is dominated by the tundra biome and has areas with few natural landmarks.\nModern cairns.\nDifferent types of cairns exist from rough piles of stones to interlocking dry stone round cylinders. The most important cairns commonly used around the world are interlocking stone survey cairns constructed around a central survey mark about every 30\u00a0km on the tallest peaks across a nation. These physical survey mark cairn systems are the basis for national survey grids to interconnect individual land survey measurements for entire nations. On occasion these permanent interlocking stone cairns are taken down then reconstructed to re-mark measurements to increase the accuracy of the national survey grid. They can also be used in unpopulated countries as emergency location points. In North America and Northern Europe any type of cairn can be used to mark mountain bike and hiking trails and other cross-country trail blazing, especially in mountain regions at or above the tree line. For example, the extensive trail network maintained by the DNT, the Norwegian Trekking Association, extensively uses cairns in conjunction with T-painted rock faces to mark trails. Other examples of these can be seen in the lava fields of Volcanoes National Park to mark several hikes. Placed at regular intervals, a series of cairns can be used to indicate a path across stony or barren terrain, even across glaciers. Such cairns are often placed at junctions or in places where the trail direction is not obvious. They may also be used to indicate an obscured danger such as a sudden drop, or a noteworthy point such as the summit of a mountain. Most trail cairns are small, usually being a foot or less in height. However, they may be built taller so as to protrude through a layer of snow. Hikers passing by often add a stone, as a small bit of maintenance to counteract the erosive effects of severe weather. North American trail marks are sometimes called \"ducks\" or \"duckies\", because they sometimes have a \"beak\" pointing in the direction of the route. The expression \"two rocks do not make a duck\" reminds hikers that just one rock resting upon another could be the result of accident or nature rather than intentional trail marking.\nThe building of cairns for recreational purposes along trails, to mark one's personal passage through the area, can result in an overabundance of rock piles. This distracts from cairns used as genuine navigational guides, and also conflicts with the Leave No Trace ethic. This ethic of outdoor practice advocates for leaving the outdoors undisturbed and in its natural condition.\nCoastal cairns, or \"sea marks\", are also common in the northern latitudes, especially in the island-strewn waters of Scandinavia and eastern Canada. Often indicated on navigation charts, they may be painted white or lit as beacons for greater visibility offshore.\nModern cairns may also be erected for historical or memorial commemoration or simply for decorative or artistic reasons. One example is a series of many cairns marking British soldiers' mass graves at the site of the Battle of Isandlwana, South Africa. Another is the Matthew Flinders Cairn on the side of Arthur's Seat, a small mountain on the shores of Port Phillip Bay, Australia. A large cairn, commonly referred to as \"the igloo\" by the locals, was built atop a hill next to the I-476 highway in Radnor, Pennsylvania and is a part of a series of large rock sculptures initiated in 1988 to symbolize the township's Welsh heritage and to beautify the visual imagery along the highway. Some are merely places where farmers have collected stones removed from a field. These can be seen in the Catskill Mountains, North America where there is a strong Scottish heritage, and may also represent places where livestock were lost. In locales exhibiting fantastic rock formations, such as the Grand Canyon, tourists often construct simple cairns in reverence of the larger counterparts. By contrast, cairns may have a strong aesthetic purpose, for example in the art of Andy Goldsworthy.\nHistory.\nEurope.\nThe building of cairns for various purposes goes back into prehistory in Eurasia, ranging in size from small rock sculptures to substantial man-made hills of stone (some built on top of larger, natural hills). The latter are often relatively massive Bronze Age or earlier structures which, like kistvaens and dolmens, frequently contain burials; they are comparable to tumuli (kurgans), but of stone construction instead of earthworks. \"Cairn\" originally could more broadly refer to various types of hills and natural stone piles, but today is used exclusively of artificial ones.\nThe word \"cairn\" derives from Scots (with the same meaning), in turn from Scottish Gaelic , which is essentially the same as the corresponding words in other native Celtic languages of Britain, Ireland and Brittany, including Welsh (and ), Breton , Irish , and Cornish or . Cornwall () itself may actually be named after the cairns that dot its landscape, such as Cornwall's highest point, Brown Willy Summit Cairn, a 5\u00a0m (16\u00a0ft) high and 24\u00a0m (79\u00a0ft) diameter mound atop Brown Willy hill in Bodmin Moor, an area with many ancient cairns. Burial cairns and other megaliths are the subject of a variety of legends and folklore throughout Britain and Ireland. In Scotland, it is traditional to carry a stone up from the bottom of a hill to place on a cairn at its top. In such a fashion, cairns would grow ever larger. An old Scottish Gaelic blessing is , \"I'll put a stone on your stone\". In Highland folklore it is recounted that before Highland clans fought in a battle, each man would place a stone in a pile. Those who survived the battle returned and removed a stone from the pile. The stones that remained were built into a cairn to honour the dead. Cairns in the region were also put to vital practical use. For example, D\u00fan Aonghasa, an all-stone Iron Age Irish hill fort on Inishmore in the Aran Islands, is still surrounded by small cairns and strategically placed jutting rocks, used collectively as an alternative to defensive earthworks because of the karst landscape's lack of soil.\nIn Scandinavia, cairns have been used for centuries as trail and sea marks, among other purposes. In Iceland, cairns were often used as markers along the numerous single-file roads or paths that crisscrossed the island; many of these ancient cairns are still standing, although the paths have disappeared. In Norse Greenland, cairns were used as a hunting implement, a game-driving \"lane\", used to direct reindeer towards a game jump.\nIn the mythology of ancient Greece, cairns were associated with Hermes, the god of overland travel. According to one legend, Hermes was put on trial by Hera for slaying her favorite servant, the monster Argus. All of the other gods acted as a jury, and as a way of declaring their verdict they were given pebbles, and told to throw them at whichever person they deemed to be in the right, Hermes or Hera. Hermes argued so skillfully that he ended up buried under a heap of pebbles, and this was the first cairn.\nIn Croatia, in areas of ancient Dalmatia, such as Herzegovina and the Krajina, they are known as \"gromila\".\nIn Portugal a cairn is called a . In a legend the moledros are enchanted soldiers, and if one stone is taken from the pile and put under a pillow, in the morning a soldier will appear for a brief moment, then will change back to a stone and magically return to the pile. The cairns that mark the place where someone died or cover the graves alongside the roads where in the past people were buried are called . The same name given to the stones was given to the dead whose identity was unknown. The or are, in the Galician legends, spirits of the night. The word \"Fes\" or \"Fieis\" is thought to mean fairy, the same root as \"fate\" (\"fado\"), that can take the same meaning as the proto-Celtic *b\u0101sto-, *b\u0101sso-, meaning \"death\".\nNorth and northeast Africa.\nCairns (\"taalo\") are a common feature at El Ayo, Haylan, Qa\u2019ableh, Qombo'ul, Heis, Salweyn and Gelweita, among other places. Somaliland in general is home to a lot of such historical settlements and archaeological sites wherein are found numerous ancient ruins and buildings, many of obscure origins. However, many of these old structures have yet to be properly explored, a process which would help shed further light on local history and facilitate their preservation for posterity.\nSince Neolithic times, the climate of North Africa has become drier. A reminder of the desertification of the area is provided by megalithic remains, which occur in a great variety of forms and in vast numbers in presently arid and uninhabitable wastelands: cairns (\"kerkour\"), dolmens and circles like Stonehenge, underground cells excavated in rock, barrows topped with huge slabs, and step pyramid-like mounds.\nAsia and the Pacific.\nStarting in the Bronze Age, burial cists were sometimes interred into cairns, which would be situated in conspicuous positions, often on the skyline above the village of the deceased. Though most often found in the British Isles, evidence of Bronze Age cists have been found in Mongolia. The stones may have been thought to deter grave robbers and scavengers. Another explanation is that they were to stop the dead from rising. There remains a Jewish tradition of placing small stones on a person's grave as a token of respect, though this is generally to relate the longevity of stone to the eternal nature of the soul and is not usually done in a cairn fashion. Stupas in India and Tibet probably started out in a similar fashion, although they now generally contain the ashes of a Buddhist saint or lama.\nA traditional and often decorated, heap-formed cairn called an \"ovoo\" is made in Mongolia. It primarily serves religious purposes, and finds use in both Tengriist and Buddhist ceremonies.\nIn Hawaii, cairns, called by the Hawaiian word , are still being built today. Though in other cultures the cairns were typically used as trail markers and sometimes funerary sites, the ancient Hawaiians also used them as altars or security tower. The Hawaiian people are still building these cairns today, using them as the focal points for ceremonies honoring their ancestors and spirituality. \nIn South Korea cairns are quite prevalent, often found along roadsides and trails, up on mountain peaks, and adjacent to Buddhist temples. Hikers frequently add stones to existing cairns trying to get just one more on top of the pile, to bring good luck. This tradition has its roots in the worship of San-shin, or Mountain Spirit, so often still revered in Korean culture.\nThe Americas.\nThroughout what today are the continental United States and Canada, some Indigenous peoples of the Americas have built structures similar to cairns. In some cases these are general trail markers, and in other cases they mark game-driving \"lanes\", such as those leading to buffalo jumps.\nPeoples from some of the Indigenous cultures of arctic North America (i.e. northern Canada, Alaska and Greenland) have built carefully constructed stone sculptures called and, which serve as landmarks and directional markers. The oldest of these structures are very old and pre-date contact with Europeans. They are iconic of the region (an even features on the flag of the Canadian far-northeastern territory, Nunavut).\nCairns have been used throughout what is now Latin America, since pre-Columbian times, to mark trails. Even today, in the Andes of South America, the Quechuan peoples build cairns as part of their spiritual and religious traditions.\nArchaeology.\nIn February 2020, ancient cairns dated back to 4,500 year-old used to bury the leaders or chieftains of neolithic tribes people were revealed in the Cwmcelyn in Blaenau Gwent by the Aberystruth Archaeological Society.\nAnthropomorphism.\nAlthough the practice is not common in English, cairns are sometimes referred to by their anthropomorphic qualities. In German and Dutch, a cairn is known as and respectively, meaning literally \"stone man\". A form of the Inuit is also meant to represent a human figure, and is called an (\"imitation of a person\"). In Italy, especially the Italian Alps, a cairn is an , or a \"small man\".\nSea cairns.\nCoastal cairns called sea marks are also common in the northern latitudes, and are placed along shores and on islands and islets. Usually painted white for improved offshore visibility, they serve as navigation aids. In Sweden they are called , in Finland , in Norway , and are indicated in navigation charts and maintained as part of the nautical marking system. Inversely, they are used on land as sea cliff warnings in rugged and hilly terrain in the foggy Faroe Islands. In the Canadian Maritimes, cairns have been used as beacons like small lighthouses to guide boats, as depicted in the novel \"The Shipping News\"."}
{"id": "7198", "revid": "70", "url": "https://en.wikipedia.org/wiki?curid=7198", "title": "Characteristic subgroup", "text": "In mathematics, particularly in the area of abstract algebra known as group theory, a characteristic subgroup is a subgroup that is mapped to itself by every automorphism of the parent group. Because every conjugation map is an inner automorphism, every characteristic subgroup is normal; though the converse is not guaranteed. Examples of characteristic subgroups include the commutator subgroup and the center of a group.\nDefinition.\nA subgroup of a group is called a characteristic subgroup if for every automorphism of , one has ; then write . \nIt would be equivalent to require the stronger condition = for every automorphism of , because implies the reverse inclusion .\nBasic properties.\nGiven , every automorphism of induces an automorphism of the quotient group , which yields a homomorphism .\nIf has a unique subgroup of a given index, then is characteristic in .\nRelated concepts.\nNormal subgroup.\nA subgroup of that is invariant under all inner automorphisms is called normal; also, an invariant subgroup.\nSince and a characteristic subgroup is invariant under all automorphisms, every characteristic subgroup is normal. However, not every normal subgroup is characteristic. Here are several examples:\nStrictly characteristic subgroup.\nA ', or a ', which is invariant under surjective endomorphisms. For finite groups, surjectivity of an endomorphism implies injectivity, so a surjective endomorphism is an automorphism; thus being \"strictly characteristic\" is equivalent to \"characteristic\". This is not the case anymore for infinite groups.\nFully characteristic subgroup.\nFor an even stronger constraint, a \"fully characteristic subgroup\" (also, \"fully invariant subgroup\"; cf. invariant subgroup), , of a group , is a group remaining invariant under every endomorphism of ; that is,\nEvery group has itself (the improper subgroup) and the trivial subgroup as two of its fully characteristic subgroups. The commutator subgroup of a group is always a fully characteristic subgroup.\nEvery endomorphism of induces an endomorphism of , which yields a map .\nVerbal subgroup.\nAn even stronger constraint is verbal subgroup, which is the image of a fully invariant subgroup of a free group under a homomorphism. More generally, any verbal subgroup is always fully characteristic. For any reduced free group, and, in particular, for any free group, the converse also holds: every fully characteristic subgroup is verbal.\nTransitivity.\nThe property of being characteristic or fully characteristic is transitive; if is a (fully) characteristic subgroup of , and is a (fully) characteristic subgroup of , then is a (fully) characteristic subgroup of .\nMoreover, while normality is not transitive, it is true that every characteristic subgroup of a normal subgroup is normal.\nSimilarly, while being strictly characteristic (distinguished) is not transitive, it is true that every fully characteristic subgroup of a strictly characteristic subgroup is strictly characteristic.\nHowever, unlike normality, if and is a subgroup of containing , then in general is not necessarily characteristic in .\nContainments.\nEvery subgroup that is fully characteristic is certainly strictly characteristic and characteristic; but a characteristic or even strictly characteristic subgroup need not be fully characteristic.\nThe center of a group is always a strictly characteristic subgroup, but it is not always fully characteristic. For example, the finite group of order 12, , has a homomorphism taking to , which takes the center, , into a subgroup of , which meets the center only in the identity.\nThe relationship amongst these subgroup properties can be expressed as:\nExamples.\nFinite example.\nConsider the group (the group of order 12 that is the direct product of the symmetric group of order 6 and a cyclic group of order 2). The center of is its second factor . Note that the first factor, , contains subgroups isomorphic to , for instance ; let be the morphism mapping onto the indicated subgroup. Then the composition of the projection of onto its second factor , followed by , followed by the inclusion of into as its first factor, provides an endomorphism of under which the image of the center, , is not contained in the center, so here the center is not a fully characteristic subgroup of .\nCyclic groups.\nEvery subgroup of a cyclic group is characteristic.\nSubgroup functors.\nThe derived subgroup (or commutator subgroup) of a group is a verbal subgroup. The torsion subgroup of an abelian group is a fully invariant subgroup.\nTopological groups.\nThe identity component of a topological group is always a characteristic subgroup."}
{"id": "7199", "revid": "7641765", "url": "https://en.wikipedia.org/wiki?curid=7199", "title": "List of cat breeds", "text": "The following list of cat breeds includes only domestic cat breeds and domestic\u00a0\u00d7 wild hybrids. The list includes established breeds recognized by various cat registries, new and experimental breeds, landraces being established as standardized breeds, distinct domestic populations not being actively developed and lapsed (extinct) breeds.\nAs of 2019, The International Cat Association (TICA) recognizes 71 standardized breeds, the Cat Fanciers' Association (CFA) recognizes 44, and the F\u00e9d\u00e9ration Internationale F\u00e9line (FIFe) recognizes 43.\nInconsistency in breed classification and naming among registries means that an individual animal may be considered different breeds by different registries (though not necessarily eligible for registry in them all, depending on its exact ancestry). For example, TICA's Himalayan is considered a colorpoint variety of the Persian by the CFA, while the Javanese (or Colorpoint Longhair) is a color variation of the Balinese in both the TICA and the CFA; both breeds are merged (along with the Colorpoint Shorthair) into a single \"mega-breed\", the Colourpoint, by the World Cat Federation (WCF), who have repurposed the name \"Javanese\" for the Oriental Longhair. Also, \"Colo[u]rpoint Longhair\" refers to different breeds in other registries. There are many examples of nomenclatural overlap and differences of this sort. Furthermore, many geographical and cultural names for cat breeds are fanciful selections made by Western breeders to be exotic sounding and bear no relationship to the actual origin of the breeds; the Balinese, Javanese, and Himalayan are all examples of this trend.\nThe domestic short-haired and domestic long-haired cat types are not breeds, but terms used (with various spellings) in the cat fancy to describe \"mongrel\" cats by coat length, ones that do not belong to a particular breed. Some registries permit them to be pedigreed and they have been used as foundation stock in the establishment of some breeds. They should not be confused with standardized breeds with similar names, such as the British Shorthair and Oriental Longhair."}
{"id": "7200", "revid": "1005546509", "url": "https://en.wikipedia.org/wiki?curid=7200", "title": "Class action", "text": "A class action, also known as a class-action lawsuit, class suit, or representative action, is a type of lawsuit where one of the parties is a group of people who are represented collectively by a member or members of that group. The class action originated in the United States and is still predominantly a U.S. phenomenon, but Canada, as well as several European countries with civil law, have made changes in recent years to allow consumer organizations to bring claims on behalf of consumers.\nDescription.\nIn a typical class action, a plaintiff sues a defendant or a number of defendants on behalf of a group, or class, of absent parties. This differs from a traditional lawsuit, where one party sues another party, and all of the parties are present in court. Although standards differ between states and countries, class actions are most common where the allegations usually involve at least 40 people who have been injured by the same defendant in the same way. Instead of each damaged person bringing his or her own lawsuit, the class action allows all the claims of all class members\u2014whether they know they have been damaged or not\u2014to be resolved in a single proceeding through the efforts of the representative plaintiff(s) and appointed class counsel.\nHistory.\nEngland.\nThe antecedent of the class action was what modern observers call \"group litigation\", which appears to have been quite common in medieval England from about 1200 onward. These lawsuits involved groups of people either suing or being sued in actions at common law. These groups were usually based on existing societal structures like villages, towns, parishes, and guilds. Unlike modern courts, the medieval English courts did not question the right of the actual plaintiffs to sue on behalf of a group or a few representatives to defend an entire group.\nFrom 1400 to 1700, group litigation gradually switched from being the norm in England to the exception. The development of the concept of the corporation led to the wealthy supporters of the corporate form becoming suspicious of all unincorporated legal entities, which in turn led to the modern concept of the unincorporated or voluntary association. The tumultuous history of the Wars of the Roses and then the Star Chamber resulted in periods during which the common law courts were frequently paralyzed, and out of the confusion the Court of Chancery emerged with exclusive jurisdiction over group litigation. \nBy 1850, the Parliament of England had enacted several statutes on a case-by-case basis to deal with issues regularly faced by certain types of organizations, like joint-stock companies, and with the impetus for most types of group litigation removed, it went into a steep decline in English jurisprudence from which it never recovered. It was further weakened by the fact that equity pleading in general was falling into disfavor, which culminated in the Judicature Acts of 1874 and 1875. Group litigation was essentially dead in England after 1850.\nUnited States.\nClass actions survived in the United States thanks to the influence of Supreme Court Associate Justice Joseph Story, who imported it into U.S. law through summary discussions in his two equity treatises as well as his opinion in \"West v. Randall\" (1820). However, Story did not necessarily endorse class actions, because he \"could not conceive of a modern function or a coherent theory for representative litigation\".\nThe oldest predecessor to the class-action rule in the United States was in the Federal Equity Rules, specifically Equity Rule 48, promulgated in 1842.\nWhere the parties on either side are very numerous, and cannot, without manifest inconvenience and oppressive delays in the suit, be all brought before it, the court in its discretion may dispense with making all of them parties, and may proceed in the suit, having sufficient parties before it to represent all the adverse interests of the plaintiffs and the defendants in the suit properly before it. But in such cases, the decree shall be without prejudice to the rights and claims of all the absent parties.\nThis allowed for representative suits in situations where there were too many individual parties (which now forms the first requirement for class-action litigation \u2013 numerosity). However, this rule did not allow such suits to bind similarly situated absent parties, which rendered the rule ineffective. Within ten years, the Supreme Court interpreted Rule 48 in such a way so that it could apply to absent parties under certain circumstances, but only by ignoring the plain meaning of the rule. In the rules published in 1912, Equity Rule 48 was replaced with Equity Rule 38 as part of a major restructuring of the Equity Rules, and when federal courts merged their legal and equitable procedural systems in 1938, Equity Rule 38 became Rule 23 of the Federal Rules of Civil Procedure.\nModern developments.\nA major revision of the FRCP in 1966 radically transformed Rule 23, made the opt-out class action the standard option, and gave birth to the modern class action. Entire treatises have been written since to summarize the huge mass of law that sprang up from the 1966 revision of Rule 23. Just as medieval group litigation bound all members of the group regardless of whether they all actually appeared in court, the modern class action binds \"all\" members of the class, except for those who choose to opt out (if the rules permit them to do so).\nThe Advisory Committee that drafted the new Rule 23 in the mid-1960s was influenced by two major developments. First was the suggestion of Harry Kalven, Jr. and Maurice Rosenfield in 1941 that class-action litigation by individual shareholders on behalf of all shareholders of a company could effectively supplement direct government regulation of securities markets and other similar markets. The second development was the rise of the civil rights movement, environmentalism and consumerism. The groups behind these movements, as well as many others in the 1960s, 1970s and 1980s, all turned to class actions as a means for achieving their goals. For example, a 1978 environmental law treatise reprinted the \"entire\" text of Rule 23 and mentioned \"class actions\" 14 times in its index.\nBusinesses targeted by class actions for inflicting massive aggregate harm have sought ways to avoid class actions altogether. In the 1990s, the U.S. Supreme Court issued a number of decisions which strengthened the \"federal policy favoring arbitration\". In response, lawyers have added provisions to consumer contracts of adhesion called \"collective action waivers\", which prohibit those signing the contracts from bringing class-action suits. In 2011, the U.S. Supreme Court ruled in a 5\u20134 decision in \"AT&amp;T Mobility v. Concepcion\" that the Federal Arbitration Act of 1925 preempts state laws that prohibit contracts from disallowing class-action lawsuits, which will make it more difficult for consumers to file class-action lawsuits. The dissent pointed to a saving clause in the federal act which allowed states to determine how a contract or its clauses may be revoked.\nIn two major 21st-century cases, the Supreme Court ruled 5\u20134 against certification of class actions due to differences in each individual members' circumstances: first in \"Wal-Mart v. Dukes\" (2011) and later in \"Comcast Corp. v. Behrend\" (2013).\nCompanies may insert the phrase \"may elect to resolve any claim by individual arbitration\" into their consumer and employment contracts to use arbitration and prevent class-action lawsuits.\nRejecting arguments that they violated employees\u2019 rights to collective bargaining, and that modestly-valued consumer claims would be more efficiently litigated within the parameters of one lawsuit, the U. S. Supreme Court, in \"Epic Systems Corp. v. Lewis\" (2018), sanctioned the use of so-called \"class action waivers\". Citing its deference to freedom to contract principles, the Epic Systems opinion opened the door dramatically to the use of these waivers as a condition of employment, consumer purchases and the like. Some commentators in opposition to the ruling see it as a \"death knell\" to many employment and consumer class actions, and have increasingly pushed for legislation to circumvent it in hopes of reviving otherwise-underrepresented parties\u2019 ability to litigate on a group basis. Supporters (mostly pro-business) of the high court\u2019s ruling argue its holding is consistent with private contract principles. Many of those supporters had long-since argued that class action procedures were generally inconsistent with due process mandates and unnecessarily promoted litigation of otherwise small claims\u2014thus heralding the ruling's anti-litigation effect.\nIn 2017, the U.S. Supreme Court issued its opinion in Bristol-Meyer Squibb Co. v. Superior Court of California, 137 S. Ct. 1773 (2017), holding that over five hundred plaintiffs from other states cannot bring a consolidated mass action against the pharmaceutical giant in the State of California. This opinion may arguably render nationwide mass action and class action impossible in any single state beside the defendant's home state.\nIn 2020, the 11th Circuit Court of Appeals found incentive awards are impermissible. Incentive awards are a relatively modest payment made to class representatives as part of a class settlement. The ruling was a response to an objector who claimed Rule 23 required that the fee petition be filed \"before\" the time frame for class member objections to be filed; and payments to the class representative violates doctrine from two U.S. Supreme Court cases from the 1800's.\nStatistics.\nAs of 2010, there was no publicly maintained list of nonsecurities class-action settlements, although a securities class-action database exists in the Stanford Law School Securities Class Action Clearinghouse and several for-profit companies maintain lists of the securities settlements. One study of federal settlements required the researcher to manually search databases of lawsuits for the relevant records, although state class actions were not included due to the difficulty in gathering the information. Another source of data is U.S. Bureau of Justice Statistics \"Civil Justice Survey of State Courts\", which offers statistics for the year 2005.\nAdvantages.\nProponents of class actions state that they offer a number of advantages because they aggregate many individualized claims into one representational lawsuit.\nFirst, aggregation can increase the efficiency of the legal process, and lower the costs of litigation. In cases with common questions of law and fact, aggregation of claims into a class action may avoid the necessity of repeating \"days of the same witnesses, exhibits and issues from trial to trial\". \"Jenkins v. Raymark Indus. Inc.\", 782 F.2d 468, 473 (5th Cir. 1986) (granting certification of a class action involving asbestos).\nSecond, a class action may overcome \"the problem that small recoveries do not provide the incentive for any individual to bring a solo action prosecuting his or her rights\". \"Amchem Prods., Inc. v. Windsor\", 521 U.S. 591, 617 (1997) (quoting \"Mace v. Van Ru Credit Corp.\", 109 F.3d 388, 344 (7th Cir. 1997)). \"A class action solves this problem by aggregating the relatively paltry potential recoveries into something worth someone's (usually an attorney's) labor.\" \"Amchem Prods., Inc.\", 521 U.S. at 617 (quoting \"Mace\", 109 F.3d at 344). In other words, a class action ensures that a defendant who engages in widespread harmbut does so minimally against each individual plaintiffmust compensate those individuals for their injuries. For example, thousands of shareholders of a public company may have losses too small to justify separate lawsuits, but a class action can be brought efficiently on behalf of all shareholders. Perhaps even more important than compensation is that class treatment of claims may be the only way to impose the costs of wrongdoing on the wrongdoer, thus deterring future wrongdoing.\nThird, class-action cases may be brought to purposely change behavior of a class of which the defendant is a member. \"Landeros v. Flood\" (1976) was a landmark case decided by the California Supreme Court that aimed at purposefully changing the behavior of doctors, encouraging them to report suspected child abuse. Otherwise, they would face the threat of civil action for damages in tort proximately flowing from the failure to report the suspected injuries. Previously, many physicians had remained reluctant to report cases of apparent child abuse, despite existing law that required it.\nFourth, in \"limited fund\" cases, a class action ensures that all plaintiffs receive relief and that early-filing plaintiffs do not raid the fund (i.e., the defendant) of all its assets before other plaintiffs may be compensated. See \"Ortiz v. Fibreboard Corp.\", 527 U.S. 815 (1999). A class action in such a situation centralizes all claims into one venue where a court can equitably divide the assets amongst all the plaintiffs if they win the case.\nFinally, a class action avoids the situation where different court rulings could create \"incompatible standards\" of conduct for the defendant to follow. See Fed. R. Civ. P. 23(b)(1)(A). For example, a court might certify a case for class treatment where a number of individual bond-holders sue to determine whether they may convert their bonds to common stock. Refusing to litigate the case in one trial could result in different outcomes and inconsistent standards of conduct for the defendant corporation. Thus, courts will generally allow a class action in such a situation. See, e.g., \"Van Gemert v. Boeing Co.\", 259 F. Supp. 125 (S.D.N.Y. 1966).\nWhether a class action is superior to individual litigation depends on the case and is determined by the judge's ruling on a motion for class certification. The Advisory Committee Note to Rule 23, for example, states that mass torts are ordinarily \"not appropriate\" for class treatment. Class treatment may not improve the efficiency of a mass tort because the claims frequently involve individualized issues of law and fact that will have to be re-tried on an individual basis. See \"Castano v. Am. Tobacco Co.\", 84 F.3d 734 (5th Cir. 1996) (rejecting nationwide class action against tobacco companies). Mass torts also involve high individual damage awards; thus, the absence of class treatment will not impede the ability of individual claimants to seek justice. Other cases, however, may be more conducive to class treatment.\nThe preamble to the Class Action Fairness Act of 2005, passed by the United States Congress, found:\nClass-action lawsuits are an important and valuable part of the legal system when they permit the fair and efficient resolution of legitimate claims of numerous parties by allowing the claims to be aggregated into a single action against a defendant that has allegedly caused harm. \nCriticisms.\nThere are several criticisms of class actions. The preamble to the Class Action Fairness Act stated that some abusive class actions harmed class members with legitimate claims and defendants that have acted responsibly, adversely affected interstate commerce, and undermined public respect for the country's judicial system.\nClass members often receive little or no benefit from class actions. Examples cited for this include large fees for the attorneys, while leaving class members with coupons or other awards of little or no value; unjustified awards are made to certain plaintiffs at the expense of other class members; and confusing notices are published that prevent class members from being able to fully understand and effectively exercise their rights.\nFor example, in the United States, class lawsuits sometimes bind all class members with a low settlement. These \"coupon settlements\" (which usually allow the plaintiffs to receive a small benefit such as a small check or a coupon for future services or products with the defendant company) are a way for a defendant to forestall major liability by precluding many people from litigating their claims separately, to recover reasonable compensation for the damages. However, existing law requires judicial approval of all class-action settlements, and in most cases class members are given a chance to opt out of class settlement, though class members, despite opt-out notices, may be unaware of their right to opt out because they did not receive the notice, did not read it, or did not understand it.\nThe Class Action Fairness Act of 2005 addresses these concerns. Coupon settlements may be scrutinized by an independent expert before judicial approval in order to ensure that the settlement will be of value to the class members (28 U.S.C.A. 1712(d)). Further, if the action provides for settlement in coupons, \"the portion of any attorney\u2019s fee award to class counsel that is attributable to the award of the coupons shall be based on the value to class members of the coupons that are redeemed\". 28 U.S.C.A. 1712(a).\nEthics.\nClass action cases present significant ethical challenges. Defendants can hold reverse auctions and any of several parties can engage in collusive settlement discussions. Subclasses may have interests that diverge greatly from the class but may be treated the same. Proposed settlements could offer some groups (such as former customers) much greater benefits than others. In one paper presented at an ABA conference on class actions in 2007, authors commented that \"competing cases can also provide opportunities for collusive settlement discussions and reverse auctions by defendants anxious to resolve their new exposure at the most economic cost\".\nDefendant class action.\nAlthough normally plaintiffs are the class, defendant class actions are also possible. For example, in 2005, the Roman Catholic Archdiocese of Portland in Oregon was sued as part of the Catholic priest sex-abuse scandal. All parishioners of the Archdiocese's churches were cited as a defendant class. This was done to include their assets (local churches) in any settlement. Where both the plaintiffs and the defendants have been organized into court-approved classes, the action is called a bilateral class action.\nMass actions.\nIn a class action, the plaintiff seeks court approval to litigate on behalf of a group of similarly situated persons. Not every plaintiff looks for, or could obtain, such approval. As a procedural alternative, plaintiff's counsel may attempt to sign up every similarly situated person that counsel can find as a client. Plaintiff's counsel can then join the claims of all of these persons in one complaint, a so-called \"mass action\", hoping to have the same efficiencies and economic leverage as if a class had been certified.\nBecause mass actions operate outside the detailed procedures laid out for class actions, they can pose special difficulties for both plaintiffs, defendants, and the court. For example, settlement of class actions follows a predictable path of negotiation with class counsel and representatives, court scrutiny, and notice. There may not be a way to uniformly settle all of the many claims brought via a mass action. Some states permit plaintiff's counsel to settle for all the mass action plaintiffs according to a majority vote, for example. Other states, such as New Jersey, require each plaintiff to approve the settlement of that plaintiff's own individual claims.\nClass action legislation.\nArgentina.\nClass actions were recognized in \"Halabi\" leading case (Supreme Court, 2009).\nAustralia and New Zealand.\nClass actions became part of the Australian legal landscape only when the Federal Parliament amended the Federal Court of Australia Act (\"the FCAA\") in 1992 to introduce the \"representative proceedings\", the equivalent of the American \"class actions\". \nLikewise, class actions appeared slowly in the New Zealand legal system. However, a group can bring litigation through the action of a representative under the High Court Rules which provide that one or a multitude of persons may sue on behalf of, or for the benefit of, all persons \"with the same interest in the subject matter of a proceeding\". The presence and expansion of litigation funders have been playing a significant role in the emergence of class actions in New Zealand. For example, the \"Fair Play on Fees\" proceedings in relation to penalty fees charged by banks was funded by Litigation Lending Services (LLS), a company specializing in the funding and management of litigation in Australia and New Zealand. It was the biggest class-action suit in New Zealand history.\nAustria.\nThe Austrian Code of Civil Procedure (\"Zivilprozessordnung\"\u00a0\u2013 ZPO) does not provide for a special proceeding for complex class-action litigation. However, Austrian consumer organizations (\"Verein f\u00fcr Konsumenteninformation\" (VKI) and the Federal Chamber of Labour / \"Bundesarbeitskammer\") have brought claims on behalf of hundreds or even thousands of consumers. In these cases the individual consumers assigned their claims to one entity, who has then brought an ordinary (two party) lawsuit over the assigned claims. The monetary benefits were redistributed among the class. This technique, labelled as \"class action Austrian style\", allows for a significant reduction of overall costs. The Austrian Supreme Court, in a judgment, confirmed the legal admissibility of these lawsuits under the condition that all claims are essentially based on the same grounds.\nThe Austrian Parliament unanimously requested the Austrian Federal Minister for Justice to examine the possibility of new legislation providing for a cost-effective and appropriate way to deal with mass claims. Together with the Austrian Ministry for Social Security, Generations and Consumer Protection, the Justice Ministry opened the discussion with a conference held in Vienna in June 2005. With the aid of a group of experts from many fields, the Justice Ministry began drafting the new law in September 2005. With the individual positions varying greatly, a political consensus could not be reached.\nCanada.\nProvincial laws in Canada allow class actions. All provinces permit plaintiff classes, and some permit defendant classes. Quebec was the first province to enact class proceedings legislation, in 1978. Ontario was next, with the Class Proceedings Act, 1992. As of 2008, 9 of 10 provinces had enacted comprehensive class actions legislation. In Prince Edward Island, where no comprehensive legislation exists, following the decision of the Supreme Court of Canada in \"Western Canadian Shopping Centres Inc. v. Dutton\", [2001] 2 S.C.R. 534, class actions may be advanced under a local rule of court. The Federal Court of Canada permits class actions under Part V.1 of the Federal Courts Rules.\nLegislation in Saskatchewan, Manitoba, Ontario, and Nova Scotia expressly or by judicial opinion has been read to allow for what are informally known as national \"opt-out\" class actions, whereby residents of other provinces may be included in the class definition and potentially be bound by the court's judgment on common issues unless they opt out in a prescribed manner and time. Court rulings have determined that this permits a court in one province to include residents of other provinces in the class action on an \"opt-out\" basis.\nJudicial opinions have indicated that provincial legislative national opt-out powers should not be exercised to interfere with the ability of another province to certify a parallel class action for residents of other provinces. The first court to certify will generally exclude residents of provinces whose courts have certified a parallel class action. However, in the Vioxx litigation, two provincial courts certified overlapping class actions whereby Canadian residents were class members in two class actions in two provinces. Both decisions are under appeal.\nThe largest class action suit in Canada was settled in 2005 after Nora Bernard initiated efforts that led to an estimated 79,000 survivors of Canada's residential school system suing the Canadian government. The settlement amounted to upwards of $5 billion.\nChile.\nChile approved class actions in 2004. The Chilean model is technically an opt-out issue class action, followed by a compensatory stage which can be collective or individual. This means that the class action is designed to declare the defendant generally liable with \"erga omnes\" effects if and only if the defendant is found liable, and the declaratory judgment can be used then to pursue damages in the same procedure or in individual ones in different jurisdictions. If the latter is the case, the liability cannot be discussed, but only the damages. There under the Chilean procedural rules, one particular case works as an opt-out class action for damages. This is the case when defendants can identify and compensate consumers directly, i.e. because it is their banking institution. In such cases the judge can skip the compensatory stage and order redress directly. Since 2005 more than 100 cases have been filed, mostly by \"Servicio Nacional del Consumidor\" [SERNAC], the Chilean consumer protection agency. Salient cases have been \"Condecus v. BancoEstado\" and \"SERNAC v. La Polar\".\nFrance.\nUnder French law, an association can represent the collective interests of consumers; however, each claimant must be individually named in the lawsuit. On January 4, 2005, President Chirac urged changes that would provide greater consumer protection. A draft bill was proposed in April 2006, but did not pass.\nFollowing the change of majority in France in 2012, the new government proposed introducing class actions into French law. The project of \"loi Hamon\" of May 2013 aimed to limit the class action to consumer and competition disputes. The law was passed on March 1, 2014.\nGermany.\nIn Germany, the concept of a class action suit in the common form is not permitted, as German law does not recognize the concept of a targeted class being affected by certain actions. This requires each plaintiff to individually prove that they were affected by an action, and present their individual damages, and prove the causality between both parties.\nJoint litigation () is a legal act that may permit plaintiffs that are in the same legal community with respect to the dispute, or are entitled by the same factual or legal reason. These are not typically regarded as class action suits, as each individual plaintiff is entitled to compensation for their individual, incurred damages and not as a result of being a member of a class.\nCombination of court cases () is another method which permits a judge to combine multiple separate court cases into a single trial with a single verdict. According to \u00a7 147 ZPO, this is only permissible if all cases are regarding the same factual and legal event and basis.\nOn November 1, 2005, Germany enacted the \"Act on Model Case Proceedings in Disputes under Capital Markets Law (Capital Markets Model Case Act)\" allowing sample proceedings to be brought before the courts in litigation arising from mass capital markets transactions. This was intended to be limited to cases regarding disputes in investment markets against several potential plaintiffs. It does not apply to any other civil law proceeding. It is not like class actions in the United Statesit only applies to parties who have already filed suit, and does not allow a claim to be brought in the name of an unknown group of claimants. The effects of the law will be monitored over five years. It contains a sunset clause, and it would automatically cease to have effect on November 1, 2010, unless the legislature decided to prolong the law, or extend it to other mass civil case proceedings.\nIndia.\nDecisions of the Indian Supreme Court in the 1980s loosened strict \"locus standi\" requirements to permit the filing of suits on behalf of rights of deprived sections of society by public-minded individuals or bodies. Although not strictly \"class action litigation\" as it is understood in American law, Public Interest Litigation arose out of the wide powers of judicial review granted to the Supreme Court of India and the various High Courts under and Article 226 of the Constitution of India. The sort of remedies sought from courts in Public Interest Litigation go beyond mere award of damages to all affected groups, and have sometimes (controversially) gone on to include Court monitoring of the implementation of legislation and even the framing of guidelines in the absence of Parliamentary legislation.\nHowever, this innovative jurisprudence did not help the victims of the Bhopal gas tragedy, who were unable to fully prosecute a class-action litigation (as understood in the American sense) against Union Carbide due to procedural rules that would make such litigation impossible to conclude and unwieldy to carry out. Instead, the Government of India exercised its right of \"parens patriae\" to appropriate all the claims of the victims and proceeded to litigate on their behalf, first in the New York courts and later, in the Indian courts. Ultimately, the matter was settled between the Union of India and Union Carbide (in a settlement overseen by the Supreme Court of India) for a sum of as a complete settlement of all claims of all victims for all time to come.\nPublic interest litigation has now broadened in scope to cover larger and larger groups of citizens who may be affected by government inaction. Examples of this trend include the conversion of all public transport in the city of Delhi from diesel engines to CNG engines on the basis of the orders of the Delhi High Court; the monitoring of forest use by the High Courts and the Supreme Court to ensure that there is no unjustified loss of forest cover; and the directions mandating the disclosure of assets of electoral candidates for the Houses of Parliament and State Assembly.\nThe Supreme Court has observed that the PIL has tended to become a means to gain publicity or obtain relief contrary to constitutionally valid legislation and policy. Observers point out that many High Courts and certain Supreme Court judges are reluctant to entertain PILs filed by non-governmental organizations and activists, citing concerns of separation of powers and parliamentary sovereignty.\nIreland.\nIn Irish law, there is no such thing as a \"class action\" per se. Third-party litigation funding is prohibited under Irish law. Instead, there is the 'representative action' () or 'test case' (\"c\u00e1s samplach\"). A representative action is \"where one claimant or defendant, with the same interest as a group of claimants or defendants in an action, institutes or defends proceedings on behalf of that group of claimants or defendants.\"\nSome test cases in Ireland have included:\nItaly.\nItaly has class action legislation. Consumer associations can file claims on behalf of groups of consumers to obtain judicial orders against corporations that cause injury or damage to consumers. These types of claims are increasing, and Italian courts have allowed them against banks that continue to apply compound interest on retail clients' current account overdrafts. The introduction of class actions is on the government's agenda. On November 19, 2007, the Senato della Repubblica passed a class-action law in Finanziaria 2008, a financial document for the economy management of the government. From 10 December 2007, in order of Italian legislation system, the law is before the House and has to be passed also by the Camera dei Deputati, the second house of Italian Parliament, to become an effective law. In 2004, the Italian parliament considered the introduction of a type of class action, specifically in the area of consumer law. No such law has been enacted, but scholars demonstrated that class actions (\"azioni rappresentative\") do not contrast with Italian principles of civil procedure. Class action is regulated by art. 140 bis of the Italian consumers' code and has been in force since 1 July 2009.\nNetherlands.\nDutch law allows associations (\"verenigingen\") and foundations (\"stichtingen\") to bring a so-called collective action on behalf of other persons, provided they can represent the interests of such persons according to their by-laws (\"statuten\") (section 3:305a Dutch Civil Code). All types of actions are permitted. This includes a claim for monetary damages, provided the event occurred after 15 November 2016 (purusuant to new legislation which entered into force 1 January 2020). Most class actions over the past decade have been in the field of securities fraud and financial services. The acting association or foundation may come to a collective settlement with the defendant. The settlement may also include\u00a0\u2013 and usually primarily consists of\u00a0\u2013 monetary compensation of damages. Such settlement can be declared binding for all injured parties by the Amsterdam Court of Appeal (section 7:907 Dutch Civil Code). The injured parties have an opt-out right during the opt-out period set by the Court, usually 3 to 6 months. Settlements involving injured parties from outside The Netherlands can also be declared binding by the Court. Since US courts are reluctant to take up class actions brought on behalf of injured parties not residing in the US who have suffered damages due to acts or omissions committed outside the US, combinations of US class actions and Dutch collective actions may come to a settlement that covers plaintiffs worldwide. An example of this is the Royal Dutch Shell Oil Reserves Settlement that was declared binding upon both US and non-US plaintiffs.\nPoland.\n\"Pozew zbiorowy\" or class action has been allowed under Polish law since July 19, 2010. A minimum of 10 persons, suing based on the same law, is required.\nRussia.\nCollective litigation has been allowed under Russian law since 2002. Basic criteria are, like in the US, numerosity, commonality, and typicality.\nSpain.\nSpanish law allows nominated consumer associations to take action to protect the interests of consumers. A number of groups already have the power to bring collective or class actions: certain consumer associations, bodies legally constituted to defend the \"collective interest\" and groups of injured parties.\nRecent changes to Spanish civil procedure rules include the introduction of a quasi-class action right for certain consumer associations to claim damages on behalf of unidentified classes of consumers. The rules require consumer associations to represent an adequate number of affected parties who have suffered the same harm. Also any judgment made by the Spanish court will list the individual beneficiaries or, if that is not possible, conditions that need to be fulfilled for a party to benefit from a judgment.\nSwitzerland.\nSwiss law does not allow for any form of class action. When the government proposed a new federal code of civil procedure in 2006, replacing the cantonal codes of civil procedure, it rejected the introduction of class actions, arguing that\nUnited Kingdom.\nEngland and Wales.\nThe Civil Procedure Rules of the courts of England and Wales came into force in 1999 and have provided for representative actions in limited circumstances (under Part 19.6). These have not been much used, with only two reported cases at the court of first instance in the first ten years after the Civil Procedure Rules took effect. However, a sectoral mechanism was adopted by the Consumer Rights Act 2015, taking effect on October 1, 2015. Under the provisions therein, opt-in or opt-out collective procedures may be certified for breaches of competition law. This is currently the closest mechanism to a class action in England and Wales.\nUnited States.\n In the United States, the class representative, also called a lead plaintiff, named plaintiff, or representative plaintiff is the named party in a class-action lawsuit. Although the class representative is named as a party to the litigation, the court must approve the class representative when it certifies the lawsuit as a class action.\nThe class representative must be able to represent the interests of all the members of the class, by being typical of the class members and not having conflicts with them. He or she is responsible to hire the attorney, file the lawsuit, consult on the case, and agree to any settlement. In exchange, the class representative may be entitled to compensation (at the discretion of the court) out of the recovery amount.\nFederal courts.\nIn federal courts, class actions are governed by Federal Rules of Civil Procedure Rule and 28 U.S.C.A. \u00a7\u00a01332(d). Cases in federal courts are only allowed to proceed as class actions if the court has jurisdiction to hear the case, and if the case meets the criteria set out in Rule 23. In the vast majority of federal class actions, the class is acting as the plaintiff. However, Rule 23 also provides for defendant class actions.\nTypically, federal courts are thought to be more favorable for defendants, and state courts more favorable for plaintiffs. Many class actions are filed initially in state court. The defendant will frequently try to remove the case to federal court. The Class Action Fairness Act of 2005 increases defendants' ability to remove state cases to federal court by giving federal courts original jurisdiction for all class actions with damages exceeding $5,000,000 exclusive of interest and costs. The Class Action Fairness Act contains carve-outs for, among other things, shareholder class actions covered by the Private Securities Litigation Reform Act of 1995 and those concerning internal corporate governance issues (the latter typically being brought as shareholder derivative actions in the state courts of Delaware, the state of incorporation of most large corporations).\nIn securities class actions that allege violations of Section 11 of the Securities Act of 1933, \"officers and directors are liable together with the corporation for material misrepresentations in the registration statement.\" To have \"standing\" to sue under Section 11 of the 1933 Act in a class action, a plaintiff must be able to prove that he can \"trace\" his shares to the registration statement and offering in question, as to which there is alleged a material misstatement or omission. In the absence of an ability to actually trace his shares, such as when securities issued at multiple times are held by the Depository Trust Company in a fungible bulk and physical tracing of particular shares may be impossible, the plaintiff may be barred from pursuing his claim for lack of standing.\nJurisdiction.\nClass actions may be brought in federal court if the claim arises under federal law or if the claim falls under 28 U.S.C. \u00a7 1332(d). Under \u00a7\u00a01332(d)(2) the federal district courts have original jurisdiction over any civil action where the amount in controversy exceeds $5,000,000 and\nNationwide plaintiff classes are possible, but such suits must have a commonality of issues across state lines. This may be difficult if the civil law in the various states lack significant commonalities. Large class actions brought in federal court frequently are consolidated for pre-trial purposes through the device of multidistrict litigation (MDL). It is also possible to bring class actions under state law, and in some cases the court may extend its jurisdiction to all the members of the class, including out of state (or even internationally) as the key element is the jurisdiction that the court has over the defendant.\nClass certification under Rule 23.\nFor the case to proceed as a class action and bind absent class members, the court must certify the class under Rule 23 on a motion from the party wishing to proceed on a class basis. For a class to be certified, the moving party must meet all of the criteria listed under Rule 23(a), and at least one of the criteria listed under Rule 23(b).\nThe 23(a) criteria are referred to as numerosity, commonality, typicality, and adequacy. Numerosity refers to the number of people in the class. To be certified, the class has to have enough members that simply adding each of them as a named party to the lawsuit would be impractical. There is no bright-line rule to determine numerosity, but classes with hundreds of members are generally deemed to be sufficiently numerous. To satisfy commonality, there must be a common question of law and fact such that \"determination of its truth or falsity will resolve an issue that is central to the validity of each one of the claims in one stroke\". The typicality requirement ensures that the claims or defenses of the named plaintiff are typical of those of everyone else in the class. Finally, adequacy requirement states that the named plaintiff must fairly and adequately represent the interests of the absent class members.\nRule 23(b)(3) allows class certification if \"questions of law or fact common to class members \"predominate\" over any questions affecting only individual members, and that a class action is \"superior\" to other available methods for fairly and efficiently adjudicating the controversy.\"\nNotice and settlement.\nDue process requires in most cases that notice describing the class action be sent, published, or broadcast to class members. As part of this notice procedure, there may have to be several notices, first a notice giving class members the opportunity to opt out of the class, i.e. if individuals wish to proceed with their own litigation they are entitled to do so, only to the extent that they give timely notice to the class counsel or the court that they are opting out. Second, if there is a settlement proposal, the court will usually direct the class counsel to send a settlement notice to all the members of the certified class, informing them of the details of the proposed settlement.\nState courts.\nSince 1938, many states have adopted rules similar to the FRCP. However, some states, like California, have civil procedure systems, which deviate significantly from the federal rules; the California Codes provide for four separate types of class actions. As a result, there are two separate treatises devoted solely to the complex topic of California class actions. Some states, such as Virginia, do not provide for any class actions, while others, such as New York, limit the types of claims that may be brought as class actions.\nIn fiction.\nJohn Grisham's 2003 novel \"The King of Torts\" is a fable of the rights and wrongs of class actions."}
{"id": "7201", "revid": "1010104059", "url": "https://en.wikipedia.org/wiki?curid=7201", "title": "Contempt of court", "text": "Contempt of court, often referred to simply as \"contempt\", is the offense of being disobedient to or disrespectful toward a court of law and its officers in the form of behavior that opposes or defies the authority, justice and dignity of the court. A similar attitude towards a legislative body is termed contempt of Parliament or contempt of Congress. The verb for \"to commit contempt\" is contemn (as in \"to contemn a court order\") and a person guilty of this is a contemnor. \nThere are broadly two categories of contempt: being disrespectful to legal authorities in the courtroom, or willfully failing to obey a court order. Contempt proceedings are especially used to enforce equitable remedies, such as injunctions. In some jurisdictions, the refusal to respond to subpoena, to testify, to fulfill the obligations of a juror, or to provide certain information can constitute contempt of the court.\nWhen a court decides that an action constitutes contempt of court, it can issue an order that in the context of a court trial or hearing declares a person or organization to have disobeyed or been disrespectful of the court's authority, called \"found\" or \"held\" in contempt. That is the judge's strongest power to impose sanctions for acts that disrupt the court's normal process.\nA finding of being in contempt of court may result from a failure to obey a lawful order of a court, showing disrespect for the judge, disruption of the proceedings through poor behavior, or publication of material or non-disclosure of material, which in doing so is deemed likely to jeopardize a fair trial. A judge may impose sanctions such as a fine or jail for someone found guilty of contempt of court, which makes contempt of court a process crime. Judges in common law systems usually have more extensive power to declare someone in contempt than judges in civil law systems.\nIn use today.\nContempt of court is essentially seen as a form of disturbance that may impede the functioning of the court. The judge may impose fines and/or jail time upon any person committing contempt of court. The person is usually let out upon his or her agreement to fulfill the wishes of the court. Civil contempt can involve acts of omission. The judge will make use of warnings in most situations that may lead to a person being charged with contempt if the warnings are ignored. It is relatively rare that a person is charged for contempt without first receiving at least one warning from the judge. Constructive contempt, also called \"consequential contempt\", is when a person fails to fulfill the will of the court as it applies to outside obligations of the person. In most cases, constructive contempt is considered to be in the realm of civil contempt due to its passive nature.\nIndirect contempt is something that is associated with civil and constructive contempt and involves a failure to follow court orders. Criminal contempt includes anything that could be considered a disturbance, such as repeatedly talking out of turn, bringing forth previously banned evidence, or harassment of any other party in the courtroom. Direct contempt is an unacceptable act in the presence of the judge (\"in facie curiae\"), and generally begins with a warning, and may be accompanied by an immediate imposition of punishment. Yawning in some cases can be considered contempt of court.\nContempt of court has a significant impact on journalism in the form of restrictions on court reporting which are set out in statute in the UK.\nAustralia.\nIn Australia, a judge may impose a fine or jail for contempt of court, including for refusing to stand up for a judge.\nBelgium.\nA Belgian correctional or civil judge may immediately try the person for insulting the court.\nCanada.\nCommon law offence.\nIn Canada, contempt of court is an exception to the general principle that all criminal offences are set out in the federal Criminal Code. Contempt of court and contempt of Parliament are the only remaining common law offences in Canada.\nContempt of court includes the following behaviors:\nCanadian Federal courts.\n\"This section applies only to Federal Court of Appeal and Federal Court.\"\nUnder Federal Court Rules, Rules 466, and Rule 467 a person who is accused of Contempt needs to be first served with a contempt order and then appear in court to answer the charges. Convictions can only be made when proof beyond a reasonable doubt is achieved.\nIf it is a matter of urgency or the contempt was done in front of a judge, that person can be punished immediately. Punishment can range from the person being imprisoned for a period of less than five years or until the person complies with the order or fine.\nTax Court of Canada.\nUnder Tax Court of Canada Rules of \"Tax Court of Canada Act\", a person who is found to be in contempt may be imprisoned for a period of less than two years or fined. Similar procedures for serving an order first is also used at the Tax Court.\nProvincial courts.\nDifferent procedures exist for different provincial courts. For example, in British Columbia, a justice of the peace can only issue a summons to an offender for contempt, which will be dealt with by a judge, even if the offence was done in the face of the justice.\nHong Kong.\nJudges from the Court of Final Appeal, High Court, District Court along with members from the various tribunals and Coroner's Court all have the power to impose immediate punishments for contempt in the face of the court, derived from legislation or through common law:\nThe use of insulting or threatening language in the magistrates' courts or against a magistrate is in breach of section 99 of the Magistrates Ordinance (Cap 227) which states the magistrate can 'summarily sentence the offender to a fine at level 3 and to imprisonment for 6 months.'\nIn addition, certain appeal boards are given the statutory authority for contempt by them (e.g., Residential Care Home, Hotel and Guesthouse Accommodation, Air Pollution Control, etc.). For contempt in front of these boards, the chairperson will certify the act of contempt to the Court of First Instance who will then proceed with a hearing and determine the punishment.\nEngland and Wales.\nIn England and Wales (a common law jurisdiction), the law on contempt is partly set out in case law (common law), and partly codified by the Contempt of Court Act 1981. Contempt may be classified as \"criminal\" or \"civil\". The maximum penalty for criminal contempt under the 1981 Act is committal to prison for two years.\nDisorderly, contemptuous or insolent behavior toward the judge or magistrates while holding the court, tending to interrupt the due course of a trial or other judicial proceeding, may be prosecuted as \"direct\" contempt. The term \"direct\" means that the court itself cites the person in contempt by describing the behavior observed on the record. Direct contempt is distinctly different from indirect contempt, wherein another individual may file papers alleging contempt against a person who has willfully violated a lawful court order.\nThere are limits to the powers of contempt created by rulings of European Court of Human Rights. Reporting on contempt of court, the Law Commission commented that \"punishment of an advocate for what he or she says in court, whether a criticism of the judge or a prosecutor, amounts to an interference with his or her rights under article 10 of the ECHR\" and that such limits must be \"prescribed by law\" and be \"necessary in a democratic society\", citing Nikula v Finland \nCriminal contempt.\nThe Crown Court is a superior court according to the Senior Courts Act 1981, and Crown Courts have the power to punish contempt. The Divisional Court as part of the High Court has ruled that this power can apply in these three circumstances:\nWhere it is necessary to act quickly, a judge may act to impose committal (to prison) for contempt.\nWhere it is not necessary to be so urgent, or where indirect contempt has taken place the Attorney General can intervene and the Crown Prosecution Service will institute criminal proceedings on his behalf before a Divisional Court of the Queen's Bench Division of the High Court of Justice of England and Wales.\nMagistrates' courts also have powers under the 1981 Act to order to detain any person who \"insults the court\" or otherwise disrupts its proceedings until the end of the sitting. Upon contempt being admitted or proved the (invariably) District Judge (sitting as a magistrate) may order committal to prison for a maximum of one month, impose a fine of up to \u00a32,500, or both.\nIt will be contempt to bring an audio recording device or picture-taking device of any sort into an English court without the consent of the court.\nIt will not be contempt according to section 10 of the Act for a journalist to refuse to disclose his sources, unless the court has considered the evidence available and determined that the information is \"necessary in the interests of justice or national security or for the prevention of disorder or crime\".\nStrict liability contempt.\nUnder the Contempt of Court Act it is criminal contempt to publish anything which creates a real risk that the course of justice in proceedings may be seriously impaired. It only applies where proceedings are active, and the Attorney General has issued guidance as to when he believes this to be the case, and there is also statutory guidance. The clause prevents the newspapers and media from publishing material that is too extreme or sensationalist about a criminal case until the trial or linked trials are over and the juries have given their verdicts.\nSection 2 of the Act defines and limits the previous common law definition of contempt (which was previously based upon a presumption that any conduct could be treated as contempt, regardless of intent), to only instances where there can be proved an intent to cause a substantial risk of serious prejudice to the administration of justice (i.e./e.g., the conduct of a trial).\nCivil contempt.\nIn civil proceedings there are two main ways in which contempt is committed:\nIndia.\nIn India, contempt of court is of two types:\nPunishment.\nSix months jail, or fine up to \u20b92000, or both.\nUnited States.\nIn United States jurisprudence, acts of contempt are generally divided into direct or indirect and civil or criminal. Direct contempt occurs in the presence of a judge; civil contempt is \"coercive and remedial\" as opposed to punitive. In the United States, relevant statutes include and Federal Rule of Criminal Procedure 42.\nContempt of court in a civil suit is generally not considered to be a criminal offense, with the party benefiting from the order also holding responsibility for the enforcement of the order. However, some cases of civil contempt have been perceived as intending to harm the reputation of the plaintiff, or to a lesser degree, the judge or the court.\nSanctions for contempt may be criminal or civil. If a person is to be punished criminally, then the contempt must be proven beyond a reasonable doubt, but once the charge is proven, then punishment (such as a fine or, in more serious cases, imprisonment) is imposed unconditionally. The civil sanction for contempt (which is typically incarceration in the custody of the sheriff or similar court officer) is limited in its imposition for so long as the disobedience to the court's order continues: once the party complies with the court's order, the sanction is lifted. The imposed party is said to \"hold the keys\" to his or her own cell, thus conventional due process is not required. In federal and most state courts, the burden of proof for civil contempt is clear and convincing evidence, a lower standard than in criminal cases.\nIn civil contempt cases there is no principle of proportionality. In \"Chadwick v. Janecka\" (3d Cir. 2002), a U.S. court of appeals held that H. Beatty Chadwick could be held indefinitely under federal law, for his failure to produce US$2.5 million as state court ordered in a civil trial. Chadwick had been imprisoned for nine years at that time and continued to be held in prison until 2009, when a state court set him free after 14 years, making his imprisonment the longest on a contempt charge to date.\nCivil contempt is only appropriate when the imposed party has the power to comply with the underlying order. Controversial contempt rulings have periodically arisen from cases involving asset protection trusts, where the court has ordered a settlor of an asset protection trust to repatriate assets so that the assets may be made available to a creditor. A court cannot maintain an order of contempt where the imposed party does not have the ability to comply with the underlying order. This claim when made by the imposed party is known as the \"impossibility defense\".\nContempt of court is considered a prerogative of the court, and \"the requirement of a jury does not apply to 'contempts committed in disobedience of any lawful writ, process, order, rule, decree, or command entered in any suit or action brought or prosecuted in the name of, or on behalf of, the United States.'\" This stance is not universally agreed with by other areas of the legal world, and there have been many calls to have contempt cases to be tried by jury, rather than by judge, as a potential conflict of interest rising from a judge both accusing and sentencing the defendant. At least one Supreme Court Justice has made calls for jury trials to replace judge trials on contempt cases.\nThe United States Marshals Service is the agency component that first holds all federal prisoners. It uses the Prisoner Population Management System /Prisoner Tracking System. The only types of records that are disclosed as being in the system are those of \"federal prisoners who are in custody pending criminal proceedings.\" The records of \"alleged civil contempors\" are not listed in the Federal Register as being in the system leading to a potential claim for damages under The Privacy Act, .\nNews media in the United States.\nIn the United States, because of the broad protections granted by the First Amendment, with extremely limited exceptions, unless the media outlet is a party to the case, a media outlet cannot be found in contempt of court for reporting about a case because a court cannot order the media in general not to report on a case or forbid it from reporting facts discovered publicly. Newspapers cannot be closed because of their content.\nCriticism.\nThere have been criticisms over the practice of trying contempt from the bench. In particular, Supreme Court Justice Hugo Black wrote in a dissent, \"It is high time, in my judgment, to wipe out root and branch the judge-invented and judge-maintained notion that judges can try criminal contempt cases without a jury.\""}
{"id": "7202", "revid": "41157102", "url": "https://en.wikipedia.org/wiki?curid=7202", "title": "Corroborating evidence", "text": "Corroborating evidence (or corroboration) is evidence that tends to support a proposition that is already supported by some initial evidence, therefore confirming the proposition. For example, W, a witness, testifies that she saw X drive his automobile into a green car. Meanwhile, Y, another witness, testifies that when he examined X's car, later that day, he noticed green paint on its fender. There can also be corroborating evidence related to a certain source, such as what makes an author think a certain way due to the evidence that was supplied by witnesses or objects.\nAnother type of corroborating evidence comes from using the Baconian method, i.e., the method of agreement, method of difference, and method of concomitant variations.\nThese methods are followed in experimental design. They were codified by Francis Bacon, and developed further by John Stuart Mill and consist of controlling several variables, in turn, to establish which variables are causally connected. These principles are widely used intuitively in various kinds of proofs, demonstrations, and investigations, in addition to being fundamental to experimental design.\nIn law, corroboration refers to the requirement in some jurisdictions, such as in Scotland, that any evidence adduced be backed up by at least one other source (see Corroboration in Scots law).\nAn example of corroboration.\nDefendant says, \"It was like what he/she (a witness) said but...\". This is Corroborative evidence from the defendant that the evidence the witness gave is true and correct.\nCorroboration is not needed in certain instances. For example, there are certain statutory exceptions. In the Education (Scotland) Act, it is only necessary to produce a register as proof of lack of attendance. No further evidence is needed.\nEngland and Wales.\nPerjury\nSee section 13 of the Perjury Act 1911.\nSpeeding offences\nSee section 89(2) of the Road Traffic Regulation Act 1984.\nSexual offences\nSee section 32 of the Criminal Justice and Public Order Act 1994.\nConfessions by mentally handicapped persons\nSee section 77 of the Police and Criminal Evidence Act 1984.\nEvidence of children\nSee section 34 of the Criminal Justice Act 1988.\nEvidence of accomplices\nSee section 32 of the Criminal Justice and Public Order Act 1994."}
{"id": "7203", "revid": "19553220", "url": "https://en.wikipedia.org/wiki?curid=7203", "title": "Cross-examination", "text": "In law, cross-examination is the interrogation of a witness called by one's opponent. It is preceded by direct examination (in Ireland, the United Kingdom, Australia, Canada, South Africa, India and Pakistan known as examination-in-chief) and may be followed by a redirect (re-examination in Ireland, England, Scotland, Australia, Canada, South Africa, India, Hong Kong, and Pakistan). Redirect examination, performed by the attorney or pro se individual who performed the direct examination, clarifies the witness' testimony provided during cross-examination including any subject matter raised during cross-examination but not discussed during direct examination. Recross examination addresses the witness' testimony discussed in redirect by the opponent. Depending on the judge's discretion, opponents are allowed multiple opportunities to redirect and recross examine witnesses (may vary by jurisdiction).\nVariations by jurisdiction.\nIn the United States federal Courts, a cross-examining attorney is typically not permitted to ask questions that do not pertain to the testimony offered during direct examination, but most state courts do permit a lawyer to cross-examine a witness on matters not raised during direct examination. Similarly, courts in England, South Africa, Australia, and Canada allow a cross-examiner to exceed the scope of direct examination.\nSince a witness called by the opposing party is presumed to be hostile, cross-examination does permit leading questions. A witness called by a direct examiner, on the other hand, may only be treated as hostile by that examiner after being permitted to do so by the judge, at the request of that examiner and as a result of the witness being openly antagonistic and/or prejudiced against the party that called them.\nAffecting the outcome of jury trials.\nCross-examination is a key component of a trial and the topic is given substantial attention during courses on trial advocacy. The opinions of a jury or judge are often changed if cross examination casts doubt on the witness. On the other hand, a credible witness may reinforce the substance of their original statements and enhance the judge's or jury's belief. Though the closing argument is often considered the deciding moment of a trial, effective cross-examination wins trials.\nAttorneys anticipate hostile witness' responses during pretrial planning, and often attempt to shape the witnesses' perception of the questions to draw out information helpful to the attorney's case. Typically during an attorney's closing argument he will repeat any admissions made by witnesses that favor their case. Indeed, in the United States, cross-examination is seen as a core part of the entire adversarial system of justice, in that it \"is the principal means by which the believability of a witness and the truth of his testimony are tested.\" Another key component affecting a trial outcome is the jury selection, in which attorneys will attempt to include jurors from whom they feel they can get a favorable response or at the least unbiased fair decision. So while there are many factors affecting the outcome of a trial, the cross-examination of a witness will often influence an open-minded unbiased jury searching for the certainty of facts upon which to base their decision."}
{"id": "7206", "revid": "7997523", "url": "https://en.wikipedia.org/wiki?curid=7206", "title": "Christiania", "text": "Christiania may refer to:"}
{"id": "7207", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=7207", "title": "Charles d'Abancourt", "text": "Charles Xavier Joseph de Franque Ville d'Abancourt (4 July 17589 September 1792) was a French statesman, minister to Louis XVI.\nBiography.\nD'Abancourt was born in Douai, and was the nephew of Charles Alexandre de Calonne. He was Louis XVI's last minister of war (July 1792), and organised the defence of the Tuileries Palace during the 10 August attack. Ordered by the Legislative Assembly to send away the Swiss Guards, he refused, and was arrested for treason to the nation and sent to Orl\u00e9ans to be tried.\nAt the end of August the Assembly ordered Abancourt and the other prisoners at Orl\u00e9ans to be transferred to Paris with an escort commanded by Claude Fournier \"l'Americain\". At Versailles, they learned of the September Massacres in Paris. Abancourt and his fellow-prisoners were murdered in cold blood in massacres on 9 September 1792 at Versailles, and Fournier was unjustly charged with complicity in the crime."}
{"id": "7210", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=7210", "title": "Cubic feet", "text": ""}
{"id": "7211", "revid": "5229608", "url": "https://en.wikipedia.org/wiki?curid=7211", "title": "Curtiss P-40 Warhawk", "text": "The Curtiss P-40 Warhawk is an American single-engined, single-seat, all-metal fighter and ground-attack aircraft that first flew in 1938. The P-40 design was a modification of the previous Curtiss P-36 Hawk which reduced development time and enabled a rapid entry into production and operational service. The Warhawk was used by most Allied powers during World War II, and remained in frontline service until the end of the war. It was the third most-produced American fighter of World War II, after the P-51 and P-47; by November 1944, when production of the P-40 ceased, 13,738 had been built, all at Curtiss-Wright Corporation's main production facilities at Buffalo, New York.\nP-40 Warhawk was the name the United States Army Air Corps gave the plane, and after June 1941, the USAAF\nadopted the name for all models, making it the official name in the U.S. for all P-40s. The British Commonwealth and Soviet air forces used the name Tomahawk for models equivalent to the original P-40, P-40B, and P-40C, while the name Kittyhawk for models equivalent to the P-40D and all later variants.\nP-40s first saw combat with the British Commonwealth squadrons of the Desert Air Force in the Middle East and North African campaigns, during June 1941. No. 112 Squadron Royal Air Force, was among the first to operate Tomahawks in North Africa and the unit was the first Allied military aviation unit to feature the \"shark mouth\" logo, copying similar markings on some Luftwaffe Messerschmitt Bf 110 twin-engine fighters. \nThe P-40's lack of a two-speed supercharger made it inferior to Luftwaffe fighters such as the Messerschmitt Bf 109 or the Focke-Wulf Fw 190 in high-altitude combat and it was rarely used in operations in Northwest Europe. However, between 1941 and 1944, the P-40 played a critical role with Allied air forces in three major theaters: North Africa, the Southwest Pacific, and China. It also had a significant role in the Middle East, Southeast Asia, Eastern Europe, Alaska and Italy. The P-40's performance at high altitudes was not as important in those theaters, where it served as an air superiority fighter, bomber escort and fighter-bomber. Although it gained a postwar reputation as a mediocre design, suitable only for close air support, more recent research including scrutiny of the records of individual Allied squadrons indicates that this was not the case: the P-40 performed surprisingly well as an air superiority fighter, at times suffering severe losses, but also inflicting a very heavy toll on enemy aircraft. Based on war-time victory claims, over 200 Allied fighter pilots \u2013 from the UK, Australia, New Zealand, Canada, South Africa, the US and the Soviet Union \u2013 became aces flying the P-40. These included at least 20 double aces, mostly over North Africa, China, Burma and India, the South West Pacific and Eastern Europe. The P-40 offered the additional advantages of low cost and durability, which kept it in production as a ground-attack aircraft long after it was obsolete as a fighter.\nDesign and development.\nOrigins.\nOn 14 October 1938, Curtiss test pilot Edward Elliott flew the prototype XP-40 on its first flight in Buffalo. The XP-40 was the 10th production Curtiss P-36 Hawk, with its Pratt &amp; Whitney R-1830 Twin Wasp 14-cylinder air-cooled radial engine replaced at the direction of Chief Engineer Don R. Berlin by a liquid-cooled, supercharged Allison V-1710 V-12 engine. The first prototype placed the glycol coolant radiator in an underbelly position on the fighter, just aft of the wing's trailing edge. USAAC Fighter Projects Officer Lieutenant Benjamin S. Kelsey flew this prototype some 300 miles in 57 minutes, approximately . Hiding his disappointment, he told reporters that future versions would likely go faster. Kelsey was interested in the Allison engine because it was sturdy and dependable, and it had a smooth, predictable power curve. The V-12 engine offered as much power as a radial engine but had a smaller frontal area and allowed a more streamlined cowl than an aircraft with a radial engine, promising a theoretical 5% increase in top speed.\nCurtiss engineers worked to improve the XP-40's speed by moving the radiator forward in steps. Seeing little gain, Kelsey ordered the aircraft to be evaluated in a NACA wind tunnel to identify solutions for better aerodynamic qualities. From 28 March to 11 April 1939, the prototype was studied by NACA. Based on the data obtained, Curtiss moved the glycol coolant radiator forward to the chin; its new air scoop also accommodated the oil cooler air intake. Other improvements to the landing gear doors and the exhaust manifold combined to give performance that was satisfactory to the USAAC. Without beneficial tail winds, Kelsey flew the XP-40 from Wright Field back to Curtiss's plant in Buffalo at an average speed of . Further tests in December 1939 proved the fighter could reach .\nAn unusual production feature was a special truck rig to speed delivery at the main Curtiss plant in Buffalo, New York. The rig moved the newly built P-40s in two main components, the main wing and the fuselage, the eight miles from the plant to the airport where the two units were mated for flight and delivery.\nPerformance characteristics.\nThe P-40 was conceived as a pursuit aircraft and was agile at low and medium altitudes but suffered from a lack of power at higher altitudes. At medium and high speeds it was one of the tightest-turning early monoplane designs of the war, and it could out turn most opponents it faced in North Africa and the Russian Front. In the Pacific Theater it was out-turned at lower speeds by the lightweight fighters A6M Zero and Nakajima Ki-43 \"Oscar\" which lacked the P-40's structural strength for high-speed hard turns. The American Volunteer Group Commander Claire Chennault advised against prolonged dog-fighting with the Japanese fighters due to speed reduction favoring the Japanese.\nAllison's V-1710 engines produced at sea level and . This was not powerful compared with contemporary fighters, and the early P-40 variants' top speeds were only average. The single-stage, single-speed supercharger meant that the P-40 was a poor high-altitude fighter. Later versions, with Allisons or more powerful 1,400\u00a0hp Packard Merlin engines were more capable. Climb performance was fair to poor, depending on the subtype. Dive acceleration was good and dive speed was excellent. The highest-scoring P-40 ace, Clive Caldwell (RAAF), who claimed 22 of his 28\u00bd kills in the type, said that the P-40 had \"almost no vices\", although \"it was a little difficult to control in terminal velocity\". The P-40 had one of the fastest maximum dive speeds of any fighter of the early war period, and good high-speed handling.\nThe P-40 tolerated harsh conditions and a variety of climates. Its semi-modular design was easy to maintain in the field. It lacked innovations such as boosted ailerons or automatic leading edge slats, but its strong structure included a five-spar wing, which enabled P-40s to pull high-G turns and survive some midair collisions. Intentional ramming attacks against enemy aircraft were occasionally recorded as victories by the Desert Air Force and Soviet Air Forces. Caldwell said P-40s \"would take a tremendous amount of punishment, violent aerobatics as well as enemy action\". Operational range was good by early war standards and was almost double that of the Supermarine Spitfire or Messerschmitt Bf 109, although inferior to the Mitsubishi A6M Zero, Nakajima Ki-43 and Lockheed P-38 Lightning.\nCaldwell found the P-40C Tomahawk's armament of two .50\u00a0in (12.7\u00a0mm) Browning AN/M2 \"light-barrel\" dorsal nose-mount synchronized machine guns and two .303 Browning machine guns in each wing to be inadequate. This was improved with the P-40D (Kittyhawk I) which abandoned the synchronized gun mounts and instead had two .50\u00a0in (12.7\u00a0mm) guns in each wing, although Caldwell still preferred the earlier Tomahawk in other respects. The D had armor around the engine and the cockpit, which enabled it to withstand considerable damage. This allowed Allied pilots in Asia and the Pacific to attack Japanese fighters head on, rather than try to out-turn and out-climb their opponents. Late-model P-40s were well armored. Visibility was adequate, although hampered by a complex windscreen frame, and completely blocked to the rear in early models by a raised turtledeck. Poor ground visibility and relatively narrow landing gear track caused many losses on the ground.\nCurtiss tested a follow-on design, the Curtiss XP-46, but it offered little improvement over newer P-40 models and was cancelled.\nOperational history.\nIn April 1939, the U.S. Army Air Corps, having witnessed the new, sleek, high-speed, in-line-engined fighters of the European air forces, placed the largest fighter order it had ever made for 524 P-40s.\nFrench Air Force.\nAn early order came from the French \"Arm\u00e9e de l'Air\", which was already operating P-36s. The \"Arm\u00e9e de l'Air\" ordered 100 (later the order was increased to 230) as the Hawk 81A-1 but the French were defeated before the aircraft had left the factory and the aircraft were diverted to British and Commonwealth service (as the Tomahawk I), in some cases complete with metric flight instruments.\nIn late 1942, as French forces in North Africa split from the Vichy government to side with the Allies, U.S. forces transferred P-40Fs from 33rd FG to \"GC II/5\", a squadron that was historically associated with the Lafayette Escadrille. GC II/5 used its P-40Fs and Ls in combat in Tunisia and later for patrol duty off the Mediterranean coast until mid-1944, when they were replaced by Republic P-47D Thunderbolts.\nBritish Commonwealth.\nDeployment.\nIn all, 18 Royal Air Force (RAF) squadrons, four Royal Canadian Air Force (RCAF), three South African Air Force (SAAF) and two Royal Australian Air Force (RAAF) squadrons serving with RAF formations, used P-40s. The first units to convert were Hawker Hurricane squadrons of the Desert Air Force (DAF), in early 1941. The first Tomahawks delivered came without armor, bulletproof windscreens or self-sealing fuel tanks, which were installed in subsequent shipments. Pilots used to British fighters sometimes found it difficult to adapt to the P-40's rear-folding landing gear, which was more prone to collapse than the lateral-folding landing gear of the Hawker Hurricane or Supermarine Spitfire. In contrast to the \"three-point landing\" commonly employed with British types, P-40 pilots were obliged to use a \"wheels landing\": a longer, low angle approach that touched down on the main wheels first.\nTesting showed the aircraft did not have the performance needed for use in Northwest Europe at high-altitude, due to the service ceiling limitation. Spitfires used in the theater operated at heights around , while the P-40's Allison engine, with its single-stage, low altitude rated supercharger, worked best at or lower. When the Tomahawk was used by Allied units based in the UK from February 1941, this limitation relegated the Tomahawk to low-level reconnaissance with RAF Army Cooperation Command\nand only No. 403 Squadron RCAF was used in the fighter role for a mere 29 sorties, before being replaced by Spitfires. Air Ministry deemed the P-40 unsuitable for the theater. UK P-40 squadrons from mid-1942 re-equipped with aircraft such as Mustangs \nThe Tomahawk was superseded in North Africa by the more powerful Kittyhawk (\"D\"-mark onwards) types from early 1942, though some Tomahawks remained in service until 1943. Kittyhawks included many improvements and were the DAF's air superiority fighter for the critical first few months of 1942, until \"tropicalised\" Spitfires were available. In 2012, the virtually intact remains of a Kittyhawk were found; it had run out of fuel in the Egyptian Sahara in June 1942.\nDAF units received nearly 330 Packard V-1650 Merlin-powered P-40Fs, called Kittyhawk IIs, most of which went to the USAAF, and the majority of the 700 \"lightweight\" L models, also powered by the Packard Merlin, in which the armament was reduced to four .50\u00a0in (12.7\u00a0mm) Brownings (Kittyhawk IIA). The DAF also received some 21 of the later P-40K and the majority of the 600 P-40Ms built; these were known as Kittyhawk IIIs. The \"lightweight\" P-40Ns (Kittyhawk IV) arrived from early 1943 and were used mostly as fighter-bombers. From July 1942 until mid-1943, elements of the U.S. 57th Fighter Group (57th FG) were attached to DAF P-40 units. The British government also donated 23 P-40s to the Soviet Union.\nCombat performance.\nTomahawks and Kittyhawks bore the brunt of \"Luftwaffe\" and \"Regia Aeronautica\" fighter attacks during the North African campaign. The P-40s were considered superior to the Hurricane, which they replaced as the primary fighter of the Desert Air Force. \nThe P-40 initially proved quite effective against Axis aircraft and contributed to a slight shift of momentum in the Allies' favor. The gradual replacement of Hurricanes by the Tomahawks and Kittyhawks led to the \"Luftwaffe\" accelerating retirement of the Bf 109E and introducing the newer Bf 109F; these were to be flown by the veteran pilots of elite \"Luftwaffe\" units, such as \"Jagdgeschwader\" 27 (JG27), in North Africa. The P-40 was generally considered roughly equal or slightly superior to the Bf 109 at low altitude but inferior at high altitude, particularly against the Bf 109F. Most air combat in North Africa took place well below , negating much of the Bf 109's superiority. The P-40 usually had an advantage over the Bf 109 in horizontal maneuvers (turning), dive speed and structural strength, was roughly equal in firepower but was slightly inferior in speed and outclassed in rate of climb and operational ceiling.\nThe P-40 was generally superior to early Italian fighter types, such as the Fiat G.50 Freccia and the Macchi C.200. Its performance against the Macchi C.202 \"Folgore\" elicited varying opinions. Some observers consider the Macchi C.202 superior. Caldwell, who scored victories against them in his P-40, felt that the \"Folgore\" was superior to the P-40 and the Bf 109 except that its armament of only two or four machine guns was inadequate. Other observers considered the two equally matched or favored the \"Folgore\" in aerobatic performance, such as turning radius. Aviation historian Walter J. Boyne wrote that over Africa, the P-40 and the \"Folgore\" were \"equivalent\". Against its lack of high-altitude performance, the P-40 was considered to be a stable gun platform, and its rugged construction meant that it was able to operate from rough front line airstrips with a good rate of serviceability.\nThe earliest victory claims by P-40 pilots include Vichy French aircraft, during the 1941 Syria-Lebanon campaign, against Dewoitine D.520s, a type often considered to be the best French fighter of the war. The P-40 was deadly against Axis bombers in the theater, as well as against the Bf 110 twin-engine fighter. In June 1941, Caldwell, of No. 250 Squadron RAF in Egypt, flying as F/O Jack Hamlyn's wingman, recorded in his log book that he was involved in the first air combat victory for the P-40. This was a CANT Z.1007 bomber on 6 June. The claim was not officially recognized, as the crash of the CANT was not witnessed. The first official victory occurred on 8 June, when Hamlyn and Flt Sgt Tom Paxton destroyed a CANT Z.1007 from \"211a Squadriglia\" of the \"Regia Aeronautica\", over Alexandria. Several days later, the Tomahawk was in action over Syria with No. 3 Squadron RAAF, which claimed 19 aerial victories over Vichy French aircraft during June and July 1941, for the loss of one P-40 (and one lost to ground fire).\nSome DAF units initially failed to use the P-40's strengths or used outdated defensive tactics such as the Lufbery circle. The superior climb rate of the Bf 109 enabled fast, swooping attacks, neutralizing the advantages offered by conventional defensive tactics. Various new formations were tried by Tomahawk units from 1941 to 1942, including \"fluid pairs\" (similar to the German \"rotte\"); one or two \"weavers\" at the back of a squadron in formation and whole squadrons bobbing and weaving in loose formations. Werner Schr\u00f6er, who was credited with destroying 114 Allied aircraft in only 197 combat missions, referred to the latter formation as \"bunches of grapes\", because he found them so easy to pick off. The leading German \"expert\" in North Africa, Hans-Joachim Marseille, claimed as many as 101 P-40s during his career.\nFrom 26 May 1942, Kittyhawk units operated primarily as fighter-bomber units, giving rise to the nickname \"Kittybomber\". As a result of this change in role and because DAF P-40 squadrons were frequently used in bomber escort and close air support missions, they suffered relatively high losses; many Desert Air Force P-40 pilots were caught flying low and slow by marauding Bf 109s.\nCaldwell believed that Operational Training Units did not properly prepare pilots for air combat in the P-40 and as a commander, stressed the importance of training novice pilots properly.\nCompetent pilots who took advantage of the P-40's strengths were effective against the best of the \"Luftwaffe\" and \"Regia Aeronautica\". In August 1941, Caldwell was attacked by two Bf 109s, one of them piloted by German ace Werner Schr\u00f6er. Although Caldwell was wounded three times and his Tomahawk was hit by more than 100 bullets and five 20 mm cannon shells, Caldwell shot down Schr\u00f6er's wingman and returned to base. Some sources also claim that in December 1941, Caldwell killed a prominent German \"Experte\", Erbo von Kageneck (69 kills), while flying a P-40. Caldwell's victories in North Africa included 10 Bf 109s and two Macchi C.202s. Billy Drake of 112 Squadron was the leading British P-40 ace with 13 victories. James \"Stocky\" Edwards (RCAF), who achieved 12 kills in the P-40 in North Africa, shot down German ace Otto Schulz (51 kills) while flying a Kittyhawk with No. 260 Squadron RAF. Caldwell, Drake, Edwards and Nicky Barr were among at least a dozen pilots who achieved ace status twice over while flying the P-40. A total of 46 British Commonwealth pilots became aces in P-40s, including seven double aces.\nChinese Air Force.\nFlying Tigers (American Volunteer Group).\nThe Flying Tigers, known officially as the 1st American Volunteer Group (AVG), were a unit of the Chinese Air Force, recruited from U.S. Navy, Marines and Army aviators.\nChennault received crated Model Bs which his airmen assembled in Burma at the end of 1941, adding self-sealing fuel tanks and a second pair of wing guns, such that the aircraft became a hybrid of B and C models. These were not well-liked by their pilots: they lacked drop tanks for extra range, and there were no bomb racks on the wings. Chennault considered the liquid-cooled engine vulnerable in combat because a single bullet through the coolant system would cause the engine to overheat in minutes. The Tomahawks also had no radios, so the AVG improvised by installing a fragile radio transceiver, the RCA-7-H, which had been built for a Piper Cub. Because the plane had a single-stage low-altitude supercharger, its effective ceiling was about . The most critical problem was the lack of spare parts; the only source was from damaged aircraft. The planes were viewed as cast-offs that no one else wanted, dangerous and difficult to fly. But the pilots did appreciate some of the planes' features. There were two heavy sheets of steel behind the pilot's head and back that offered solid protection, and overall the planes were ruggedly constructed.\nCompared to opposing Japanese fighters, the P-40B's strengths were that it was sturdy, well armed, faster in a dive and possessed an excellent rate of roll. While the P-40s could not match the maneuverability of the Japanese Army air arm's Nakajima Ki-27s and Ki-43s, nor the much more famous Zero naval fighter in slow, turning dogfights, at higher speeds the P-40s were more than a match. AVG leader Claire Chennault trained his pilots to use the P-40's particular performance advantages. The P-40 had a higher dive speed than any Japanese fighter aircraft of the early war years, for example, and could exploit so-called \"boom-and-zoom\" tactics. The AVG was highly successful, and its feats were widely publicized by an active cadre of international journalists to boost sagging public morale at home. According to its official records, in just months, the Flying Tigers destroyed 297 enemy aircraft for the loss of just four of its own in air-to-air combat.\nIn the spring of 1942, the AVG received a small number of Model E's. Each came equipped with a radio, six .50-caliber machine guns, and auxiliary bomb racks that could hold 35-lb fragmentation bombs. Chennault's armorer added bomb racks for 570-lb Russian bombs, which the Chinese had in abundance. These planes were used in the battle of the Salween River Gorge in late May 1942, which kept the Japanese from entering China from Burma and threatening Kunming. Spare parts, however, remained in short supply. \"Scores of new planes...were now in India, and there they stayed\u2014in case the Japanese decided to invade... the AVG was lucky to get a few tires and spark plugs with which to carry on its daily war.\"\n4th Air Group.\nChina received 27 P-40E models in early 1943. These were assigned to squadrons of the 4th Air Group.\nUnited States Army Air Forces.\nA total of 15 USAAF pursuit/fighter groups (FG), along with other pursuit/fighter squadrons and a few tactical reconnaissance (TR) units, operated the P-40 during 1941\u201345. As was also the case with the Bell P-39 Airacobra, many USAAF officers considered the P-40 exceptional but it was gradually replaced by the Lockheed P-38 Lightning, the Republic P-47 Thunderbolt and the North American P-51 Mustang. The bulk of the fighter operations by the USAAF in 1942\u201343 were borne by the P-40 and the P-39. In the Pacific, these two fighters, along with the U.S. Navy Grumman F4F Wildcat, contributed more than any other U.S. types to breaking Japanese air power during this critical period.\nPacific theaters.\nThe P-40 was the main USAAF fighter aircraft in the South West Pacific and Pacific Ocean theaters during 1941\u201342. At Pearl Harbor and in the Philippines, USAAF P-40 squadrons suffered crippling losses on the ground and in the air to Japanese fighters such as the A6M Zero and Ki-43 Oscar respectively. During the attack on Pearl Harbor, most of the USAAF fighters were P-40Bs, most of which were destroyed. However, a few P-40s managed to get in the air and shoot down several Japanese aircraft, most notably by George Welch and Kenneth Taylor.\nIn the Dutch East Indies campaign, the 17th Pursuit Squadron (Provisional), formed from USAAF pilots evacuated from the Philippines, claimed 49 Japanese aircraft destroyed, for the loss of 17 P-40s The seaplane tender USS \"Langley\" was sunk by Japanese airplanes while delivering P-40s to Tjilatjap, Java. In the Solomon Islands and New Guinea Campaigns and the air defence of Australia, improved tactics and training allowed the USAAF to better use the strengths of the P-40. Due to aircraft fatigue, scarcity of spare parts and replacement problems, the US Fifth Air Force and Royal Australian Air Force created a joint P-40 management and replacement pool on 30 July 1942 and many P-40s went back and forth between the air forces.\nThe 49th Fighter Group was in action in the Pacific from the beginning of the war. Robert DeHaven scored 10 kills (of 14 overall) in the P-40 with the 49th FG. He compared the P-40 favorably with the P-38:\nThe 8th, 15th, 18th, 24th, 49th, 343rd and 347th PGs/FGs, flew P-40s in the Pacific theaters between 1941 and 1945, with most units converting to P-38s from 1943 to 1944. In 1945, the 71st Reconnaissance Group employed them as armed forward air controllers during ground operations in the Philippines, until it received delivery of P-51s. They claimed 655 aerial victories.\nContrary to conventional wisdom, with sufficient altitude, the P-40 could turn with the A6M and other Japanese fighters, using a combination of a nose-down vertical turn with a bank turn, a technique known as a low yo-yo. Robert DeHaven describes how this tactic was used in the 49th Fighter group:\nChina Burma India Theater.\nUSAAF and Chinese P-40 pilots performed well in this theater against many Japanese types such as the Ki-43, Nakajima Ki-44 \"Tojo\" and the Zero. The P-40 remained in use in the China Burma India Theater (CBI) until 1944 and was reportedly preferred over the P-51 Mustang by some US pilots flying in China. The American Volunteer Group (Flying Tigers) was integrated into the USAAF as the 23rd Fighter Group in June 1942. The unit continued to fly newer model P-40s until the end of the war, achieving a high kill-to-loss ratio.\nIn the Battle of the Salween River Gorge of May 1942 the AVG used the P-40E model equipped with wing racks that could carry six 35-pound fragmentation bombs and Chennault's armorer developed belly racks to carry Russian 570-pound bombs, which the Chinese had in large quantity.\nUnits arriving in the CBI after the AVG in the 10th and 14th Air Forces continued to perform well with the P-40, claiming 973 kills in the theater, or 64.8 percent of all enemy aircraft shot down. Aviation historian Carl Molesworth stated that \"...the P-40 simply dominated the skies over Burma and China. They were able to establish air superiority over free China, northern Burma and the Assam valley of India in 1942, and they never relinquished it.\" The 3rd, 5th, 51st and 80th FGs, along with the 10th TRS, operated the P-40 in the CBI. CBI P-40 pilots used the aircraft very effectively as a fighter-bomber. The 80th Fighter Group in particular used its so-called \"B-40\" (P-40s carrying 1,000-pound high-explosive bombs) to destroy bridges and kill bridge repair crews, sometimes demolishing their target with one bomb. At least 40 U.S. pilots reached ace status while flying the P-40 in the CBI.\nEurope and Mediterranean theaters.\nOn 14 August 1942, the first confirmed victory by a USAAF unit over a German aircraft in World War II was achieved by a P-40C pilot. 2nd Lt Joseph D. Shaffer, of the 33rd Fighter Squadron, intercepted a Focke-Wulf Fw 200C-3 maritime patrol aircraft that overflew his base at Reykjav\u00edk, Iceland. Shaffer damaged the Fw 200, which was finished off by a P-38F. Warhawks were used extensively in the Mediterranean Theater of Operations (MTO) by USAAF units, including the 33rd, 57th, 58th, 79th, 324th and 325th Fighter Groups. While the P-40 suffered heavy losses in the MTO, many USAAF P-40 units achieved high kill-to-loss ratios against Axis aircraft; the 324th FG scored better than a 2:1 ratio in the MTO. In all, 23 U.S. pilots became aces in the MTO on the P-40, most of them during the first half of 1943.\nP-40 pilots from the 57th FG were the first USAAF fliers to see action in the MTO, while attached to Desert Air Force Kittyhawk squadrons, from July 1942. The 57th was also the main unit involved in the \"Palm Sunday Massacre\", on 18 April 1943. Decoded Ultra signals revealed a plan for a large formation of Junkers Ju 52 transports to cross the Mediterranean, escorted by German and Italian fighters. Between 1630 and 1830 hours, all wings of the group were engaged in an intensive effort against the enemy air transports. Of the four Kittyhawk wings, three had left the patrol area before a convoy of a 100+ enemy transports were sighted by 57th FG, which tallied 74 aircraft destroyed. The group was last in the area, and intercepted the Ju 52s escorted by large numbers of Bf 109s, Bf 110s and Macchi C.202s. The group claimed 58 Ju 52s, 14 Bf 109s and two Bf 110s destroyed, with several probables and damaged. Between 20 and 40 of the Axis aircraft landed on the beaches around Cap Bon to avoid being shot down; six Allied fighters were lost, five of them P-40s.\nOn 22 April, in Operation Flax, a similar force of P-40s attacked a formation of 14 Messerschmitt Me 323 \"Gigant\" (\"Giant\") six-engine transports, covered by seven Bf 109s from II./JG 27. All the transports were shot down, for a loss of three P-40s. The 57th FG was equipped with the Curtiss fighter until early 1944, during which time they were credited with at least 140 air-to-air kills. On 23 February 1943, during Operation Torch, the pilots of the 58th FG flew 75 P-40Ls off the aircraft carrier to the newly captured Vichy French airfield, Cazas, near Casablanca, in French Morocco. The aircraft supplied the 33rd FG and the pilots were reassigned.\nThe 325th FG (known as the \"Checkertail Clan\") flew P-40s in the MTO and was credited with at least 133 air-to-air kills from April\u2013October 1943, of which 95 were Bf 109s and 26 were Macchi C.202s, for the loss of 17 P-40s in combat. The 325th FG historian Carol Cathcart wrote:\nCathcart wrote that Lt. Robert Sederberg assisted a comrade being attacked by five Bf 109s, destroyed at least one German aircraft, and may have shot down as many as five. Sederberg was shot down and became a prisoner of war.\nA famous African-American unit, the 99th FS, better known as the \"Tuskegee Airmen\" or \"Redtails\", flew P-40s in stateside training and for their initial eight months in the MTO. On 9 June 1943, they became the first African-American fighter pilots to engage enemy aircraft, over Pantelleria, Italy. A single Focke-Wulf Fw 190 was reported damaged by Lieutenant Willie Ashley Jr. On 2 July the squadron claimed its first verified kill; a Fw 190 destroyed by Captain Charles Hall. The 99th continued to score with P-40s until February 1944, when they were assigned P-39s and P-51 Mustangs.\nThe much-lightened P-40L was most heavily used in the MTO, primarily by U.S. pilots. Many US pilots stripped down their P-40s even further to improve performance, often removing two or more of the wing guns from the P-40F/L.\nRoyal Australian Air Force.\nThe Kittyhawk was the main fighter used by the RAAF in World War II, in greater numbers than the Spitfire. Two RAAF squadrons serving with the Desert Air Force, No. 3 and No. 450 Squadrons, were the first Australian units to be assigned P-40s. Other RAAF pilots served with RAF or SAAF P-40 squadrons in the theater.\nMany RAAF pilots achieved high scores in the P-40. At least five reached \"double ace\" status: Clive Caldwell, Nicky Barr, John Waddy, Bob Whittle (11 kills each) and Bobby Gibbes (10 kills) in the Middle East, North African and/or New Guinea campaigns. In all, 18 RAAF pilots became aces while flying P-40s.\nNicky Barr, like many Australian pilots, considered the P-40 a reliable mount: \"The Kittyhawk became, to me, a friend. It was quite capable of getting you out of trouble more often than not. It was a real warhorse.\"\nAt the same time as the heaviest fighting in North Africa, the Pacific War was also in its early stages, and RAAF units in Australia were completely lacking in suitable fighter aircraft. Spitfire production was being absorbed by the war in Europe; P-38s were trialled, but were difficult to obtain; Mustangs had not yet reached squadrons anywhere, and Australia's tiny and inexperienced aircraft industry was geared towards larger aircraft. USAAF P-40s and their pilots originally intended for the U.S. Far East Air Force in the Philippines, but diverted to Australia as a result of Japanese naval activity were the first suitable fighter aircraft to arrive in substantial numbers. By mid-1942, the RAAF was able to obtain some USAAF replacement shipments.\nRAAF Kittyhawks played a crucial role in the South West Pacific theater. They fought on the front line as fighters during the critical early years of the Pacific War, and the durability and bomb-carrying abilities (1,000\u00a0lb/454\u00a0kg) of the P-40 also made it ideal for the ground attack role. For example, 75, and 76 Squadrons played a critical role during the Battle of Milne Bay, fending off Japanese aircraft and providing effective close air support for the Australian infantry, negating the initial Japanese advantage in light tanks and sea power.\nThe RAAF units that most used Kittyhawks in the South West Pacific were 75, 76, 77, 78, 80, 82, 84 and 86 Squadrons. These squadrons saw action mostly in the New Guinea and Borneo campaigns.\nLate in 1945, RAAF fighter squadrons in the South West Pacific began converting to P-51Ds. However, Kittyhawks were in use with the RAAF until the end of the war, in Borneo. In all, the RAAF acquired 841 Kittyhawks (not counting the British-ordered examples used in North Africa), including 163 P-40E, 42 P-40K, 90 P-40 M and 553 P-40N models. In addition, the RAAF ordered 67 Kittyhawks for use by No. 120 (Netherlands East Indies) Squadron (a joint Australian-Dutch unit in the South West Pacific). The P-40 was retired by the RAAF in 1947.\nRoyal Canadian Air Force.\nA total of 13 Royal Canadian Air Force units operated the P-40 in the North West European or Alaskan theaters.\nIn mid-May 1940, Canadian and US officers watched comparative tests of a XP-40 and a Spitfire, at RCAF Uplands, Ottawa. While the Spitfire was considered to have performed better, it was not available for use in Canada and the P-40 was ordered to meet home air defense requirements. In all, eight Home War Establishment Squadrons were equipped with the Kittyhawk: 72 Kittyhawk I, 12 Kittyhawk Ia, 15 Kittyhawk III and 35 Kittyhawk IV aircraft, for a total of 134 aircraft. These aircraft were mostly diverted from RAF Lend-Lease orders for service in Canada. The P-40 Kittyhawks were obtained in lieu of 144 P-39 Airacobras originally allocated to Canada but reassigned to the RAF.\nHowever, before any home units received the P-40, three RCAF Article XV squadrons operated Tomahawk aircraft from bases in the United Kingdom. No. 403 Squadron RCAF, a fighter unit, used the Tomahawk Mk II briefly before converting to Spitfires. Two Army Co-operation (close air support) squadrons: 400 and 414 Sqns trained with Tomahawks, before converting to Mustang Mk. I aircraft and a fighter/reconnaissance role. Of these, only No. 400 Squadron used Tomahawks operationally, conducting a number of armed sweeps over France in the late 1941. RCAF pilots also flew Tomahawks or Kittyhawks with other British Commonwealth units based in North Africa, the Mediterranean, South East Asia and (in at least one case) the South West Pacific.\nIn 1942, the Imperial Japanese Navy occupied two islands, Attu and Kiska, in the Aleutians, off Alaska. RCAF home defense P-40 squadrons saw combat over the Aleutians, assisting the USAAF. The RCAF initially sent 111 Squadron, flying the Kittyhawk I, to the US base on Adak island. During the drawn-out campaign, 12 Canadian Kittyhawks operated on a rotational basis from a new, more advanced base on Amchitka, southeast of Kiska. 14 and 111 Sqns took \"turn-about\" at the base. During a major attack on Japanese positions at Kiska on 25 September 1942, Squadron Leader Ken Boomer shot down a Nakajima A6M2-N (\"Rufe\") seaplane. The RCAF also purchased 12 P-40Ks directly from the USAAF while in the Aleutians. After the Japanese threat diminished, these two RCAF squadrons returned to Canada and eventually transferred to England without their Kittyhawks.\nIn January 1943, a further Article XV unit, 430 Squadron was formed at RAF Hartford Bridge, England and trained on obsolete Tomahawk IIA. The squadron converted to the Mustang I before commencing operations in mid-1943.\nIn early 1945 pilots from No. 133 Squadron RCAF, operating the P-40N out of RCAF Patricia Bay, (Victoria, British Columbia), intercepted and destroyed two Japanese balloon-bombs, which were designed to cause wildfires on the North American mainland. On 21 February, Pilot Officer E. E. Maxwell shot down a balloon, which landed on Sumas Mountain in Washington State. On 10 March, Pilot Officer J. 0. Patten destroyed a balloon near Saltspring Island, British Columbia. The last interception took place on 20 April 1945 when Pilot Officer P.V. Brodeur from 135 Squadron out of Abbotsford, British Columbia shot down a balloon over Vedder Mountain.\nThe RCAF units that operated P-40s were, in order of conversion: \nRoyal New Zealand Air Force.\nSome Royal New Zealand Air Force (RNZAF) pilots and New Zealanders in other air forces flew British P-40s while serving with DAF squadrons in North Africa and Italy, including the ace Jerry Westenra.\nA total of 301 P-40s were allocated to the RNZAF under Lend-Lease, for use in the Pacific Theater, although four of these were lost in transit. The aircraft equipped 14 Squadron, 15 Squadron, 16 Squadron, 17 Squadron, 18 Squadron, 19 Squadron and 20 Squadron.\nRNZAF P-40 squadrons were successful in air combat against the Japanese between 1942 and 1944. Their pilots claimed 100 aerial victories in P-40s, whilst losing 20 aircraft in combat Geoff Fisken, the highest scoring British Commonwealth ace in the Pacific, flew P-40s with 15 Squadron, although half of his victories were claimed with the Brewster Buffalo.\nThe overwhelming majority of RNZAF P-40 victories were scored against Japanese fighters, mostly Zeroes. Other victories included Aichi D3A \"Val\" dive bombers. The only confirmed twin engine claim, a Ki-21 \"Sally\" (misidentified as a G4M \"Betty\") fell to Fisken in July 1943.\nFrom late 1943 and 1944, RNZAF P-40s were increasingly used against ground targets, including the innovative use of naval depth charges as improvised high-capacity bombs. The last front line RNZAF P-40s were replaced by Vought F4U Corsairs in 1944. The P-40s were relegated to use as advanced pilot trainers.\nThe remaining RNZAF P-40s, excluding the 20 shot down and 154 written off, were mostly scrapped at Rukuhia in 1948.\nSoviet Union.\nThe Soviet \"Voyenno-Vozdushnye Sily\" (VVS; \"Military Air Forces\") and \"Morskaya Aviatsiya\" (MA; \"Naval Air Service\") also referred to P-40s as \"Tomahawks\" and \"Kittyhawks\". In fact, the Curtiss P-40 Tomahawk / Kittyhawk was the first Allied fighter supplied to the USSR under the Lend-Lease agreement.\nThe USSR received 247 P-40B/Cs (equivalent to the Tomahawk IIA/B in RAF service) and 2,178 P-40E, -K, -L, and -N models between 1941 and 1944. The Tomahawks were shipped from Great Britain and directly from the US, many of them arriving incomplete, lacking machine guns and even the lower half of the engine cowling. In late September 1941, the first 48 P-40s were assembled and checked in the USSR. Test flights showed some manufacturing defects: generator and oil pump gears and generator shafts failed repeatedly, which led to emergency landings. The test report indicated that the Tomahawk was inferior to Soviet \"M-105P-powered production fighters in speed and rate of climb. However, it had good short field performance, horizontal maneuverability, range, and endurance.\" Nevertheless, Tomahawks and Kittyhawks were used against the Germans. The 126th IAP, fighting on the Western and Kalinin Fronts, were the first unit to receive the P-40. The regiment entered action on 12 October 1941. By 15 November 1941, that unit had shot down 17 German aircraft. However, Lt (SG) Smirnov noted that the P-40 armament was sufficient for strafing enemy lines but rather ineffective in aerial combat. Another pilot, S.G. Ridnyy (Hero of Soviet Union), remarked that he had to shoot half the ammunition at 50\u2013100 meters (165\u2013340\u00a0ft) to shoot down an enemy aircraft.\nIn January 1942, some 198 aircraft sorties were flown (334 flying hours) and 11 aerial engagements were conducted, in which five Bf 109s, one Ju 88, and one He 111 were downed. These statistics reveal a surprising fact: it turns out that the Tomahawk was fully capable of successful air combat with a Bf 109. The reports of pilots about the circumstances of the engagements confirm this fact. On 18 January 1942, Lieutenants S. V. Levin and I. P. Levsha (in pair) fought an engagement with seven Bf 109s and shot down two of them without loss. On 22 January, a flight of three aircraft led by Lieutenant E. E. Lozov engaged 13 enemy aircraft and shot down two Bf 109Es, again without loss. Altogether, in January, two Tomahawks were lost; one downed by German anti-aircraft artillery and one lost to Messerschmitts.\nThe Soviets stripped down their P-40s significantly for combat, in many cases removing the wing guns altogether in P-40B/C types, for example. Soviet Air Force reports state that they liked the range and fuel capacity of the P-40, which were superior to most of the Soviet fighters, though they still preferred the P-39. Soviet pilot Nikolai G. Golodnikov recalled: \"The cockpit was vast and high. At first it felt unpleasant to sit waist-high in glass, as the edge of the fuselage was almost at waist level. But the bullet-proof glass and armored seat were strong and visibility was good. The radio was also good. It was powerful, reliable, but only on HF (high frequency). The American radios did not have hand microphones but throat microphones. These were good throat mikes: small, light and comfortable.\" The biggest complaint of some Soviet airmen was its poor climb rate and problems with maintenance, especially with burning out the engines. VVS pilots usually flew the P-40 at War Emergency Power settings while in combat, which brought acceleration and speed performance closer to that of their German rivals, but could burn out engines in a matter of weeks. Tires and batteries also failed. The fluid in the engine's radiators often froze, cracking their cores, which made the Allison engine unsuitable for operations during harsh winter conditions. During the winter of 1941, the 126 IAP (Fighter Aviation Regiment) suffered from cracked radiators on 38 occasions. Often, entire regiments were reduced to a single flyable aircraft because no replacement parts were available. They also had difficulty with the more demanding requirements for fuel and oil quality of the Allison engines. A fair number of burned-out P-40s were re-engined with Soviet Klimov M-105 engines, but these performed relatively poorly and were relegated to rear area use.\nThe P-40 saw the most front line use in Soviet hands in 1942 and early 1943. Deliveries over the Alaska-Siberia ALSIB ferry route began in October 1942. It was used in the northern sectors and played a significant role in the defense of Leningrad. The most numerically important types were P-40B/C, P-40E and P-40K/M. By the time the better P-40F and N types became available, production of superior Soviet fighters had increased sufficiently so that the P-40 was replaced in most Soviet Air Force units by the Lavochkin La-5 and various later Yakovlev types. In spring 1943, Lt D.I. Koval of the 45th IAP gained ace status on the North Caucasian front, shooting down six German aircraft flying a P-40. Some Soviet P-40 squadrons had good combat records. Some Soviet pilots became aces on the P-40, though not as many as on the P-39 Airacobra, the most numerous Lend-Lease fighter used by the Soviet Union. However, Soviet commanders thought the Kittyhawk significantly outclassed the Hurricane, although it was \"not in the same league as the Yak-1\".\nJapan.\nThe Japanese Army captured some P-40s and later operated a number in Burma. The Japanese appear to have had as many as 10 flyable P-40Es. For a brief period in 1943, a few of them were actually used operationally by 2 \"Hiko Chutai\", 50 \"Hiko Sentai\" (2nd Air Squadron, 50th Air Regiment) in the defense of Rangoon. Testimony of this is given by Yasuhiko Kuroe, a member of the 64 \"Hiko Sentai\". In his memoirs, he says one Japanese-operated P-40 was shot down in error by a friendly Mitsubishi Ki-21 \"Sally\" over Rangoon.\nOther nations.\nThe P-40 was used by over two dozen countries during and after the war. The P-40 was used by Brazil, Egypt, Finland and Turkey. The last P-40s in military service, used by the Brazilian Air Force (FAB), were retired in 1954.\nIn the air war over Finland, several Soviet P-40s were shot down or had to crash-land due to other reasons. The Finns, short of good aircraft, collected these and managed to repair one P-40M, P-40M-10-CU 43\u20135925, \"white 23\", which received Finnish Air Force serial number KH-51 (KH denoting \"Kittyhawk\", as the British designation of this type was Kittyhawk III). This aircraft was attached to an operational squadron HLeLv 32 of the Finnish Air Force, but lack of spares kept it on the ground, with the exception of a few evaluation flights.\nSeveral P-40Ns were used by the Royal Netherlands East Indies Army Air Force with No. 120 (Netherlands East Indies) Squadron RAAF against the Japanese before being used during the fighting in Indonesia until February 1949.\nVariants and development stages.\nThis new liquid-cooled engine fighter had a radiator mounted under the rear fuselage\nbut the prototype XP-40 was later modified and the radiator was moved forward under the engine.\nSurvivors.\nOn 11 May 2012, a crashed P-40 Kittyhawk (ET574) was found in the Sahara desert. No trace of the pilot has been found to date. Due to the extreme arid conditions, little corrosion of the metal surfaces occurred. The conditions in which it was found are similar to those preferred for aircraft boneyard. An attempt has been made to bring back the Kittyhawk to Great Britain with the RAF Museum paying a salvage team with Supermarine Spitfire PK664 to recover the Kittyhawk. This turned out to be unsuccessful as the Kittyhawk is now being displayed outside at a military museum at El Alamein, having received a makeover many consider 'hideous', and PK664 being reported lost.\nOf the 13,738 P-40s built, only 28 remain airworthy, with three of them being converted to dual-controls/dual-seat configuration. Approximately 13 aircraft are on static display and another 36 airframes are under restoration for either display or flight."}
{"id": "7212", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=7212", "title": "Creed", "text": "A creed, also known as a confession, symbol, or statement of faith, is a statement of the shared beliefs of (an often religious) community in the form of a fixed formula summarizing core tenets.\nThe earliest creed in Christianity, \"Jesus is Lord\", originated in the writings of Saint Paul. One of the most widely used Christian creeds is the Nicene Creed, first formulated in AD 325 at the First Council of Nicaea. It was based on Christian understanding of the canonical gospels, the letters of the New Testament and, to a lesser extent, the Old Testament. Affirmation of this creed, which describes the Trinity, is generally taken as a fundamental test of orthodoxy for most Christian denominations, and was historically purposed against Arianism. The Apostles' Creed is also broadly accepted. Some Christian denominations and other groups have rejected the authority of those creeds.\nMuslims declare the \"shahada\", or testimony: \"I bear witness that there is no god but (the One) God \"(Allah)\", and I bear witness that Muhammad is God's messenger.\"\nWhether Judaism is creedal has been a point of some controversy. Although some say Judaism is noncreedal in nature, others say it recognizes a single creed, the \"Shema Yisrael\", which begins: \"Hear, O Israel: the our God, the is one.\"\nTerminology.\nThe word \"creed\" is particularly used for a concise statement which is recited as part of liturgy. The term is anglicized from Latin \"credo\" \"I believe\", the incipit of the Latin texts of the Apostles' Creed and the Nicene Creed. A creed is sometimes referred to as a \"symbol\" in a specialized meaning of that word (which was first introduced to Late Middle English in this sense), after Latin \"symbolum\" \"creed\" (as in \"Symbolum Apostolorum\" = \"Apostles' Creed\"), after Greek \"symbolon\" \"token, watchword\".\nSome longer statements of faith in the Protestant tradition are instead called \"confessions of faith\", or simply \"confession\" (as in e.g. Helvetic Confession). Within Evangelical Protestantism, the terms \"doctrinal statement\" or \"doctrinal basis\" tend to be preferred. Doctrinal statements may include positions on lectionary and translations of the Bible, particularly in fundamentalist churches of the King James Only movement.\nThe term \"creed\" is sometimes extended to comparable concepts in non-Christian theologies; thus the Islamic concept of \"\u02bfaq\u012bdah\" (literally \"bond, tie\") is often rendered as \"creed\".\nChristian creeds.\nSeveral creeds have originated in Christianity.\nChristian confessions of faith.\nProtestant denominations are usually associated with confessions of faith, which are similar to creeds but usually longer.\nChristians without creeds.\nSome Christian denominations do not profess a creed. This stance is often referred to as \"non-creedalism\". The Religious Society of Friends, also known as the Quakers, consider that they have no need for creedal formulations of faith. The Church of the Brethren and other Schwarzenau Brethren churches also espouse no creed, referring to the New Testament, as their \"rule of faith and practice.\" Jehovah's Witnesses contrast \"memorizing or repeating creeds\" with acting to \"do what Jesus said\". Unitarian Universalists do not share a creed.\nMany evangelical Protestants similarly reject creeds as definitive statements of faith, even while agreeing with some creeds' substance. The Baptists have been non-creedal \"in that they have not sought to establish binding authoritative confessions of faith on one another\". While many Baptists are not opposed to the ancient creeds, they regard them as \"not so final that they cannot be revised and re-expressed. At best, creeds have a penultimacy about them and, of themselves, could never be the basis of Christian fellowship\". Moreover, Baptist \"confessions of faith\" have often had a clause such as this from the First London (Particular) Baptist Confession (Revised edition, 1646):\nSimilar reservations about the use of creeds can be found in the Restoration Movement and its descendants, the Christian Church (Disciples of Christ), the Churches of Christ, and the Christian churches and churches of Christ. Restorationists profess \"no creed but Christ\".\nBishop John Shelby Spong, retired Episcopal Bishop of Newark, has written that dogmas and creeds were merely \"a stage in our development\" and \"part of our religious childhood.\" In his book, \"Sins of the Scripture\", Spong wrote that \"Jesus seemed to understand that no one can finally fit the holy God into his or her creeds or doctrines. That is idolatry.\"\nIn the Swiss Reformed Churches, there was a quarrel about the Apostles' Creed in the mid-19th century. As a result, most cantonal reformed churches stopped prescribing any particular creed.\nThe Church of Jesus Christ of Latter-day Saints.\nWithin the sects of the Latter Day Saint movement, the \"Articles of Faith\" are a list composed by Joseph Smith as part of an 1842 letter sent to \"Long\" John Wentworth, editor of the \"Chicago Democrat\". It is canonized with the Bible, the \"Book of Mormon\", the \"Doctrine &amp; Covenants\" and \"Pearl of Great Price\", as part of the standard works of The Church of Jesus Christ of Latter-day Saints.\nCreedal works include:\nJewish creed.\nWhether Judaism is creedal in character has generated some controversy. Rabbi Milton Steinberg wrote that \"By its nature Judaism is averse to formal creeds which of necessity limit and restrain thought\" and asserted in his book \"Basic Judaism\" (1947) that \"Judaism has never arrived at a creed.\" The 1976 Centenary Platform of the Central Conference of American Rabbis, an organization of Reform rabbis, agrees that \"Judaism emphasizes action rather than creed as the primary expression of a religious life.\"\nOthers, however, characterize the Shema Yisrael as a creedal statement in strict monotheism embodied in a single prayer: \"Hear O Israel, the Lord is our God, the Lord is One\" (; transliterated \"Shema Yisrael Adonai Eloheinu Adonai Echad\").\nA notable statement of Jewish principles of faith was drawn up by Maimonides as his 13 Principles of Faith.\nIslamic creed.\nThe shahada, the two-part statement that \"There is no god but Allah; Muhammad is the messenger of God\" is often popularly called \"the Islamic creed\" and its utterance is one of the \"five pillars\".\nIn Islamic theology, the term most closely corresponding to \"creed\" is \"\u02bfaq\u012bdah\" ()\nThe first such creed was written as \"a short answer to the pressing heresies of the time\" is known as \"Al-Fiqh Al-Akbar\" and ascribed to Ab\u016b \u1e24an\u012bfa. Two well known creeds were the \"Fiqh Akbar II\" \"representative\" of the al-Ash'ari, and \"Fiqh Akbar III\", \"representative\" of the Ash-Shafi'i.\n\"Iman\" () in Islamic theology denotes a believer's religious faith. Its most simple definition is the belief in the six articles of faith, known as \"ark\u0101n al-\u012bm\u0101n\"."}
{"id": "7213", "revid": "28903366", "url": "https://en.wikipedia.org/wiki?curid=7213", "title": "Claudius Aelianus", "text": "Claudius Aelianus (, modern Greek transliteration \"Kl\u00e1vdios Elian\u00f3s\"; c. 175c. 235 AD), commonly Aelian (), born at Praeneste, was a Roman author and teacher of rhetoric who flourished under Septimius Severus and probably outlived Elagabalus, who died in 222. He spoke Greek so fluently that he was called \"honey-tongued\" ( ); Roman-born, he preferred Greek authors, and wrote in a slightly archaizing Greek himself.\nHis two chief works are valuable for the numerous quotations from the works of earlier authors, which are otherwise lost, and for the surprising lore, which offers unexpected glimpses into the Greco-Roman world-view. It is also the only Greco-Roman work to mention Gilgamesh.\n\"De Natura Animalium\".\n\"On the Nature of Animals\" (\"On the Characteristics of Animals\" is an alternative title; , \"\"; usually cited, though, by its Latin title: \"De Natura Animalium\") is a curious collection, in seventeen books, of brief stories of natural history, sometimes selected with an eye to conveying allegorical moral lessons, sometimes because they are just so astonishing:\nThe Loeb Classical Library introduction characterizes the book as\nAelian's anecdotes on animals rarely depend on direct observation: they are almost entirely taken from written sources, often Pliny the Elder, but also other authors and works now lost, to whom he is thus a valuable witness. He is more attentive to marine life than might be expected, though, and this seems to reflect first-hand personal interest; he often quotes \"fishermen\". At times he strikes the modern reader as thoroughly credulous, but at others he specifically states that he is merely reporting what is told by others, and even that he does not believe them. Aelian's work is one of the sources of medieval natural history and of the bestiaries of the Middle Ages.\nThe portions of the text that are still extant are badly mangled and garbled and replete with later interpolations. Conrad Gessner (or Gesner), the Swiss scientist and natural historian of the Renaissance, made a Latin translation of Aelian's work, to give it a wider European audience. An English translation by A. F. Scholfield has been published in the Loeb Classical Library, 3 vols. (1958-59).\n\"Varia Historia\".\n\"Various History\" (, \"\")\u2014for the most part preserved only in an abridged form\u2014is Aelian's other well-known work, a miscellany of anecdotes and biographical sketches, lists, pithy maxims, and descriptions of natural wonders and strange local customs, in 14\u00a0books, with many surprises for the cultural historian and the mythographer, anecdotes about the famous Greek philosophers, poets, historians, and playwrights and myths instructively retold. The emphasis is on \"various\" moralizing tales about heroes and rulers, athletes and wise men; reports about food and drink, different styles in dress or lovers, local habits in giving gifts or entertainments, or in religious beliefs and death customs; and comments on Greek painting. Aelian gives an account of fly fishing, using lures of red wool and feathers, of lacquerwork, serpent worship \u2014 Essentially the \"Various History\" is a Classical \"magazine\" in the original senses of that word. He is not perfectly trustworthy in details, and his agenda was heavily influenced by Stoic opinions, perhaps so that his readers will not feel guilty, but Jane Ellen Harrison found survivals of archaic rites mentioned by Aelian very illuminating in her \"Prolegomena to the Study of Greek Religion\" (1903, 1922).\nThe first printing was in 1545. The standard modern text is Mervin R. Dilts's, of 1974.\nTwo English translations of the \"Various History,\" by Fleming (1576) and Stanley (1665) made Aelian's miscellany available to English readers, but after 1665 no English translation appeared, until three English translations appeared almost simultaneously: James G. DeVoto, \"Claudius Aelianus: \u03a0\u03bf\u03b9\u03ba\u03af\u03bb\u03b7\u03c2 \u1f39\u03c3\u03c4\u03bf\u03c1\u03af\u03b1\u03c2 (\"Varia Historia\")\" Chicago, 1995; Diane Ostrom Johnson, \"An English Translation of Claudius Aelianus' \"Varia Historia\"\", 1997; and N. G. Wilson, \"Aelian: Historical Miscellany\" in the Loeb Classical Library.\nOther works.\nConsiderable fragments of two other works, \"On Providence\" and \"Divine Manifestations\", are preserved in the early medieval encyclopedia, the \"Suda.\" Twenty \"letters from a farmer\" after the manner of Alciphron are also attributed to him. The letters are invented compositions to a fictitious correspondent, which are a device for vignettes of agricultural and rural life, set in Attica, though mellifluous Aelian once boasted that he had never been outside Italy, never been aboard a ship (which is at variance, though, with his own statement, \"de Natura Animalium\" XI.40, that he had seen the bull Serapis with his own eyes). Thus conclusions about actual agriculture in the \"Letters\" are as likely to evoke Latium as Attica. The fragments have been edited in 1998 by D. Domingo-Foraste, but are not available in English. The \"Letters\" are available in the Loeb Classical Library, translated by Allen Rogers Benner and Francis H. Fobes (1949)."}
